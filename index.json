[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a Statistician, currently working as part of a statistics group supporting an engineering research lab. My main research interests are spatial statistics and statistical computing, but I also enjoy \u0026ldquo;playing in everyone else\u0026rsquo;s backyard\u0026rdquo; (i.e. collaborations). Some of the backyards I\u0026rsquo;ve played in - from my previous gig as an assistant professor at West Virginia University - include Forensics and medical applications.\n","date":1590883200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1590883200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a Statistician, currently working as part of a statistics group supporting an engineering research lab. My main research interests are spatial statistics and statistical computing, but I also enjoy \u0026ldquo;playing in everyone else\u0026rsquo;s backyard\u0026rdquo; (i.e. collaborations). Some of the backyards I\u0026rsquo;ve played in - from my previous gig as an assistant professor at West Virginia University - include Forensics and medical applications.","tags":null,"title":"Casey Jelsema","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["rmarkdown"],"content":"Preface For this post, I\u0026rsquo;m going to assume you are fairly familiar with RMarkdown. If not, you may want to start with RMarkdown: The Definitive Guide, or R for Data Science. The short version is that RMarkdown is a flavor of the markup language Markdown, which uses plain-text formatting and can be rendered into a variety of output types, including PDF, MS Word, HTML, and others. With RMarkdown, you can incorporate R \u0026ldquo;code chunks\u0026rdquo; to create a fully reproducible document.\nWith RMarkdown, you can also create a PowerPoint presentation, meaning that you can create a reproducible slide deck which includes code and results. You might ask why use PowerPoint when there are a variety of presentation formats that RMarkdown supports. I personally like the xaringan  format. However, sometimes your hands are tied.\n Perhaps you\u0026rsquo;re doing an analysis for someone who insists on PP (maybe they\u0026rsquo;ll just take a couple of your slides to incorporate into a larger presentation). Maybe you have a corporate PP template that you have to use. Maybe the technology that the presentation will be projected on doesn\u0026rsquo;t support HTML slides. Maybe it\u0026rsquo;s something else.  So for whatever the reason, you need to make do with PowerPoint.\nAnother complication: Suppose you need to do the same basic analysis for a number of variables. For example, say you need to generate a set of boxplots, and a table of summary statistics for each variable in Fisher\u0026rsquo;s Iris data, comparing across the three Species. You\u0026rsquo;re already using RMarkdown, because you don\u0026rsquo;t want to be running results in R and then pasting figures and tables over to the slide deck manually. But you also don\u0026rsquo;t really want to be copy-pasting the same code all over, right? (Hint: No, you don\u0026rsquo;t). So we\u0026rsquo;d like to automate this process. You should be familiar with writing loops in R, and with that and a child document we can do exactly this.\nWe\u0026rsquo;ll need two files for this task, I\u0026rsquo;m going to name them main_doc.Rmd and child_doc.Rmd. The next sections will walk through the code that will go into each.\nMain Document Only the main document will need a YAML header, child documents can do without (come to think of it, I\u0026rsquo;m not sure if they are allowed to have YAML headers). A basic one might look like this:\n---\rtitle: \u0026quot;Looping for PP Slides\u0026quot;\rsubtitle: \u0026quot;Presentation subtitle\u0026quot;\rauthor: \u0026quot;Casey Jelsema\u0026quot;\rdate: \u0026quot;Generated | `r format(Sys.Date(), '%B %d, %Y')`\u0026quot;\routput:\rpowerpoint_presentation: default\r---\r Though note that I included one minorly fancy thing here, by setting the date to be a bit of R code so that it automatically updates when the file is knit. You may or may not want that. You can also put it elsewhere, for example in document type reports, I often set the subtitle to be the date.\nFor PP presentations, the section header (#) is what tells the document to start a new slide. You can change this with a setting in the YAML header. For instance, maybe you will have several sections, so you want two pound signs to denote the start of a new slide, you\u0026rsquo;d add slide_level: 2 to the output section, nested underneath powerpoint_presentation, see slide level). There are also a variety of other options available, including setting a reference document for a custom template, or having a two-column slide. Both of those are described here.\nThe rest of the main document is below. I\u0026rsquo;ll just past it all and then talk about it.\n```{r, setup, echo = FALSE, message=FALSE, include=FALSE }\rknitr::opts_chunk$set( echo=FALSE, message=FALSE)\rlibrary(\u0026quot;tidyverse\u0026quot;)\rlibrary(\u0026quot;flextable\u0026quot;)\r```\r# Introduction\rIn this presentation we use Fisher's iris data as an example.\r```{r, load-data, results=\u0026quot;hide\u0026quot; }\rdata(iris)\riris \u0026lt;- iris %\u0026gt;% mutate( Species = str_to_sentence(Species) )\r```\r- There is a code chunk here where I'm loading the data and doing some formatting to clean it up.\r- For the demonstration I'm just going to loop through to make some box plots and a table for each of the variables.\r```{r, loop-over-params, results=\u0026quot;hide\u0026quot; }\rparam_vec \u0026lt;- colnames(iris)[1:4]\rnParam \u0026lt;- length(param_vec)\rout \u0026lt;- rep(NA,nParam)\rfor( ii in 1:nParam ){\rparam_ii \u0026lt;- param_vec[ii]\rparam_ii_nice \u0026lt;- str_replace( param_vec[ii], \u0026quot;\\\\.\u0026quot;, \u0026quot; \u0026quot;)\rout[ii] \u0026lt;- knitr::knit_child(\u0026quot;child_doc.Rmd\u0026quot;)\r}\r```\r```{r, print-slides, results=\u0026quot;asis\u0026quot;}\rcat( paste(out, collapse=\u0026quot;\\n\u0026quot;) )\r```\r The setup chunk, is, well, your setup chunk. I usually use that to load which packages I\u0026rsquo;ll be using, setting options, sometimes specifying little helper functions I use, setting up a theme for ggplots or creating linetypes for flextable. The setup chunk here is a pretty basic one.\nThen the section header # Introduction gives us a first slide (well, beyond the title slide) with some comments. Note that the code chunk if there, between the first line and the two bullet points, but we don\u0026rsquo;t see it. If you\u0026rsquo;re familiar with RMarkdown, this shouldn\u0026rsquo;t be surprising.\nThe next chunk, loop-over-params is where the action is. So what am I doing?\n Get the variables to loop over, and get the number of them. Here I was easily able to extract them from the data. You might need to do a bit more work for that. The out object is setting up a container for me to put the slides for each variable, so that I\u0026rsquo;m not \u0026ldquo;growing\u0026rdquo; the object throughout my loop. Then in the for-loop, I\u0026rsquo;m grabbing the variable name, creating a nicer version of it for printing. Finally, I run the child document with the command knitr::knit_child(\u0026quot;child_doc.Rmd\u0026quot;).  This works because I wrote the child document with a generic param_ii, so as that object gets modified, the child document uses different variables. The results get put into the iith space of the out container.\nFinally, the last chunk here will print the results of the looping so that they are incorporated into the document. You need to use the results=\u0026quot;asis\u0026quot; option, and collapse all of the results together with a newline.\nChild Document A basic version of the child document is here. One thing I want to draw your attention to is that none of these chunks are labeled. That\u0026rsquo;s because if they get run multiple times, RMarkdown will (appropriately!) complain that there are chunks with the same name. I haven\u0026rsquo;t tried to dynamically name the chunks yet. An quick search showed that I\u0026rsquo;m not alone in this, and there were some solutions proposed. I haven\u0026rsquo;t tried them, and have not had a need to really try.\n```{r, results=\u0026quot;asis\u0026quot;}\r# figure-slide-title\rcat(\u0026quot;# Response: \u0026quot;, param_ii_nice, \u0026quot; | Box plots\u0026quot; )\r```\r```{r, fig.height=3.5 }\r# figure-slide\rggplot( iris , aes_string(x=\u0026quot;Species\u0026quot;, y=param_ii) ) +\rgeom_boxplot( aes(fill=Species) ) +\rlabs( y=param_ii_nice )\r```\r The first chunk is setting the title of the slide, using the \u0026ldquo;nice\u0026rdquo; version of the parameter name. Again, I\u0026rsquo;m printing with the chunk option results=\u0026quot;asis\u0026quot;, because we want to paste exactly what we write into the Rmd. After that, I\u0026rsquo;m just making the figure. One bit you may not be familiar with is the aes_string command in ggplot. Note that typically variable names are unquoted in ggplot. If you have a quoted string, then aes_string will help you out here.\nThen on to the table.\n```{r, results=\u0026quot;asis\u0026quot;}\r# table-slide-title\rcat(\u0026quot;# Response: \u0026quot;, param_ii_nice, \u0026quot; | Statistics\u0026quot; )\r```\r- Comments you want to make in the child slide need to use some conditional logic.\r- Otherwise the same comments go on every child slide.\r- That means they should probably be rather basic, if this is supposed to be automated.\r```{r, ft.left=1, ft.top=6 }\r# table-slide\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit()\r```\r Again, I\u0026rsquo;m first defining the slide title with a results=\u0026quot;asis\u0026quot; chunk option, then making the content of the slide. I tend to use the flextable package for tables, though some parts of officer help to do some fine-tuning. There are other table-making packages, but these have been more than enough for my needs.\nI don\u0026rsquo;t remember exactly why I switched from using the kable function to flextable, I think that I was having a difficult time with MS Word tables. Maybe I just didn\u0026rsquo;t put in enough effort. Regardless, flextable has been easy for me to make tables in the formats I\u0026rsquo;ve needed, and the author is actively, improving it, so I haven\u0026rsquo;t had a reason to look elsewhere.\nIn the table-slide chunk, note that there are chunk options ft.left=1, ft.top=4. These define where the top-left of the table will be placed. It helps to arrange the content and the output on a slide. I\u0026rsquo;m not sure if they work with tables generated from other packages.\nSo, if you\u0026rsquo;ve copied these code chunks into main_doc.Rmd and child_doc.Rmd (well, you can call the first one whatever you like, but the code references child_doc.Rmd), then you should be able to generate a slide deck that has two slides for each variable, along with a first introductory slide.\nYou should be able to take it from there. Add more slides at the beginning as necessary, add more slides after looping over the variables, add more slides into the child document, make a fancier loop, or anything. Once you have the building blocks, it\u0026rsquo;s just a matter of arranging them appropriately. My hope is that this little demo gave you another building block.\nTwo other aspects I\u0026rsquo;d like to demonstrate are: Two-column slides; and conditional logic for dynamically generating the output comments.\nTwo-column slides This one is pretty easy. As I noted above, the RMarkdown book has examples. The basic idea is that you have a set of colons (:) to denote the start and end of column sets, and a (smaller) set of colons to denote the start and end of columns, like so:\n:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\rContent of the left column.\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\rContent of the right column.\r:::\r::::::\r So, for instance, we might replace the table-slide with the following, we get a two-column slide. Note that the column notation goes outside the chunk, or put another way, the code chunk goes inside a particular column.\n```{r, results=\u0026quot;asis\u0026quot;}\rcat(\u0026quot;# Two-column Slide: \u0026quot;, param_ii_nice, \u0026quot; | Statistics\u0026quot; )\r```\r:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\r```{r, ft.left=1, ft.top=2 }\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit()\r```\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\r- Comments you want to make in the child slide need to use some conditional logic.\r- Otherwise the same comments go on every child slide.\r- That means they should probably be rather basic, if this is supposed to be automated.\r:::\r::::::\r You can put several tables in a single column, using multiple code chunks, so that you can set ft.left and ft.top for each table. I\u0026rsquo;ll demonstrate that in the last section.\nConditional logic for results In the placeholder text for the table slide, I mentioned that you\u0026rsquo;ll probably want to use conditional logic for the results, otherwise every iteration of the child slide will have exactly the same text. Let\u0026rsquo;s say we need to run an ANOVA on each outcome. Yes, there are issues with this, one of which is that you\u0026rsquo;d need to be paying attention to how many comparisons you\u0026rsquo;re making, and adjust p-values appropriately. I\u0026rsquo;m not going to worry about that for the moment.\nIn this part, I\u0026rsquo;ve added a bit of formatting to the tables. One thing I did was define a border. To reduce typing, I add the command h1 \u0026lt;- officer::fp_border( width=0.75 ) to the setup chunk of my document. Often I define several lines of varying width and style (e.g., a dashed line). This way I can use them in any table.\n```{r, results=\u0026quot;asis\u0026quot;}\rcat(\u0026quot;# ANOVA: \u0026quot;, param_ii_nice, \u0026quot;\u0026quot; )\r```\r:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\r```{r, ft.left=1, ft.top=2 }\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit() %\u0026gt;%\rborder( part=\u0026quot;header\u0026quot;, i=1, border.top=h1, border.bottom=h1 ) %\u0026gt;%\rborder( part=\u0026quot;body\u0026quot;, i=3, border.bottom=h1 ) %\u0026gt;%\ralign( part=\u0026quot;all\u0026quot;, j=2:4, align=\u0026quot;center\u0026quot; )\r```\r```{r, ft.left=1, ft.top=4 }\rfrml \u0026lt;- as.formula( str_c( param_ii , \u0026quot; ~ Species\u0026quot; ) )\rlm_out \u0026lt;- lm( frml, data=iris )\rpost_hoc \u0026lt;- TukeyHSD( aov(lm_out) )$Species %\u0026gt;%\ras.data.frame() %\u0026gt;%\rrownames_to_column() %\u0026gt;%\rrename( \u0026quot;Contrast\u0026quot; = \u0026quot;rowname\u0026quot;, \u0026quot;Diff\u0026quot;=\u0026quot;diff\u0026quot;, \u0026quot;pvalue\u0026quot;=\u0026quot;p adj\u0026quot;) %\u0026gt;%\rdplyr::select( Contrast, Diff, pvalue ) %\u0026gt;%\rmutate( tpval = ifelse( pvalue \u0026lt; 0.0001, \u0026quot;\u0026lt; 0.0001\u0026quot;, sprintf(\u0026quot;%0.4f\u0026quot;, pvalue) ) )\rpost_hoc %\u0026gt;% dplyr::select( Contrast, Diff, tpval ) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit() %\u0026gt;%\rset_header_labels( \u0026quot;tpval\u0026quot;=\u0026quot;p-value\u0026quot; ) %\u0026gt;%\rborder( part=\u0026quot;header\u0026quot;, i=1, border.top=h1, border.bottom=h1 ) %\u0026gt;%\rborder( part=\u0026quot;body\u0026quot;, i=3, border.bottom=h1 ) %\u0026gt;%\ralign( part=\u0026quot;all\u0026quot;, j=2:3, align=\u0026quot;center\u0026quot; )\r```\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\r```{r, results=\u0026quot;asis\u0026quot;}\r# Conditional logic to build simple sentences for results\rany_diff \u0026lt;- any( post_hoc[[\u0026quot;pvalue\u0026quot;]] \u0026lt;= 0.05 )\rwhich_diff \u0026lt;- which( post_hoc[[\u0026quot;pvalue\u0026quot;]] \u0026lt;= 0.05 )\rif( any_diff ){\rwhich_contrasts \u0026lt;- str_replace( post_hoc[[\u0026quot;Contrast\u0026quot;]][which_diff], \u0026quot;-\u0026quot;, \u0026quot; and \u0026quot;)\rndiff \u0026lt;- length(which_contrasts)\rbullets \u0026lt;- str_c( \u0026quot;- Pairwise comparisons conducted using Tukey's method.\\n\u0026quot;,\r\u0026quot;- Significant differences in mean \u0026quot;, param_ii_nice, \u0026quot; were found between: \u0026quot; )\rif( ndiff==1 ){\rbullets \u0026lt;- str_c( bullets, which_contrasts )\r} else if( ndiff==2 ){\rbullets \u0026lt;- str_c( bullets, str_c( which_contrasts, collapse=\u0026quot; as well as \u0026quot; ), \u0026quot;.\u0026quot; )\r} else{\rbullets \u0026lt;- str_c( bullets, str_c( which_contrasts[-ndiff], collapse=\u0026quot;, \u0026quot; ), \u0026quot;, and between \u0026quot;, which_contrasts[ndiff], \u0026quot;.\u0026quot; )\r}\r} else{\rbullets \u0026lt;- str_c( \u0026quot;- Pairwise comparisons conducted using Tukey's method.\\n\u0026quot;,\r\u0026quot;- There were no significant differences in mean \u0026quot;, param_ii_nice, \u0026quot; detected.\u0026quot; )\r}\rcat( bullets )\r```\r:::\r::::::\r I\u0026rsquo;ll leave you to inspect the logic creating the sentences. The basic idea is to create a string which has Markdown syntax and then print it using the chunk option results=\u0026quot;asis\u0026quot;. You can get fancier, like incorporating the p-values or other results, and so on. But again, my point here is to provide a building block so that you can use it to do things that I haven\u0026rsquo;t even thought about.\nOne thing about this is that the iris dataset happens to show a difference in mean between all species and for all variables. If you want to see the logic changing the slides, either use a different dataset (you\u0026rsquo;ll naturally need to change the analysis codes!), or when you load the data, make some modification so that the means won\u0026rsquo;t differ. For example, you can permute one variable with the line: iris[[\u0026quot;Sepal.Length\u0026quot;]] \u0026lt;- sample( iris[[\u0026quot;Sepal.Length\u0026quot;]] )\nSo that\u0026rsquo;s it for this post. I hope it will give you a nice little trick to put in your back pocket for use in some project in the future.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596307830,"objectID":"d851dc4bd1d986028f4e029f7d7d5dab","permalink":"/post-stats/looping-powerpoint-slides-in-rmarkdown/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/post-stats/looping-powerpoint-slides-in-rmarkdown/","section":"post-stats","summary":"Preface For this post, I\u0026rsquo;m going to assume you are fairly familiar with RMarkdown. If not, you may want to start with RMarkdown: The Definitive Guide, or R for Data Science. The short version is that RMarkdown is a flavor of the markup language Markdown, which uses plain-text formatting and can be rendered into a variety of output types, including PDF, MS Word, HTML, and others. With RMarkdown, you can incorporate R \u0026ldquo;code chunks\u0026rdquo; to create a fully reproducible document.","tags":["powerpoint","loop","slides","child document","rmarkdown"],"title":"Looping PowerPoint Slides in RMarkdown ","type":"post-stats"},{"authors":[],"categories":[],"content":"The short version When encountering scientific claims - or really any objective claim - it can be difficult to know what to believe, especially when there seems to be conflicting stories. In the world of COVID-19 shutdowns and stay-at-home orders, it can be hard to parse through the deluge of information and figure out what\u0026rsquo;s going on. The following set of questions can serve as a sort of \u0026ldquo;smell test\u0026rdquo; to get an initial sense of the validity of the claims at hand.\n Who or what is the source? Why should we listen to them? Are they citing sources or presenting data? What is their expertise? Are the claims extravagant? Does the source have a known bias?  The long version The COVID-19 pandemic rather abruptly thrust science into peoples\u0026rsquo; faces. In a very short timeframe we went from ordinary news, to \u0026ldquo;There\u0026rsquo;s an outbreak in China,\u0026rdquo; to \u0026ldquo;It\u0026rsquo;s spreading worldwide,\u0026rdquo; to \u0026ldquo;It\u0026rsquo;s here and governors are issuing expansive shutdowns and stay-at-home orders.\u0026rdquo; People were suddenly inundated statistical claims and scientific results (as of July 20, 2020, Nature reported there to be 65,470 scientific papers on COVID-19).\nThe problem is that most people are not statisticians or epidemiologists. Most people are not microbiologists, immunologists, or researchers in a slew of other biomedical fields. Most people are not highly trained in research methods and reading the results.\nA constantly updating deluge of technical information that most people have a difficult time understanding creates a prime situation for misinformation, confirmation bias, and conspiracy theories to propagate. We have seen much of this in the course of the COVID-19 pandemic.\nMy intent in this post is to give a few questions you can ask yourself to assess new information - whether it\u0026rsquo;s a scientific article, a news report, or a post on social media. My set of questions is not exhaustive, but represents a fairly good first pass to sort out what is reliable from what is not. Many of the claims that I\u0026rsquo;ve seen posted across social media fail at least one, if not several or all, of the questions on my list.\n Question 1: Who or what is the source, and why should we listen to them? When it comes to objective and scientific claims, I think there are two main things to pay attention to: Evidence and expertise. If you can\u0026rsquo;t identify at least one of these things, there is no reason to believe the source. It doesn\u0026rsquo;t matter if something \u0026ldquo;just makes sense\u0026rdquo; or if someone\u0026rsquo;s \u0026ldquo;gut feeling\u0026rdquo; leans a certain way. That\u0026rsquo;s valid for subjective matters, but not for assessing scientific results.\nSo from the get-go, ask yourself: Is there evidence being cited, or does this person have expertise? If the answers are \u0026ldquo;No\u0026rdquo; and \u0026ldquo;No,\u0026rdquo; then you should take anything they are claiming with a very large grain of salt.\nQuestion 2a: Are they providing evidence? There is a saying among statisticians: \u0026ldquo;In God we trust, all others bring data.\u0026rdquo; Probably a bit more common is another saying - called Hitchens\u0026rsquo;s razor: \u0026ldquo;What can be asserted without evidence can also be dismissed without evidence.\u0026rdquo; The point with both of these is that evidence reigns supreme.\nThat being said, simply presenting data does not in and of itself validate a claim. Data needs to be interpreted, and interpreting data is not always often not straightforward. There could be missing context from the subject-matter domain, technical language that could be easily misinterpreted, different \u0026ldquo;levels\u0026rdquo; of data quality, or other issues complicating the understanding of evidence.\nSome might not understand the different levels of evidence. Some more formalized thoughts are available from Burns, Rohrich, and Chung (2011) or Glasofer and Townsend (2019). To summarize a bit from these:\n Tier 1: Systematic review of Tier 2 studies. Tier 2: Carefully designed experiments, including randomization and control. In medical fields, this should include blinding (preferably double-blind), and would often be dscribed as a randomized controlled trial (RCT). Tier 3: Non-experimental studies (including observational studies). These can be subject to various biases. Tier 4: Expert opinion, case studies (i.e., just looking at one or a few individual subjects).  I\u0026rsquo;m sure people could easily expand this list, or make some small adjustments. The point here is not to be comprehensive or \u0026ldquo;final,\u0026rdquo; it\u0026rsquo;s to provide a quick and dirty way to be able to think about evidence being presented. So if you see two seemingly contradictory results, instead of picking the one that you like the best, you might be able to think about the data quality from each study, and hence assess which result is based on the more reliable data.\nThis is one way in which expertise helps: Experts have been trained and have experience interpreting data from their field. They generally have a better understanding of data quality, of what the data might say, and of what data are important to consider. That last bit can be important: A person outside the field might point to some data and say \u0026ldquo;Look at X, this is big!\u0026rdquo; But a trained and experienced professional in the field might say, \u0026ldquo;Well, X isn\u0026rsquo;t really that important, we should be talking about Y instead.\u0026rdquo;\nFor example, suppose we were talking about the dangers of an electrical shock. Someone might say, \u0026ldquo;I sustained a 1,000 volt shock and I was just fine.\u0026rdquo; But someone who knows a bit more could retort \u0026ldquo;Okay, but voltage isn\u0026rsquo;t the important thing, amperage is much more relevant here.\u0026rdquo;\nIn general, I\u0026rsquo;d rank the \u0026ldquo;reliability\u0026rdquo; of claims as:\n Expert who is citing good evidence: Very high. Expert who is citing weak evidence: Good, pending the precise nature of the data. Expert speaking about their field, but not citing evidence: Good, but with reservations. Even experts can be mistaken, or have lost their credibility as did the former scientist featured in \u0026ldquo;Plandemic.\u0026rdquo; Non-expert citing evidence: Good, but with reservations, depending on quality of evidence and appropriateness of interpretation. Non-expert without evidence: Low, no reason to give them the time of day.  Question 2b: What is their expertise? Now, just because someone has expertise doesn\u0026rsquo;t mean they are automatically a reliable source. The field of expertise also matters. I myself am a statistician, I can speak to statistical matters. Statistics is somewhat unique in that it is the language of science, permeating the scientific endeavor: Researchers need to communicate their results, and that communication typically involves statistical experimentation and analysis. As John Tukey put it, statisticians \u0026ldquo;get to play in everyone\u0026rsquo;s backyard\u0026rdquo;, meaning we get exposed to and pick up a bit of various topics. We might not be experts in the field, but we can usually understand and assess the results.\nBut you shouldn\u0026rsquo;t ask me to describe why or how a given protein does what it does. You shouldn\u0026rsquo;t ask me to describe the mechanism by which a medication functions. Or why some molecule will or will not react with another, or how the gravity of the sun and planets interact with each other in the solar system. Similarly, you should not ask me to run electrical wiring for a house or to repair a car engine.\nMy point here is that specialty matters. Nobody is an expert in all things. Some fields are closely related, so an expert in one may be highly competent in another, but many fields are exceedingly diverse. So it\u0026rsquo;s important to keep someone\u0026rsquo;s specialty in mind when listening to what they have to say.\nI\u0026rsquo;ve seen a number of times people quote physicians (MD or DO) when talking about COVID-19. That\u0026rsquo;s perfectly fine, when the topic is a clinical matter. Just as being a statistician doesn\u0026rsquo;t make me an expert in medicine, having a medical degree doesn\u0026rsquo;t make one an expert in statistics or epidemiology. Just because a topic is related in some way to the medical field does not make physicians the foremost experts.\nOne job that I\u0026rsquo;ve held has been as a professor in a biostatistics department. I\u0026rsquo;ve collaborated with MDs and residents, and I\u0026rsquo;ve taught aspiring doctors. Some of the doctors had a solid grasp of statistical methods. Most did not. Fewer still should conduct their own statistical analysis. The students aiming for med school often took one or two courses in statistics, not enough to offer a comprehensive understanding statistics, much less the ability to properly conduct statistical analysis. Some were diligent students that probably retained the ideas of the course past the end of the semester. But most were (due to the competitiveness of med school applications) were hyper-focused on grades rather than understanding, and likely retained little if anything that we talked about after the final exam.\nTo be sure, there are some MDs who are excellent researchers, Dr. Anthony Fauci is a prime example. But when talking about a statistical or epidemiological analysis an MD is not automatically an expert, they may very well be talking outside of their domain of expertise.\nQuestion 4: Are the claims extravagant? As Carl Sagan said, \u0026ldquo;Extraordinary claims require extraordinary evidence.\u0026rdquo; There are sometimes dramatic breakthroughs, but usually results are much more mundane or incremental in nature. If you see a headline or claim that seems too good to be true, it\u0026rsquo;s probably either misleading clickbait, misinformation, or conspiracy.\nForbes did an article about this subject, you can find it here. The Atlantic did a piece on Dr. John Ioannidis which can be read here. In it, Dr. Ioannidis is quoted to say:\n Often the claims made by studies are so extravagant that you can immediately cross them out without needing to know much about the specific problems with the studies\n Even scientists can be guilty of over-hyping their conclusions. Press releases may exaggurate a bit more, media outlets take it up another few notches, and people on social media blow it up to cosmic proportions.\nQuestion 5: Does the source have a known bias? Biased sources are likely to selectively interpret results, and either overlook or deliberately obscure flaws. That\u0026rsquo;s why a lot of research has a statement about funding sources. Remember those doctors from Bakersfield, California? They run a private urgent care facilities. They had an enormous conflict of interest: Personal profit. If they did not have that incredible bias, they may have been more reserved about their comments, and perhaps noticed the dramatic statistical and scientific errors they were making, and for which they were condemned by two major medical associations, the American College of Emergency Physicians and the American Academy of Emergency Medicine.\nA financial bias is not the only bias out there. Various news outlets have a strong bias, and will omit certain facts, or selectively choose stories that spin a particular narrative. If a source is known to have a strong bias (be it right or left), then caution should be exercised in assessing the claims being made. This goes doubly so when the source itself acknowledges that it is biased, which ties back to the first question: Who is the source and why should we believe them? If the source admits they are biased, we should be skeptical of anything they have to say.\nThat\u0026rsquo;s not to say that a source can\u0026rsquo;t be biased without acknowleding it, the bias might just be more difficult to verify. My point is that if the source is already overlty admitting a bias then you should from the get-go not expect a neutral or objective consideration of the topic. And if that\u0026rsquo;s the case, then you should again take any conclusion with a large grain of salt.\n So that\u0026rsquo;s it. The first real post I put on my website, and what I think is a solid set of questions to consider when faced with some scientific claim.\n","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596233884,"objectID":"92f5a8417c8fe0fa6ead5a2fd18b57db","permalink":"/post-other/a-scientific-smell-test/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/post-other/a-scientific-smell-test/","section":"post-other","summary":"The short version When encountering scientific claims - or really any objective claim - it can be difficult to know what to believe, especially when there seems to be conflicting stories. In the world of COVID-19 shutdowns and stay-at-home orders, it can be hard to parse through the deluge of information and figure out what\u0026rsquo;s going on. The following set of questions can serve as a sort of \u0026ldquo;smell test\u0026rdquo; to get an initial sense of the validity of the claims at hand.","tags":["covid","coronavirus","science","evidence"],"title":"A Scientific Smell Test","type":"post-other"},{"authors":[],"categories":["general"],"content":"Welcome! If you\u0026rsquo;ve found your way to this page \u0026hellip; well, I\u0026rsquo;m more than a little surprised. This is my personal website. I\u0026rsquo;m not entirely sure what I\u0026rsquo;ll be doing here. My plan at the moment is to use it mainly for:\n Listing professional things like publications independently of my place of employment. Host the slides for talks or workshops that I give.  I may have some posts (is that \u0026lsquo;blogging\u0026rsquo;? Save me!) about topics that pique my interest. This would likely be my thoughts on some bit of Statistics or science that came across my screen.\nI\u0026rsquo;ve been talking about science and statistics in the context of COVID-19 on social media for a while now. I may transition some of that here, since social media isn\u0026rsquo;t usually conducive to the long-form comments I make often enough.\n","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594839175,"objectID":"3b462785c865e5125b0dd0f932deec72","permalink":"/post-other/welcome-to-my-site/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/post-other/welcome-to-my-site/","section":"post-other","summary":"Welcome! If you\u0026rsquo;ve found your way to this page \u0026hellip; well, I\u0026rsquo;m more than a little surprised. This is my personal website. I\u0026rsquo;m not entirely sure what I\u0026rsquo;ll be doing here. My plan at the moment is to use it mainly for:\n Listing professional things like publications independently of my place of employment. Host the slides for talks or workshops that I give.  I may have some posts (is that \u0026lsquo;blogging\u0026rsquo;?","tags":["greetings","welcome"],"title":"Welcome to my site","type":"post-other"},{"authors":["Casey Jelsema","Rajib Paul","Joseph W McKean"],"categories":["publication","methods"],"content":"","date":1590883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590883200,"objectID":"89aa3b96937486d2635c159123cc612d","permalink":"/publication/2020-robust-v/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/2020-robust-v/","section":"publication","summary":"For large datasets, spatial covariances are often modeled using basis functions and covariance of a reduced dimensional latent spatial process. For skewed data, likelihood based approaches with Gaussian assumption may not lead to faithful inference. Any $L_{2}$ norm based estimation is susceptible to long tails and outliers due to contamination. Our method is based on an empirical binned covariance matrix using the median absolute deviation and minimizes $L_{1}$ norm between empirical covariance and the model covariance. The consistency of the proposed estimate is established theoretically. The improvement is demonstrated using simulated data and cloud data obtained from NASA's Terra satellite.","tags":["spatial"],"title":"Robust estimation of reduced rank models to large spatial datasets","type":"publication"},{"authors":[],"categories":["r package"],"content":"CLME stands for Constrained Linear Mixed Effects. I wrote this R package (CRAN link) during my postdoctoral work at NIEHS.\nThe fundamental idea is similar to the Jonckheere–Terpstra or any other test for ordered alternatives: If the treatment groups are ordinal, then a trend of some sort may be of interest. If a researcher has such a hypothesis, they can not only test for the ordered alternative, but they can constrain the estimation to respect the order from the alternative hypothesis. This results in getting more power than a comparable test that does not impose constraints or test for an order (e.g., ANOVA).\nFeel free to check out the github repository. There are a variety of improvements I\u0026rsquo;d like to make, including cleaning up my code, removing dependancies, and adding some features. Just need time to get to them. If you\u0026rsquo;d like to join, give me a shout!\nDisclaimer: This is really just a post to get something into the \u0026ldquo;Project\u0026rdquo; space of my website. I\u0026rsquo;ll probably modify this post later to add clarity.\n","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565797783,"objectID":"1cf66225e7593c17a980f29de3004b4b","permalink":"/project/clme/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/project/clme/","section":"project","summary":"CLME stands for Constrained Linear Mixed Effects. I wrote this R package (CRAN link) during my postdoctoral work at NIEHS.\nThe fundamental idea is similar to the Jonckheere–Terpstra or any other test for ordered alternatives: If the treatment groups are ordinal, then a trend of some sort may be of interest. If a researcher has such a hypothesis, they can not only test for the ordered alternative, but they can constrain the estimation to respect the order from the alternative hypothesis.","tags":["package","order restricted inference","bootstrap"],"title":"CLME","type":"project"},{"authors":[],"categories":["lecture","talk"],"content":"\rThe slides for my Introduction to Spatial Data (geared for non-Statisticians) can be found here:\nhttps://jelsema.github.io/presentations/2019-intro-spatial/intro_spatial.html#1\n","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565794885,"objectID":"7484acca2de3c888eb0a9ddec9379035","permalink":"/talk/wvu-bios-604-intro-to-spatial/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/talk/wvu-bios-604-intro-to-spatial/","section":"talk","summary":"The slides for my Introduction to Spatial Data (geared for non-Statisticians) can be found here:\nhttps://jelsema.github.io/presentations/2019-intro-spatial/intro_spatial.html#1","tags":["spatial","intro"],"title":"WVU BIOS 604 Intro to Spatial","type":"talk"},{"authors":["Casey Jelsema","Richard Kwok","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"fd1c3abfdd33965446ec4475cad4126e","permalink":"/publication/2019-threshold-knots/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2019-threshold-knots/","section":"publication","summary":"Large spatial datasets are typically modelled through a small set of knot locations; often these locations are specified by the investigator by arbitrary criteria. Existing methods of estimating the locations of knots assume their number is known a priori, or are otherwise computationally intensive. We develop a computationally efficient method of estimating both the location and number of knots for spatial mixed effects models. Our proposed algorithm, Threshold Knot Selection (TKS), estimates knot locations by identifying clusters of large residuals and placing a knot in the centroid of those clusters. We conduct a simulation study showing TKS in relation to several comparable methods of estimating knot locations. Our case study utilizes data of particulate matter concentrations collected during the course of the response and clean-up effort from the 2010 *Deepwater Horizon* oil spill in the Gulf of Mexico.","tags":["spatial"],"title":"Threshold knot selection for large-scale spatial models with applications to the Deepwater Horizon disaster","type":"publication"},{"authors":["Eric Law","Keith Morris","Casey Jelsema"],"categories":["publication","methods"],"content":"","date":1529280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529280000,"objectID":"11de56111c7b98c52114236295bf78e2","permalink":"/publication/2018-test-fire2/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-test-fire2/","section":"publication","summary":"The Association of Firearm and Toolmark Examiners recommends a minimum of two test fires be performed when an unknown firearm is submitted to a laboratory prior to doing a comparison with a cartridge case collected from a crime scene. Limited research has been performed to determine how many test fires are necessary to be representative of the match distribution of a firearm. Various makes and models of firearms comprising five calibers were tested using a hybrid equivalence test to determine how many cartridge cases were required to represent the match distribution of an unknown firearm based on both breech face and firing pin correlation scores from an IBIS® Heritage^(TM) System. The same general trend was observed for each caliber of firearm where the equivalence percentage increased from 10 to 30 cartridge cases. Overall, 15 cartridge cases are sufficient for above an 80% probability of representing the full match distribution for an unknown firearm. To approach full equivalence, 25 cartridge cases are enough because 30 cartridge cases were not found to be significantly higher in equivalence percentage for any caliber of firearm tested.","tags":["monte carlo","equivalence testing","forensics","test fires"],"title":"Determining the number of test fires needed to represent the variability present within firearms of various calibers","type":"publication"},{"authors":["Ori Davidov","Casey Jelsema","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"daa2d98e4f42f496756703976902fb26","permalink":"/publication/2018-osin/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-osin/","section":"publication","summary":"There are many applications in which a statistic follows, at least asymptotically, a normal distribution with a singular or nearly singular variance matrix. A classic example occurs in linear regression models under multicollinearity but there are many more such examples. There is well-developed theory for testing linear equality constraints when the alternative is two-sided and the variance matrix is either singular or nonsingular. In recent years, there is considerable, and growing, interest in developing methods for situations in which the estimated variance matrix is nearly singular. However, there is no corresponding methodology for addressing one-sided, that is, constrained or ordered alternatives. In this article, we develop a unified framework for analyzing such problems. Our approach may be viewed as the trimming or winsorizing of the eigenvalues of the corresponding variance matrix. The proposed methodology is applicable to a wide range of scientific problems and to a variety of statistical models in which inequality constraints arise. We illustrate the methodology using data from a gene expression microarray experiment obtained from the NIEHS’ Fibroid Growth Study. Supplementary materials for this article are available online.","tags":["order restricted inference","bootstrap","singular"],"title":"Testing for Inequality Constraints in Singular Models by Trimming or Winsorizing the Variance Matrix","type":"publication"},{"authors":["Amy Hessl","Kevin Anchukaitis","Casey Jelsema","Benjamin Cook","Oyunsanaa Byambasuren","Caroline Leland","Baatarbileg Nachin","Neil Pederson","Hanqin Tian","Laia Andreu Hayles"],"categories":["publication","collaborative"],"content":"","date":1520985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520985600,"objectID":"ef5f07c837121252d69a8f0686615cb5","permalink":"/publication/2018-mongolia-drought/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-mongolia-drought/","section":"publication","summary":"The severity of recent droughts in semiarid regions is increasingly attributed to anthropogenic climate change, but it is unclear whether these moisture anomalies exceed those of the past and how past variability compares to future projections. On the Mongolian Plateau, a recent decade-long drought that exceeded the variability in the instrumental record was associated with economic, social, and environmental change. We evaluate this drought using an annual reconstruction of the Palmer Drought Severity Index (PDSI) spanning the last 2060 years in concert with simulations of past and future drought through the year 2100 CE. We show that although the most recent drought and pluvial were highly unusual in the last 2000 years, exceeding the 900-year return interval in both cases, these events were not unprecedented in the 2060-year reconstruction, and events of similar duration and severity occur in paleoclimate, historical, and future climate simulations. The Community Earth System Model (CESM) ensemble suggests a drying trend until at least the middle of the 21st century, when this trend reverses as a consequence of elevated precipitation. Although the potential direct effects of elevated CO2 on plant water use efficiency exacerbate uncertainties about future hydroclimate trends, these results suggest that future drought projections for Mongolia are unlikely to exceed those of the last two millennia, despite projected warming.","tags":["copula","geography","drought","mongolia"],"title":"Past and future drought in Mongolia","type":"publication"},{"authors":["Eric Law","Keith Morris","Casey Jelsema"],"categories":["publication","methods"],"content":"","date":1493769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493769600,"objectID":"6160d3ede8397d566b4b0f1ba430cd2f","permalink":"/publication/2017-test-fire1/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2017-test-fire1/","section":"publication","summary":"Many studies have been performed in recent years in the field of firearm examination with the goal of providing an objective method for comparisons of fired cartridge cases. No published research to support the number of test fires needed to represent the variability present within the impressions left on a cartridge case could be found. When a suspect firearm is submitted to a firearm examiner, typically two to four test fires are performed. The recovered cartridge cases are compared to each other to determine which characteristics from the firearm are reproducing, and then compared to any cartridge cases collected at a crime scene. The aim of this research was to determine the number of test fires examiners should perform when a suspect firearm is submitted to the lab to balance cartridge case acquisition time with performance accuracy. Each firearm in the IBIS® database at West Virginia University® is represented by approximately 100 fired cartridge case entries. Random samples of cartridge cases were taken separately from the breech face match score and firing pin match score lists. This subset was compared to the total match distribution of the firearm using a hybrid equivalence test to determine if the subset of similarity scores were statistically equivalent to the larger distribution of scores. For the sampled distribution to remain above 80% equivalent to the match distribution, a minimum of 15 cartridge cases should be used to model the match distribution, based on IBIS® scores. Thirty cartridge cases is a conservative estimate, allowing one to determine that the location and dispersion of the match and sampling distributions are equivalent with nearly 100% probability.","tags":["monte carlo","equivalence testing","forensics","test fires"],"title":"Determining the number of test fires needed to represent the variability present within 9mm Luger firearms","type":"publication"},{"authors":["Casey Jelsema","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1479513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479513600,"objectID":"b39d1577f8cf960898d166be326f8ad4","permalink":"/publication/2016-clme/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2016-clme/","section":"publication","summary":"In many applications researchers are typically interested in testing for inequality constraints in the context of linear fixed effects and mixed effects models. Although there exists a large body of literature for performing statistical inference under inequality constraints, user friendly statistical software implementing such methods is lacking, especially in the context of linear fixed and mixed effects models. In this article we introduce **CLME**, a package in the **R** language that can be used for testing a broad collection of inequality constraints. It uses residual bootstrap based methodology which is reasonably robust to non-normality as well as heteroscedasticity. The package is illustrated using two data sets. The package also contains a graphical user interface built using the shiny package.","tags":["order restricted inference","bootstrap"],"title":"CLME An R package for linear mixed effects models under inequality constraints","type":"publication"},{"authors":["Thomas (Joost) Van't Erve","Fred Lih","Casey Jelsema","Leesa Deterding","Thomas Eling","Ronald Mason","Maria Kadiiska"],"categories":["publication","collaborative"],"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"d9c3553108ffc3099a82e9d384d547fc","permalink":"/publication/2016-joost/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2016-joost/","section":"publication","summary":"Oxidative stress is elevated in numerous environmental exposures and diseases. Millions of dollars have been spent to try to ameliorate this damaging process using anti-oxidant therapies. Currently, the best accepted biomarker of oxidative stress is the lipid oxidation product 8-iso-prostaglandin F2α (8-iso-PGF2α), which has been measured in over a thousand human and animal studies. 8-iso-PGF2α generation has been exclusively attributed to nonenzymatic chemical lipid peroxidation (CLP). However, 8-iso-PGF2α can also be produced enzymatically by prostaglandin-endoperoxide synthases (PGHS) in vivo. When failing to account for PGHS-dependent generation, 8-iso-PGF2α cannot be interpreted as a selective biomarker of oxidative stress. We investigated the formation of 8-iso-PGF2α in rats exposed to carbon tetrachloride (CCl4) or lipopolysaccharide (LPS) using the 8-iso-PGF2α/PGF2α ratio to quantitatively determine the source(s) of 8-iso-PGF2α. Upon exposure to a 120mg/kg dose of CCl4, the contribution of CLP accounted for only 55.6±19.4% of measured 8-iso-PGF2α, whereas in the 1200mg/kg dose, CLP was the predominant source of 8-iso-PGF2α (86.6±8.0% of total). In contrast to CCl4, exposure to 0.5mg/kg LPS was characterized by a significant increase in both the contribution of PGHS (59.5±7.0) and CLP (40.5±14.0%). In conclusion, significant generation of 8-iso-PGF2α occurs through enzymatic as well as chemical lipid peroxidation. The distribution of the contribution is dependent on the exposure agent as well as the dose. The 8-iso-PGF2α/PGF2α ratio accurately determines the source of 8-iso-PGF2α and provides an absolute measure of oxidative stress in vivo.","tags":["order restricted inference","bootstrap"],"title":"Reinterpreting the best biomarker of oxidative stress: The 8-iso-prostaglandin F2α/prostaglandin F2α ratio shows complex origins of lipid peroxidation biomarkers in animal models","type":"publication"},{"authors":["Rajib Paul","Casey Jelsema","Rex Lau"],"categories":["publication","methods"],"content":"","date":1432166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1432166400,"objectID":"ad6c9f50ae37a69c5c44e910231fbc56","permalink":"/publication/2015-frssm/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2015-frssm/","section":"publication","summary":"In environmental studies, the datasets exhibiting non-Gaussian properties, such as heavier or lighter tails and multimodality, are very common. The research on dealing with such datasets in reduced rank perspectives is very limited. In this chapter, a flexible class of Bayesian reduced rank spatial model is developed that can handle non-Gaussian properties adequately. The spatial model provides the flexibility to deal with such properties through scale mixtures of Gaussian distributions and a two-level marginally noninformative inverse-Wishart prior. A general framework for posterior summaries based on Markov Chain Central Limit Theorem (MCCLT) has been developed and conditions of MCCLT on ergodic averages are theoretically verified. The Monte Carlo standard errors based on MCCLT are computed using batch-mean method. The performance of the proposed model and method are assessed using several simulated datasets and a dataset on daily maximum of total column ozone obtained from National Aeronautic and Space Administration Terra satellite.","tags":["spatial"],"title":"A flexible class of reduced rank spatial models for large non-Gaussian datasets","type":"publication"},{"authors":["Casey Jelsema","Rajib Paul"],"categories":["publication","methods"],"content":"","date":1375142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375142400,"objectID":"ed3635726a97802e43fbe1de050476a1","permalink":"/publication/2013-lognormal-coal/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2013-lognormal-coal/","section":"publication","summary":"We analyze data on the geochemical make-up of coal samples throughout the state of Illinois. The goal is to estimate the geochemical properties at unobserved locations over a specified region. Multivariate spatial modeling requires characterization of both spatial and cross-spatial covariances. Reduced rank spatial models are popular in analyzing large spatial datasets. We develop a multivariate spatial mixed effects model for log-normal processes and show how to implement with compositional data to predict on point locations, as well as the average prediction over a finite area. We use log-normal kriging for the components of compositional data, and show how to obtain estimates and measures of precision in isometric log-ratio coordinates.","tags":["spatial"],"title":"Lognormal block kriging with applications to goal geology","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"Experience / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Experience / Contact","type":"widget_page"}]