[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a Statistician, currently working as part of a statistics group supporting an engineering research lab. My main research interests are spatial statistics and statistical computing, but I also enjoy \u0026ldquo;playing in everyone else\u0026rsquo;s backyard\u0026rdquo; (i.e. collaborations). Some of the backyards I\u0026rsquo;ve played in - from my previous gig as an assistant professor at West Virginia University - include Forensics and medical applications.\n","date":1590883200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1590883200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a Statistician, currently working as part of a statistics group supporting an engineering research lab. My main research interests are spatial statistics and statistical computing, but I also enjoy \u0026ldquo;playing in everyone else\u0026rsquo;s backyard\u0026rdquo; (i.e. collaborations). Some of the backyards I\u0026rsquo;ve played in - from my previous gig as an assistant professor at West Virginia University - include Forensics and medical applications.","tags":null,"title":"Casey Jelsema","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["voting","elections"],"content":"\rIntroduction\rThis is part 1 of a 3-part bit on voting and elections.\nFirst of all, a disclaimer: I’m not a political scientist, I am not an expert on the constitution or any other relevant field. I’m commenting here just as an individual. These ideas are either my own, or ideas that I’ve come across and like. This is very much an “armchair expert” situation. That being said, I’d like to share my thoughts on the electoral process in the United States, and an idea for alternatives.\nA YouTuber/podcaster who goes by CGP Grey has done a series of five short videos, titled\rPolitics in the Animal Kingdom that I think does a fantastic job explaining methods of voting and some problems that arise with some of them. In this post, I’ll be summarizing some of these voting methods, but I highly recommend watching those videos.\nInitially this may seem odd to some people. Voting is voting, right? You cast your ballot for who you support, and then whoever gets the most votes wins. While that is one way of voting, it is not the only way (and actually has some significant drawbacks).\n\rPlurality voting\rThe current approach used in most races in the United States uses a plurality system\r(sometimes called a first-past-the-post (FPTP) system) of voting.\rEveryone votes for a single candidate, and to determine the winner we just look at who got the most votes. They win even if they didn’t get a majority. In case that seems odd, it’s because a majority would be over 50% of votes, while a plurality just means a larger percentage than any other individual candidate.\nFor example, in a race with 3 candidate (Calvin, Hobbes, and Suzie), suppose Calvin gets 40% of the vote, Hobbes gets 35%, and Suzie gets 25%. Calvin would win by plurality, even though he did not get a majority. Even if all of Suzie’s voters preferred Hobbes to Calvin (so that, if Suzie was not in the race, Hobbes would get 60% - a majority), Calvin still wins.\nThis is how virtually every senator and representative in the country is selected (Maine was the first to use a different method in the 2018 election, and Alaska approved a different method in the 2020 election). There are several problems with plurality voting.\n\rProblems with plurality\rNo majority\rFirst, as demonstrated in the little example above, a plurality system does not guarantee that the winner gets a majority of votes, or is the most broadly preferred candidate (which casts a shadow on the concept of “majority rule”).\n\rDuopoly\rSecond, a plurality system tends to lead to a 2-party system, which we’ve seen for a long time now. If you don’t like the 2-party system, then you should hate a FPTP system of voting.\nThe example hinted at this: If all of Suzie’s supporters would have preferred Hobbes to Calvin, then eventually they will grow tired of losing elections to their least-favorite candidate. Instead of voting for their favorite, they’ll vote a candidate that is still acceptable to them, but more likely to win. This is known as Duverger’s law, and it is how we wind up with two “big tent” parties, and why third parties are all but invisible: Since they are not likely to win, people who agree with the third party will instead vote for one of the two major-party candidates, making it even less likely for the third party to win, which leads even more supporters to vote for the main parties.\nTo be clear: I do not subscribe to the notion that a third party vote is a “wasted” vote. However, I think that it needs to be understood as a protest vote - an indication that there are engaged voters who are unsatisfied with both main parties - rather than a real attempt to elect a candidate into office.\n\rGerrymandering\rA third problem with plurality voting is the prospect of gerrymandering. I don’t think that gerrymandering is technically limited to plurality voting, but plurality voting in single-member districts (FPTP) is particularly susceptible to it.\nTo back up slightly: While it may seem like there is “an election,” in reality it’s more like 534 separate elections: 1 president, 100 senators, and 435 house representatives. Well, sort of, only 1/3 of the senate is up for election at any given election, but that doesn’t really matter for this discussion.\nSenators are elected from a state-wide vote, but house representatives are elected from a district within a state. These districts are not fixed things. Within a state they are supposed to be approximately equal population, and so as the population changes, the districts may change. Gerrymandering, also called “cracking and packing”, is the tactic by which these districts are drawn to favor one part or the other. The basic idea is that you “crack” the opposing party’s support across many districts, to dilute its effect. If it’s impossible to fully achieve that, you “pack” as many of the opposing party into a single district as possible, so that they are guaranteed to win that district, but with many of their votes concentrated there, they are weaker in multiple other districts.\nConsider a square state with 81 voters who need to be divvied up into 9 districts, each with 9 voters. These voters vote for one of two parties, Blue or Gold. In the figure below, I’ve randomly distributed the votes around the state. The vote tallies are as evenly divided as possible: 40 for Blue, and 41 for Gold.\nNow, we need to draw districts. With no knowledge of this fictional state, we might just draw 9 boxes, no? That would be one approach. It turns out that doing so, the parties each win roughly half of the districts, which seems more or less fair (and, since I distributed the votes randomly, should be expected).\nHowever, say the Blue party gets to draw the districts, and they decide to gerrymander to give themselves an advantage. Their target should be to create as many districts with 5 Blue votes as possible.\nHow might they do that? Well, I just came up with some gerrymandered lines myself. I noticed a block of Gold votes that was all connected, so I put them ALL into a single district (“packing”). So Gold gets that district. But that siphons off a lot of Gold votes, and I can spread the rest of them out so that Blue gets all of the remaining districts.\nSo even though they only received roughly (and actually just under) 50% of the vote, Blue was able to claim nearly all but one district, for roughly 90% of the representation. That seems wildly unfair. And I’m not even an expert at this! I just looked at this map for a little bit and drew some lines.\n\r\rSummary\rSo, voting systems that can be gerrymandered have this big flaw. We’d prefer a system that is immune to gerrymandering. You can continue reading my take on some alternative methods in Voting Methods Part 2.\n\r","date":1605916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606003898,"objectID":"c94fae3f44f79c42549459a3cffbfd11","permalink":"/post-other/voting-methods-part-1-fptp/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/post-other/voting-methods-part-1-fptp/","section":"post-other","summary":"Introduction\rThis is part 1 of a 3-part bit on voting and elections.\nFirst of all, a disclaimer: I’m not a political scientist, I am not an expert on the constitution or any other relevant field. I’m commenting here just as an individual. These ideas are either my own, or ideas that I’ve come across and like. This is very much an “armchair expert” situation. That being said, I’d like to share my thoughts on the electoral process in the United States, and an idea for alternatives.","tags":["voting","elections"],"title":"Voting methods part 1: FPTP","type":"post-other"},{"authors":[],"categories":["voting","elections"],"content":"\r\rIntroduction\rThis is part 2 a 3-part bit on voting and elections.\nAs before, the disclaimer that I’m not a political scientist, I am not an expert on the constitution or any other relevant field. I’m commenting here just as an individual. Also, a reminder that CGP Grey’s videos Politics in the Animal Kingdom are fantastic, and a much better explanation that I give here.\n\rAlternative voting systems\rIn Voting Methods Part 1 I talked about FPTP / plurality and some problems such as gerrymandering that can arise. So if we don’t like a system like that, what are some alternative methods that might address these problems?\nRanked choice voting\rCurrently, most Americans vote by selecting one candidate from a set as “The person they vote for.” But that’s not the only way to go about it. Back to the example with Calvin, Hobbes, and Suzie, every Suzie voter preferred Hobbes over Calvin. We could then say that their preference order was: Suzie -\u0026gt; Hobbes -\u0026gt; Calvin. So they could fill out their ballot by ranking these candidates, e.g.\nRank\nCandidate\n3\nCalvin\n2\nHobbes\n1\nSuzie\n\rThis way, when counting ballots, if the voter’s preferred candidate (in this case, Suzie) does not win, their vote goes to the second most preferred candidate, in this case Hobbes. In this way, it guards against the “wasted vote” because if you vote for someone who has a low chance of winning (like a third party), your vote will still contribute to who ultimately wins the election.\nThere are multiple ways to tally the votes. One way is known as the Borda count (though there are actual multiple ways to implement a Borda count). In the Borda count, each position is assigned some number of points. For a 3-person race, we might assign 3 points for a 1st-place rank, 2 points for a 2nd-place rank, and 1 point for a 3rd-place rank.\nBorda count: Example 1\rGoing back to the example, we’d have a situation that looks like this:\nVotes\nCHS\nHSC\nSHC\nProportion\n40%\n35%\n25%\nRank 1\nCalvin\nHobbes\nSuzie\nRank 2\nHobbes\nSuzie\nHobbes\nRank 3\nSuzie\nCalvin\nCalvin\n\rThis may seem a bit odd. The three columns at the right denote the three unique rankings of candidates that were cast. There are three more possible rankings, I’ll consider that in a moment. For now, I’ve just extended the votes so that:\n\rAll of Calvin’s voters prefer Hobbes to Suzie\rAll of Hobbes’ voters prefer Suzie to Calvin.\rAll of Suzie’ voters prefer Hobbes to Calvin.\r\rJust by inspection, we can kind of see that Hobbes is a more broadly popular candidate, right? He’s ranked 1st or 2nd, while Calvin has only 1st and 3rd place votes. Applying the Borda count would be a weighted average of the points (1st = 3 points, 2nd = 2 points, 3rd = 1 point), using the proportion of votes at each rank as the weight. For example, to get Suzie’s borda count, we took a weighted average of the ranks:\n\\[\rBC_{S} = (0.40 \\times 1) + (0.35 \\times 2) + (0.25 \\times 3)\r\\]\rApplying the same to all of the candidates, we would get the following results:\nCandidate\nCount\nCalvin\n1.80\nHobbes\n2.35\nSuzie\n1.85\n\rAnd so Hobbes would be declared the winner of the election.\n\rBorda count: Example 2\rGoing to a slightly more complex ballot, let’s say that Calvin and Suzie are the “extreme” candidates, while Hobbes is a more centrist candidate. So Hobbes’ voters second choice are a bit more split, but Suzie’s and Calvin’s would primarily go to Hobbes.\nVotes\nCHS\nCSH\nHCS\nHSC\nSHC\nSCH\nProportion\n35%\n5%\n20%\n15%\n20%\n5%\nRank 1\nCalvin\nCalvin\nHobbes\nHobbes\nSuzie\nSuzie\nRank 2\nHobbes\nSuzie\nCalvin\nSuzie\nHobbes\nCalvin\nRank 3\nSuzie\nHobbes\nSuzie\nCalvin\nCalvin\nHobbes\n\rThen applying the Borda count, we would get:\nCandidate\nCount\nCalvin\n2.05\nHobbes\n2.25\nSuzie\n1.70\n\rSo while Calvin is now a bit more popular than Suzie, Hobbes still wins the election.\n\rBorda count: Example 3\rOne more example, just because I wrote a function to turn these ballots into a borda count, and I want to get my use out of it, let’s say that Hobbes is actually in last place when considering the first-place votes. To put that in real terms: People are voting for the “extreme” candidates of their party (Calvin with his G.R.O.S.S. club, Suzie with her tea parties), but have a more moderate/centrist as a backup choice (Hobbes just wanting a tuna sandwich for everyone).\nVotes\nCHS\nHCS\nHSC\nSHC\nProportion\n45%\n5%\n5%\n45%\nRank 1\nCalvin\nHobbes\nHobbes\nSuzie\nRank 2\nHobbes\nCalvin\nSuzie\nHobbes\nRank 3\nSuzie\nSuzie\nCalvin\nCalvin\n\rSo here, only 10% of voters put Hobbes first, while 90% voted for their preferred “extreme” candidate. But when we tally up the Borda count, we get:\nCandidate\nCount\nCalvin\n1.95\nHobbes\n2.10\nSuzie\n1.95\n\rSo Hobbes still wins the day, because enough votes from either side thought he was an acceptable backup.\nUsing a Borda count isn’t the only way to tally ranked ballots, but it’s an example of how to better represent the desires of the voters compared to plurality.\n\r\rProportional representation\rAnother method that I, at least currently, am interested in is proportional representation. There are multiple ways to implement this, such as the Single Transferable Vote or\rMixed-member proportional representation, and some of them can get a little complicated, but the basic idea is that the single-member districts gets pooled together, and the representatives are allocated to (approximately) align with the result of the vote.\nOne simple (possibly naive) way of implementing is to note how much percent a representative actually represents. So if there are 10 representatives to elect, then to be elected, a candidate needs to get 10% of the vote. If no candidates get to the 10% threshold, then the least-popular candidate is removed and (using ranked voting) their votes would be transferred to their next ranked candidate. The CGP Grey video on Single Transferable Vote has a nice demonstration of this.\nOne very attractive feature of this this method is that it renders gerrymandering impossible, because there are no longer districts to be redrawn: All representatives are elected from a single “district” (the state). It also enables some more diversity, possibly allowing a third party to get a representative or two. For example, in California with it’s 53 house representatives, each represents approximately 1.9% of voters, so depending on the implementation, a party might earn a representative by getting a (very realistic) 2% of the vote.\nSTV: Example\rLet’s say that a district needs 5 representatives - so each represents 20% of the vote - and the vote breakdown is shown below.\nClearly Blue and Gold get a representative, right? But then how are the other three representatives allocated? Nobody got 40%, so there’s not really an argument that Blue or Gold should get two representatives (yet). One way of going forward is to eliminate take the party that got the least votes and allocate them to their second choice. In this case, let’s say that Light Blue voters would have otherwise voted for Blue. So the new picture looks like:\nNow with 40% of the vote, it seems reasonable to give Blue two of the five representatives. But we still have two more, with Green and Purple being tied for last place. If they both would have opted for Gold otherwise, then we would add them to Gold’s tally, as shown below.\nNow it seems rational to give Blue two representatives, and give Gold three representatives, thus proportionally allocating the representatives according to the actual vote of the people.\nNow, you might say that if Green and Purple are “close” to each other, that they might instead group together and get a representative that’s more aligned with them rather than someone from Gold. That’s true. What I showed here is just one possible way to go about allocating representatives, and it’s knowingly a bit simplistic just to illustrate the point. There are more sophisticated methods for proportional allocation and STV which can help with this.\nIt’s also rather unlikely that vote tallies will wind up being so nicely aligned with whatever threshold is needed to claim one of the representatives, so there will be some degree of mismatch between the vote and the allocation of representatives. Generally speaking, the more representatives in a region (e.g., State), the more able the system is to reflect the makeup of the people.\n\r\r\rSummary\rIn Voting Methods Part 1 I described plurality voting and some problems with it. Here in Part 2 I’ve talked a bit about some alternatives. These were both “background” for what I actually wanted to talk about in Voting Methods Part 3, some changes to the electoral process in the United States that I’d like to see.\n\r","date":1605916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606003898,"objectID":"44c82e9332a7997525004cd6f8d61af3","permalink":"/post-other/voting-methods-part-2-rcv/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/post-other/voting-methods-part-2-rcv/","section":"post-other","summary":"Introduction\rThis is part 2 a 3-part bit on voting and elections.\nAs before, the disclaimer that I’m not a political scientist, I am not an expert on the constitution or any other relevant field. I’m commenting here just as an individual. Also, a reminder that CGP Grey’s videos Politics in the Animal Kingdom are fantastic, and a much better explanation that I give here.\n\rAlternative voting systems\rIn Voting Methods Part 1 I talked about FPTP / plurality and some problems such as gerrymandering that can arise.","tags":["voting","elections"],"title":"Voting methods part 2: RCV","type":"post-other"},{"authors":[],"categories":["voting","elections"],"content":"\r\rIntroduction\rThis is part 3 of what a 3-part bit on voting and elections.\nIn Voting Methods Part 1 I described the plurality voting system that’s used in the United States, and some problems with it. In Voting Methods Part 3 I talked a bit about some alternatives, including ranked voting and proportional representation. These were both “background” for what I actually wanted to talk about now: Changes that I’d like to see in the electoral process in the United States.\nWhile it may seem like there is “an election” in the United States, in reality it’s really more like 534 separate elections: 1 president, 100 senators, and 435 house representatives. Well, sort of, only 1/3 of the senate is up for election at any given election, but that doesn’t really matter for this discussion.\nThis post will be structured as follows:\n\rDescribe the current structure of representatives and how they are elected.\rDescribe a bit of the history of why things are the way they are.\rTalk about some methods of voting, namely to highlight the drawbacks of the existing system.\rDescribe my proposed alternative method of conducting elections.\r\r\rCurrent Structure\rSenate\rEvery state gets 2 senators, regardless of size (by which we mean population size). The senators are elected by a state-wide vote. Whoever wins a plurality gets elected.\n\rHouse of Representatives\rEvery state is apportioned some number of representatives based on population, though it’s not perfect.\rOriginally, each state received a representative for every 30,000 people, so the house grew with the population. The Reapportionment Act of 1929 put a cap on the house of 435 members. Furthermore, the Uniform Congressional District Act of 1967 required that house representatives be elected from single-member districts. Apparently Hawaii and New Mexico are exempt, though still use single-member districts.\nWhat this means is that each state gets divided into some number of districts, which should be approximately equal population, and there is a race within that district to determine its representative. These races are, like the senate, decided by plurality.\n\rPresident\rFor electing the president, we use the electoral college (EC). In this, each state gets a number of electoral votes (EVs): One for each senator, and one for each house representative. Washington DC also gets 3. The states decide how to divvy these EVs between the candidates. Most states use a winner-take-all approach, where the (again, plurality) winner of the popular vote within the state gets all of the electoral votes for that state. So winning 0% and winning 49% of a state is equivalent: You get zero EV from that state.\nIn a slight deviation from the trend, the electors then meet to vote for president and vice president, but a candidate needs to get an majority (not simply a plurality) in order to be elected. This has the possibility to trigger some less well known mechanisms, but is a bit beside the point.\n\r\rA Brief History\rThis system was set up by the original 13 states as a compromise. Back then, we had the Articles of Confederation, which were being reformed. The large states liked the Virginia Plan, which allocated representation on the basis of population: Since they had more people, they’d contribute more tax, and should get more say in the government, or so their thinking went. The small states thought this gave too much power to the large states, and so favored the New Jersey Plan, which gave every state equal representation. Ultimately the Connecticut Compromise came about, which is why we have 2 senators for each state regardless of size, but the house of representatives is (somewhat) proportional to population. Then, to elect the president, they came up with the electoral college. With this method, the election of the president is a balance of equal representation for all states vs the population.\nThen the Reapportionment Act of 1929 which put a cap on the number of house representatives (435), but also ensured all states receive at least one house representative. Originally the senate was supposed to equally weight states, and the house was supposed to be population-based, but with this act the number of representatives was slightly biased slightly in favor of the small states. Not entirely, but skewed from it’s original intent. Consequently, the electoral college also received this skew. Additionally, the Uniform Congressional District Act of 1967 put some restrictions on the system.\n\rPersonal pipe dreams\rSo how do I think voting should take place? I’ll cover the senate and house first, since those are easier.\nCongress\rHouse of Representatives\rFor the house, I’d like to see proportional representation, probably by a Single Transferable Vote approach, but really any proportional allocation would be preferable to me.\nI’ll spare the details here, since I talked about them in Part 2, but something like this could actually give third parties a voice in the government, and serve to reduce the political polarization we see.\nHowever I would like to see the house returned to a better proportional representation, whether it’s by a fixed ratio of population, or by something more like the Wyoming rule where the lowest-population state gets 1 representative, and all other states are allocated representatives in a proportional manner based on relative population. Though instead of the lowest population state getting just one representative, I’d like to see them get 2 or 3, because increasing the number of representatives allows a proportional representation system to better represent the vote of the people.\n\rSenate\rFor the senate, I’d like to see ranked choice voting. These are, functionally, single-member races, so proportional representation doesn’t make sense. This way, if the “mainstream” party put forth someone really unpopular, voters who ordinarily supported that party could vote for someone else that they find less objectionable. Then as a second choice, they could select the unpopular person, or even the other mainstream party candidate.\n\rOther Thoughts\rActually, I’m not sure that I’m entirely opposed to an even larger shakeup of the current system. I’ve seen people complain about the Senate and want to get rid of it altogether. I’m not sure I’d go that far, but I’d like to see the power of the senate reduced somewhat. Currently the Senators vs Population graph looks like this.\nSo half of the senate represents less than 15% of the population of the United States, while over 50% of the population is represented by only 20 senators. Proponents of the equal representation of the senate will say this is a feature, not a bug, because the United States is the union of independent states. I tend to think that the civil war put the final nail in the coffin of that idea. I think having a second chamber of congress to be able to temper legislation and policy is a good idea, but I’m not convinced that the senate in its current implementation and degree of power is the right form of that chamber.\n\r\rPresident\rGo with popular vote\rFor the election of the president, many people are in favor of a popular vote. That’s one approach I’m fine with. A common complaint about this is that just a couple of states (or even a couple of cities) will then determine the election and rule everyone else, so the lower population states won’t get any representation. After all, doesn’t the figure I just made show that? A mere 10 states have 50% of the population.\nWell, not really. There are at least three major points that this complaint overlooks.\nFirst, it kind of ignores an important fact: Sure, just 10 states represent 50% of the population. But that’s 50% of the population. What is a country if not the people who make it up? And, going back to the Calvin and Hobbes example, if half or more of the nation votes for Hobbes, why should should Calvin become president?\nSecond, the president is a single person, just one part of the government. The lower-population states still get represented in the form of the house of representatives and the senate. If the small states think that they are not represented by the president because the large states determined him/her, couldn’t the large states say the same if the president gets determined by the small states? And if it comes to that, shouldn’t a single office holder be more representative of the larger portion of the population?\nThird, an most importantly I think, the complaint overlooks the fact that those 10 states represent quite a lot of diversity.\nLooking at these states, there is a mix of traditionally Democratic or Republican leaning states. And there is some Pacific coast, some Atlantic coast, some Midwest/Great Lakes, and Texas. It’s far from a homogeneous block. Looking at the presidential vote results, this becomes more clear.\nState\nPop\n2012\n\n2016\nDem\nRep\nDem\nRep\nCalifornia\n39,937,500\n60.24\n37.12\n\n61.73\n31.62\nTexas\n29,472,300\n41.38\n57.17\n\n43.24\n52.23\nFlorida\n21,993,000\n50.01\n49.13\n\n47.82\n49.02\nNew York\n19,440,500\n63.35\n35.17\n\n59.01\n36.52\nPennsylvania\n12,820,900\n51.97\n46.59\n\n47.46\n48.18\nIllinois\n12,659,700\n57.60\n40.73\n\n55.83\n38.76\nOhio\n11,747,700\n50.67\n47.69\n\n43.56\n51.69\nGeorgia\n10,736,100\n45.48\n53.30\n\n45.64\n50.77\nNorth Carolina\n10,611,900\n48.35\n50.39\n\n46.17\n49.83\nMichigan\n10,045,000\n54.21\n44.71\n\n47.27\n47.50\n\rOf the 10 states, 6 voted for the same party in 2012 and 2016, while 4 switched. Additionally, the margins of many of them are quite small, indicating that within these states, there is a lot of diversity. Hence, these states are not a monolithic voting bloc either individually or as a set.\nBecause of all this, I just don’t see the complaint that a couple high-population Democratic states would rule as holding water.\n\rModify the electoral college\rOkay, let’s say we think about it, and we still don’t like the popular vote for president (even switching to ranked choice voting), and for whatever reason are sticking to the electoral college. Could we do better within that system? I think so.\nCurrently, all but two states allocate all of their electoral votes to the state’s popular vote winner. But this is not required. As one example, there are two states (Maine and Nebraska) which allocate electoral votes on a per-district basis, with only the two extra votes from the senate seats being given to the popular vote winner. When I first heard of this, I thought it was a great idea: By divvying up the EVs more locally, individual districts would be more competitive, and more people would be engaged in the process. However, I’ve since realized there is a dramatic flaw with this approach. Remember how congressional districts can be gerrymandered? By allocating EVs on a per-district basis, this causes the presidential election to also be subject to gerrymandering. I view this as being very, very bad, and so I do not think per-district EVs are a good idea.\nInstead, similarly to the change I’d like to see with the house, I’m interested in states proportionally allocating their electoral votes to reflect the popular vote within that state. This means that Republicans in “blue states” and Democrats in “red states” don’t have their voice drowned out. There are some drawbacks to this idea, however.\nFirst is that all states would need to simultaneously enact this (or it would need to be done via amendment). Since this method would only help the person who lost the popular vote in a state - which would tend to be the person who lost the popular vote overall - if only one or two large states did this, then the person who lost the popular vote would be more likely to win the election.\nSecond, it opens up the possibility for some EVs to go to third party candidates. While ordinarily I’d be in favor of getting more diverse representation, the current mechanics of the EC means that if there is no majority, then the election gets sent to congress. If we’re to keep the EC, I’d rather see a bit more effort at that level rather than having the house of representatives decide the president. This would require further changes to the mechanics of elections, perhaps to allow coalitions or negotiations for sharing of power. From my understanding, this is how parliamentary systems work: If no party gets a majority, they need to form a coalition government, so both of those parties get some say in government.\rI’m not sure how that would work in our current system, but I think it’s an idea worth exploring.\nThe advantage of this approach is that it retains the balance of population-weighted vs equal representation that the current electoral college system employs, and which many people do like. It just changes the allocation of the votes and - if the house is reformed as well, moves the dial a bit on the precise balance of size vs equal.\n\r\r\rSummary\rSo, that’s my idea. For senators, a ranked choice ballot should be used. For the house, proportional representation, and for the president either a popular (ranked choice) vote, or failing that a proportional allocation of electoral votes. With these tweaks, I think there could be a substantial reduction in political polarization, as well as a better representation of the will of the people.\nAlso, as a bit of a fun thing, the website 270towin has a tool where you can implement some different methods of allocating electoral votes. I think it’s kind of fun to explore.\n\r","date":1605916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606003898,"objectID":"13819c221a9f71d7a1e55a93545cf42a","permalink":"/post-other/voting-methods-part-3-changes/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/post-other/voting-methods-part-3-changes/","section":"post-other","summary":"Introduction\rThis is part 3 of what a 3-part bit on voting and elections.\nIn Voting Methods Part 1 I described the plurality voting system that’s used in the United States, and some problems with it. In Voting Methods Part 3 I talked a bit about some alternatives, including ranked voting and proportional representation. These were both “background” for what I actually wanted to talk about now: Changes that I’d like to see in the electoral process in the United States.","tags":["voting","elections"],"title":"Voting methods part 3: Tweaking the System","type":"post-other"},{"authors":[],"categories":["lecture","general","stat101"],"content":"\rIntroduction\rShould you use a two-tailed test or a one-tailed test (or similarly, a confidence interval or 1-sided confidence bound)? For those just learning statistics, or who have had only a little training in the subject, this question comes up fairly often. And there is some conflicting information and advice out there. Most often I’ve seen comments critical of one-sided methods, such as:\n\rThe short answer is: Never use one tailed tests.\r- The Analysis Factor\n\ror\n\rTwo-Tailed Tests are the Default Choice\r- Statistics by Jim\n\rThough I did come across at least one person I’ve seen suggests the opposite!\n\rI actually intend to go all the way and argue that barring some very narrow use-cases, one should never use a two-tailed statistical test.\r- Analytics Toolkit blog\n\rI disagree with all of these statements to varying degrees, though mostly the first and the third. In this post, I’ll try to clarify how to think about this so that you can know whether to use a 1-sided or 2-sided test (spoiler: It has nothing little to do with Statistics). As an aside, “1-tailed” and “2-sided” are interchangable terms, and for the 1-tailed methods, there can be upper-tail or lower-tail.\n\rCrash course on hypothesis testing\rUnderstanding 1- or 2-tailed methods is probably easiest to describe in terms of hypothesis testing, but the same concepts apply to interval estimation (instead of, e.g., a confidence interval, you can construct a 1-sided confidence bound). So to start off, I’ll give a very short summary of basic hypothesis testing. I’m going to focus on a 1-sample t-test, but the same ideas will apply to any situation where you can have two tails. If you already understand t-tests, you probably want to skip to the next section.\nStep 1: Define your null and alternative hypotheses. Here are two such sets, for an upper-tail and two-tailed test, respectively, on the mean.\n\rUpper tailed: \\(H_{o}: \\mu \\leq \\mu_{0}\\) vs \\(H_{a}: \\mu \u0026gt; \\mu_{0}\\)\rTwo tailed: \\(H_{o}: \\mu = \\mu_{0}\\) vs \\(H_{a}: \\mu \\ne \\mu_{0}\\)\r\rIn these, \\(\\mu_{0}\\) is some hypothesized value, some value we are interested in comparing against.\nStep 2: Select a significance level, denoted by \\(\\alpha\\). This is the probability of committing a Type I error. This is the event in which we reject the null hypothesis when we should not have, because the null hypothesis is actually true. Any probability can be selected here, some common values are 0.1, 0.05, and 0.01. In practice we will not know whether or not a commit a Type I error.\nStep 3: From the data, compute the test statistic which (under certain assumptions) will follow a particular sampling distribution. One of the most basic examples is the t-test, where the test statistic is \\(t = (\\bar{x} - \\mu_{0}) / (s / \\sqrt{n} )\\) will follow a t-distribution (assuming the conditions are satisfied).\nStep 4: We then put this test statistic on the t-distribution and see if it falls into the rejection region (RR). This RR is determined from the significance level, depending on how the alternative hypothesis is specified. For an upper-tail test, all of the \\(\\alpha\\) probability is piled into the upper tail. For a two-tailed test, the α probability is split between the two tails. I made a graphic to illustrate this.\nFor each figure, the shaded region represents 5% of the total area. But we have arranged that 5% differently, because the tests are answering different questions.\nStep 5: Reject or fail to reject the null hypothesis. From that decision, you’d then take appropriate action - or not. I’ll provide a couple examples further on.\nIf you’re more familiar with p-values or critical values, do not fear. There’s a 1-1 correspondence between these. The RR is bounded by the critical value, which is the value at which the tail probability is equal to \\(\\alpha\\). Hence:\n\rIf the test statistic is in the rejection region (\\(t \\in RR\\)), then \\(p \\leq \\alpha\\)\n\r\rHow to choose 1 or 2 tails\rOkay, so that was a very quick overview of hypothesis testing. Then we come to the question of whether we should use a 1-tailed or 2-tailed method. To understand this, we need to understand what we’re actually doing when testing a hypothesis. Usually, we arrange the hypotheses so that the alternative hypothesis represents an actionable event, and the null hypothesis is the “status quo.” This means that when we reject the null hypothesis, we’re going to do something. I read and comment on the subreddits r/statistics and r/AskStatistics (among others). There, I think a user with the handle The_Sodomister phrased it well: Rejecting the null hypothesis means we “take action.” On the other hand, failing to reject the null hypothesis means we do not take action.\nSo we need to consider the context of the data, and whether we care about the direction. Let’s consider a couple of examples.\n\rAirlines regularly overbook flights (sell more tickets than they have seats). But for various reasons, not everyone shows up for their flight, so it often balances out. What they really care about is how many people show up, since too many people showing up means they have to pay out to reimburse anyone who can’t board that flight. So if they find that too many people will show up, they’ll take action by overbooking to a smaller degree. However, if too few people show up, well, those people paid for their ticket anyway, so the airline isn’t really losing out (that being said, they could be interested in the lower bound, so that they can overbook even further!).\n\rA pharmaceutical company is developing a new drug as an alternative to an existing drug. They’re doing the science to try to improve upon the existing drug, so they’d like to see improved outcomes. But it is extremely important to know if the new treatment actually leads to worse outcomes. So even though the company wants to show an effect in a particular direction, they also need to be able to detect if the effect is in the opposite direction. So the company will take action in either case (push the drug to market, or end R\u0026amp;D on the drug). So a two-tailed test would be of interest.\n\rAn engineer is designing a critical component for a rocket to send astronauts into space. That component has some failure rate, and a failure would mean the loss of millions or billions of dollars, and possibly lead to the death of the astronauts.\rSince this component was carefully designed and manufactured, the plan is to use it unless the error rate is too high. So the engineer is really only interested in the upper tail, how larger might the failure rate be, rather than how low it might be; they essentially treat the upper bound as “It’s possible for the error rate to be as large as X%, so we’re going to assume it is X%.” The question then fundamentally becomes “Is X% too high?”\n\rAn individual is considering whether to buy more of a particular stock, so they look at the average daily returns. They are interested if the daily return is positive, since they would then like to buy more. However, if the daily return is negative, that probably would lead to an action as well (say, selling that stock). Hence, detecting an effect in both directions is important.\n\r\rTo conduct a 1-tailed test, the opposite tail needs to be of utterly no concern. In the stock example, we would have to not care at all if the stock’s price was negative. There’s a good chance that’s not the case. That said, I’m no economist. Maybe for a prospective investor who doesn’t own any stock yet, their default position would be “Don’t invest,” so they really don’t care if the returns are negative, they’re only going to take action if the returns are positive.\nWhat’s important is specifying up-front what question we are interested in answering, and what results would cause us to take action. These should be clarified before data is collected. The statistical hypotheses and direction of the method are consequences of the answers. So as I hinted at before: This isn’t really really part of the statistical method, it’s part of the research hypothesis. This means it’s a subject-matter issue more than it is a statistical issue.\nIf we only take action for a particular direction, then we should only be testing in that direction. If we would take action for both directions, then we should test in both directions. The statistical part of this is, in my opinion, simply translating the question of interest from English (or whatever your language of choice might be) into statistical terms, such as be proposing a statistical model and rephrasing the question in terms of statistical parameters.\nSo I guess my take-away or tl;dr for this would be:\n\rIf you only care about (or would only take action) for results in a particular direction, then only test in that direction.\n\r\rTrailing thoughts\rThere are a few comments I want to make that I didn’t find a good place to work into the above discussion.\n\rOne of the quotes that I started off with said that two-tailed tests should be the default. The reason I disagree with this is that I don’t think there should be a “default” and an “exception.” I think we should carefully assess the context, but not be predisposed to one or the other.\n\rThis whole discussion only applies to a subset of statistical methods. Some methods, such as Analysis of Variance (ANOVA) are inherently 2-tailed. However, there are some directional variants, broadly described as tests for ordered alternatives, which put a series of inequalities (\\(\\leq\\) or \\(\\geq\\)) into the alternative hypothesis.\n\rAn old and reasonably well-known one is the Jonckheere–Terpstra test, which modifies a nonparametric ANOVA for the case of ordered alternatives.\rFor some shameless self-promotion, some of my research has been on the subject of ordered alternatives. See for example the papers on which I am co- or lead-author,\rDavidov, Jelsema, \u0026amp; Peddada (2018) and\rJelsema \u0026amp; Peddada (2016). In addition, a doctoral student I worked with at WVU used these methods in two applied papers,\rLaw, Morris, \u0026amp; Jelsema (2017) and\rLaw, Morris, \u0026amp; Jelsema (2018)\r\r\r\r","date":1602288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602351732,"objectID":"884c6fe4861a1ba01cf9274513de261b","permalink":"/post-stats/one-or-two-tailed-tests/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/post-stats/one-or-two-tailed-tests/","section":"post-stats","summary":"Introduction\rShould you use a two-tailed test or a one-tailed test (or similarly, a confidence interval or 1-sided confidence bound)? For those just learning statistics, or who have had only a little training in the subject, this question comes up fairly often. And there is some conflicting information and advice out there. Most often I’ve seen comments critical of one-sided methods, such as:\n\rThe short answer is: Never use one tailed tests.","tags":["stat101","hypothesis testing","tails"],"title":"One or two tails?","type":"post-stats"},{"authors":[],"categories":["rmarkdown"],"content":"Preface For this post, I\u0026rsquo;m going to assume you are fairly familiar with RMarkdown. If not, you may want to start with RMarkdown: The Definitive Guide, or R for Data Science. The short version is that RMarkdown is a flavor of the markup language Markdown, which uses plain-text formatting and can be rendered into a variety of output types, including PDF, MS Word, HTML, and others. With RMarkdown, you can incorporate R \u0026ldquo;code chunks\u0026rdquo; to create a fully reproducible document.\nWith RMarkdown, you can also create a PowerPoint presentation, meaning that you can create a reproducible slide deck which includes code and results. You might ask why use PowerPoint when there are a variety of presentation formats that RMarkdown supports. I personally like the xaringan  format. However, sometimes your hands are tied.\n Perhaps you\u0026rsquo;re doing an analysis for someone who insists on PP (maybe they\u0026rsquo;ll just take a couple of your slides to incorporate into a larger presentation). Maybe you have a corporate PP template that you have to use. Maybe the technology that the presentation will be projected on doesn\u0026rsquo;t support HTML slides. Maybe it\u0026rsquo;s something else.  So for whatever the reason, you need to make do with PowerPoint.\nAnother complication: Suppose you need to do the same basic analysis for a number of variables. For example, say you need to generate a set of boxplots, and a table of summary statistics for each variable in Fisher\u0026rsquo;s Iris data, comparing across the three Species. You\u0026rsquo;re already using RMarkdown, because you don\u0026rsquo;t want to be running results in R and then pasting figures and tables over to the slide deck manually. But you also don\u0026rsquo;t really want to be copy-pasting the same code all over, right? (Hint: No, you don\u0026rsquo;t). So we\u0026rsquo;d like to automate this process. You should be familiar with writing loops in R, and with that and a child document we can do exactly this.\nWe\u0026rsquo;ll need two files for this task, I\u0026rsquo;m going to name them main_doc.Rmd and child_doc.Rmd. The next sections will walk through the code that will go into each.\nMain Document Only the main document will need a YAML header, child documents can do without (come to think of it, I\u0026rsquo;m not sure if they are allowed to have YAML headers). A basic one might look like this:\n---\rtitle: \u0026quot;Looping for PP Slides\u0026quot;\rsubtitle: \u0026quot;Presentation subtitle\u0026quot;\rauthor: \u0026quot;Casey Jelsema\u0026quot;\rdate: \u0026quot;Generated | `r format(Sys.Date(), '%B %d, %Y')`\u0026quot;\routput:\rpowerpoint_presentation: default\r---\r Though note that I included one minorly fancy thing here, by setting the date to be a bit of R code so that it automatically updates when the file is knit. You may or may not want that. You can also put it elsewhere, for example in document type reports, I often set the subtitle to be the date.\nFor PP presentations, the section header (#) is what tells the document to start a new slide. You can change this with a setting in the YAML header. For instance, maybe you will have several sections, so you want two pound signs to denote the start of a new slide, you\u0026rsquo;d add slide_level: 2 to the output section, nested underneath powerpoint_presentation, see slide level). There are also a variety of other options available, including setting a reference document for a custom template, or having a two-column slide. Both of those are described here.\nThe rest of the main document is below. I\u0026rsquo;ll just past it all and then talk about it.\n```{r, setup, echo = FALSE, message=FALSE, include=FALSE }\rknitr::opts_chunk$set( echo=FALSE, message=FALSE)\rlibrary(\u0026quot;tidyverse\u0026quot;)\rlibrary(\u0026quot;flextable\u0026quot;)\r```\r# Introduction\rIn this presentation we use Fisher's iris data as an example.\r```{r, load-data, results=\u0026quot;hide\u0026quot; }\rdata(iris)\riris \u0026lt;- iris %\u0026gt;% mutate( Species = str_to_sentence(Species) )\r```\r- There is a code chunk here where I'm loading the data and doing some formatting to clean it up.\r- For the demonstration I'm just going to loop through to make some box plots and a table for each of the variables.\r```{r, loop-over-params, results=\u0026quot;hide\u0026quot; }\rparam_vec \u0026lt;- colnames(iris)[1:4]\rnParam \u0026lt;- length(param_vec)\rout \u0026lt;- rep(NA,nParam)\rfor( ii in 1:nParam ){\rparam_ii \u0026lt;- param_vec[ii]\rparam_ii_nice \u0026lt;- str_replace( param_vec[ii], \u0026quot;\\\\.\u0026quot;, \u0026quot; \u0026quot;)\rout[ii] \u0026lt;- knitr::knit_child(\u0026quot;child_doc.Rmd\u0026quot;)\r}\r```\r```{r, print-slides, results=\u0026quot;asis\u0026quot;}\rcat( paste(out, collapse=\u0026quot;\\n\u0026quot;) )\r```\r The setup chunk, is, well, your setup chunk. I usually use that to load which packages I\u0026rsquo;ll be using, setting options, sometimes specifying little helper functions I use, setting up a theme for ggplots or creating linetypes for flextable. The setup chunk here is a pretty basic one.\nThen the section header # Introduction gives us a first slide (well, beyond the title slide) with some comments. Note that the code chunk if there, between the first line and the two bullet points, but we don\u0026rsquo;t see it. If you\u0026rsquo;re familiar with RMarkdown, this shouldn\u0026rsquo;t be surprising.\nThe next chunk, loop-over-params is where the action is. So what am I doing?\n Get the variables to loop over, and get the number of them. Here I was easily able to extract them from the data. You might need to do a bit more work for that. The out object is setting up a container for me to put the slides for each variable, so that I\u0026rsquo;m not \u0026ldquo;growing\u0026rdquo; the object throughout my loop. Then in the for-loop, I\u0026rsquo;m grabbing the variable name, creating a nicer version of it for printing. Finally, I run the child document with the command knitr::knit_child(\u0026quot;child_doc.Rmd\u0026quot;).  This works because I wrote the child document with a generic param_ii, so as that object gets modified, the child document uses different variables. The results get put into the iith space of the out container.\nFinally, the last chunk here will print the results of the looping so that they are incorporated into the document. You need to use the results=\u0026quot;asis\u0026quot; option, and collapse all of the results together with a newline.\nChild Document A basic version of the child document is here. One thing I want to draw your attention to is that none of these chunks are labeled. That\u0026rsquo;s because if they get run multiple times, RMarkdown will (appropriately!) complain that there are chunks with the same name. I haven\u0026rsquo;t tried to dynamically name the chunks yet. An quick search showed that I\u0026rsquo;m not alone in this, and there were some solutions proposed. I haven\u0026rsquo;t tried them, and have not had a need to really try.\n```{r, results=\u0026quot;asis\u0026quot;}\r# figure-slide-title\rcat(\u0026quot;# Response: \u0026quot;, param_ii_nice, \u0026quot; | Box plots\u0026quot; )\r```\r```{r, fig.height=3.5 }\r# figure-slide\rggplot( iris , aes_string(x=\u0026quot;Species\u0026quot;, y=param_ii) ) +\rgeom_boxplot( aes(fill=Species) ) +\rlabs( y=param_ii_nice )\r```\r The first chunk is setting the title of the slide, using the \u0026ldquo;nice\u0026rdquo; version of the parameter name. Again, I\u0026rsquo;m printing with the chunk option results=\u0026quot;asis\u0026quot;, because we want to paste exactly what we write into the Rmd. After that, I\u0026rsquo;m just making the figure. One bit you may not be familiar with is the aes_string command in ggplot. Note that typically variable names are unquoted in ggplot. If you have a quoted string, then aes_string will help you out here.\nThen on to the table.\n```{r, results=\u0026quot;asis\u0026quot;}\r# table-slide-title\rcat(\u0026quot;# Response: \u0026quot;, param_ii_nice, \u0026quot; | Statistics\u0026quot; )\r```\r- Comments you want to make in the child slide need to use some conditional logic.\r- Otherwise the same comments go on every child slide.\r- That means they should probably be rather basic, if this is supposed to be automated.\r```{r, ft.left=1, ft.top=6 }\r# table-slide\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit()\r```\r Again, I\u0026rsquo;m first defining the slide title with a results=\u0026quot;asis\u0026quot; chunk option, then making the content of the slide. I tend to use the flextable package for tables, though some parts of officer help to do some fine-tuning. There are other table-making packages, but these have been more than enough for my needs.\nI don\u0026rsquo;t remember exactly why I switched from using the kable function to flextable, I think that I was having a difficult time with MS Word tables. Maybe I just didn\u0026rsquo;t put in enough effort. Regardless, flextable has been easy for me to make tables in the formats I\u0026rsquo;ve needed, and the author is actively, improving it, so I haven\u0026rsquo;t had a reason to look elsewhere.\nIn the table-slide chunk, note that there are chunk options ft.left=1, ft.top=4. These define where the top-left of the table will be placed. It helps to arrange the content and the output on a slide. I\u0026rsquo;m not sure if they work with tables generated from other packages.\nSo, if you\u0026rsquo;ve copied these code chunks into main_doc.Rmd and child_doc.Rmd (well, you can call the first one whatever you like, but the code references child_doc.Rmd), then you should be able to generate a slide deck that has two slides for each variable, along with a first introductory slide.\nYou should be able to take it from there. Add more slides at the beginning as necessary, add more slides after looping over the variables, add more slides into the child document, make a fancier loop, or anything. Once you have the building blocks, it\u0026rsquo;s just a matter of arranging them appropriately. My hope is that this little demo gave you another building block.\nTwo other aspects I\u0026rsquo;d like to demonstrate are: Two-column slides; and conditional logic for dynamically generating the output comments.\nTwo-column slides This one is pretty easy. As I noted above, the RMarkdown book has examples. The basic idea is that you have a set of colons (:) to denote the start and end of column sets, and a (smaller) set of colons to denote the start and end of columns, like so:\n:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\rContent of the left column.\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\rContent of the right column.\r:::\r::::::\r So, for instance, we might replace the table-slide with the following, we get a two-column slide. Note that the column notation goes outside the chunk, or put another way, the code chunk goes inside a particular column.\n```{r, results=\u0026quot;asis\u0026quot;}\rcat(\u0026quot;# Two-column Slide: \u0026quot;, param_ii_nice, \u0026quot; | Statistics\u0026quot; )\r```\r:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\r```{r, ft.left=1, ft.top=2 }\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit()\r```\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\r- Comments you want to make in the child slide need to use some conditional logic.\r- Otherwise the same comments go on every child slide.\r- That means they should probably be rather basic, if this is supposed to be automated.\r:::\r::::::\r You can put several tables in a single column, using multiple code chunks, so that you can set ft.left and ft.top for each table. I\u0026rsquo;ll demonstrate that in the last section.\nConditional logic for results In the placeholder text for the table slide, I mentioned that you\u0026rsquo;ll probably want to use conditional logic for the results, otherwise every iteration of the child slide will have exactly the same text. Let\u0026rsquo;s say we need to run an ANOVA on each outcome. Yes, there are issues with this, one of which is that you\u0026rsquo;d need to be paying attention to how many comparisons you\u0026rsquo;re making, and adjust p-values appropriately. I\u0026rsquo;m not going to worry about that for the moment.\nIn this part, I\u0026rsquo;ve added a bit of formatting to the tables. One thing I did was define a border. To reduce typing, I add the command h1 \u0026lt;- officer::fp_border( width=0.75 ) to the setup chunk of my document. Often I define several lines of varying width and style (e.g., a dashed line). This way I can use them in any table.\n```{r, results=\u0026quot;asis\u0026quot;}\rcat(\u0026quot;# ANOVA: \u0026quot;, param_ii_nice, \u0026quot;\u0026quot; )\r```\r:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\r```{r, ft.left=1, ft.top=2 }\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit() %\u0026gt;%\rborder( part=\u0026quot;header\u0026quot;, i=1, border.top=h1, border.bottom=h1 ) %\u0026gt;%\rborder( part=\u0026quot;body\u0026quot;, i=3, border.bottom=h1 ) %\u0026gt;%\ralign( part=\u0026quot;all\u0026quot;, j=2:4, align=\u0026quot;center\u0026quot; )\r```\r```{r, ft.left=1, ft.top=4 }\rfrml \u0026lt;- as.formula( str_c( param_ii , \u0026quot; ~ Species\u0026quot; ) )\rlm_out \u0026lt;- lm( frml, data=iris )\rpost_hoc \u0026lt;- TukeyHSD( aov(lm_out) )$Species %\u0026gt;%\ras.data.frame() %\u0026gt;%\rrownames_to_column() %\u0026gt;%\rrename( \u0026quot;Contrast\u0026quot; = \u0026quot;rowname\u0026quot;, \u0026quot;Diff\u0026quot;=\u0026quot;diff\u0026quot;, \u0026quot;pvalue\u0026quot;=\u0026quot;p adj\u0026quot;) %\u0026gt;%\rdplyr::select( Contrast, Diff, pvalue ) %\u0026gt;%\rmutate( tpval = ifelse( pvalue \u0026lt; 0.0001, \u0026quot;\u0026lt; 0.0001\u0026quot;, sprintf(\u0026quot;%0.4f\u0026quot;, pvalue) ) )\rpost_hoc %\u0026gt;% dplyr::select( Contrast, Diff, tpval ) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit() %\u0026gt;%\rset_header_labels( \u0026quot;tpval\u0026quot;=\u0026quot;p-value\u0026quot; ) %\u0026gt;%\rborder( part=\u0026quot;header\u0026quot;, i=1, border.top=h1, border.bottom=h1 ) %\u0026gt;%\rborder( part=\u0026quot;body\u0026quot;, i=3, border.bottom=h1 ) %\u0026gt;%\ralign( part=\u0026quot;all\u0026quot;, j=2:3, align=\u0026quot;center\u0026quot; )\r```\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\r```{r, results=\u0026quot;asis\u0026quot;}\r# Conditional logic to build simple sentences for results\rany_diff \u0026lt;- any( post_hoc[[\u0026quot;pvalue\u0026quot;]] \u0026lt;= 0.05 )\rwhich_diff \u0026lt;- which( post_hoc[[\u0026quot;pvalue\u0026quot;]] \u0026lt;= 0.05 )\rif( any_diff ){\rwhich_contrasts \u0026lt;- str_replace( post_hoc[[\u0026quot;Contrast\u0026quot;]][which_diff], \u0026quot;-\u0026quot;, \u0026quot; and \u0026quot;)\rndiff \u0026lt;- length(which_contrasts)\rbullets \u0026lt;- str_c( \u0026quot;- Pairwise comparisons conducted using Tukey's method.\\n\u0026quot;,\r\u0026quot;- Significant differences in mean \u0026quot;, param_ii_nice, \u0026quot; were found between: \u0026quot; )\rif( ndiff==1 ){\rbullets \u0026lt;- str_c( bullets, which_contrasts )\r} else if( ndiff==2 ){\rbullets \u0026lt;- str_c( bullets, str_c( which_contrasts, collapse=\u0026quot; as well as \u0026quot; ), \u0026quot;.\u0026quot; )\r} else{\rbullets \u0026lt;- str_c( bullets, str_c( which_contrasts[-ndiff], collapse=\u0026quot;, \u0026quot; ), \u0026quot;, and between \u0026quot;, which_contrasts[ndiff], \u0026quot;.\u0026quot; )\r}\r} else{\rbullets \u0026lt;- str_c( \u0026quot;- Pairwise comparisons conducted using Tukey's method.\\n\u0026quot;,\r\u0026quot;- There were no significant differences in mean \u0026quot;, param_ii_nice, \u0026quot; detected.\u0026quot; )\r}\rcat( bullets )\r```\r:::\r::::::\r I\u0026rsquo;ll leave you to inspect the logic creating the sentences. The basic idea is to create a string which has Markdown syntax and then print it using the chunk option results=\u0026quot;asis\u0026quot;. You can get fancier, like incorporating the p-values or other results, and so on. But again, my point here is to provide a building block so that you can use it to do things that I haven\u0026rsquo;t even thought about.\nOne thing about this is that the iris dataset happens to show a difference in mean between all species and for all variables. If you want to see the logic changing the slides, either use a different dataset (you\u0026rsquo;ll naturally need to change the analysis codes!), or when you load the data, make some modification so that the means won\u0026rsquo;t differ. For example, you can permute one variable with the line: iris[[\u0026quot;Sepal.Length\u0026quot;]] \u0026lt;- sample( iris[[\u0026quot;Sepal.Length\u0026quot;]] )\nSo that\u0026rsquo;s it for this post. I hope it will give you a nice little trick to put in your back pocket for use in some project in the future.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596307830,"objectID":"d851dc4bd1d986028f4e029f7d7d5dab","permalink":"/post-stats/looping-powerpoint-slides-in-rmarkdown/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/post-stats/looping-powerpoint-slides-in-rmarkdown/","section":"post-stats","summary":"Preface For this post, I\u0026rsquo;m going to assume you are fairly familiar with RMarkdown. If not, you may want to start with RMarkdown: The Definitive Guide, or R for Data Science. The short version is that RMarkdown is a flavor of the markup language Markdown, which uses plain-text formatting and can be rendered into a variety of output types, including PDF, MS Word, HTML, and others. With RMarkdown, you can incorporate R \u0026ldquo;code chunks\u0026rdquo; to create a fully reproducible document.","tags":["powerpoint","loop","slides","child document","rmarkdown"],"title":"Looping PowerPoint Slides in RMarkdown ","type":"post-stats"},{"authors":[],"categories":[],"content":"The short version When encountering scientific claims - or really any objective claim - it can be difficult to know what to believe, especially when there seems to be conflicting stories. In the world of COVID-19 shutdowns and stay-at-home orders, it can be hard to parse through the deluge of information and figure out what\u0026rsquo;s going on. The following set of questions can serve as a sort of \u0026ldquo;smell test\u0026rdquo; to get an initial sense of the validity of the claims at hand.\n Who or what is the source? Why should we listen to them? Are they citing sources or presenting data? What is their expertise? Are the claims extravagant? Does the source have a known bias?  The long version The COVID-19 pandemic rather abruptly thrust science into peoples\u0026rsquo; faces. In a very short timeframe we went from ordinary news, to \u0026ldquo;There\u0026rsquo;s an outbreak in China,\u0026rdquo; to \u0026ldquo;It\u0026rsquo;s spreading worldwide,\u0026rdquo; to \u0026ldquo;It\u0026rsquo;s here and governors are issuing expansive shutdowns and stay-at-home orders.\u0026rdquo; People were suddenly inundated statistical claims and scientific results (as of July 20, 2020, Nature reported there to be 65,470 scientific papers on COVID-19).\nThe problem is that most people are not statisticians or epidemiologists. Most people are not microbiologists, immunologists, or researchers in a slew of other biomedical fields. Most people are not highly trained in research methods and reading the results.\nA constantly updating deluge of technical information that most people have a difficult time understanding creates a prime situation for misinformation, confirmation bias, and conspiracy theories to propagate. We have seen much of this in the course of the COVID-19 pandemic.\nMy intent in this post is to give a few questions you can ask yourself to assess new information - whether it\u0026rsquo;s a scientific article, a news report, or a post on social media. My set of questions is not exhaustive, but represents a fairly good first pass to sort out what is reliable from what is not. Many of the claims that I\u0026rsquo;ve seen posted across social media fail at least one, if not several or all, of the questions on my list.\n Question 1: Who or what is the source, and why should we listen to them? When it comes to objective and scientific claims, I think there are two main things to pay attention to: Evidence and expertise. If you can\u0026rsquo;t identify at least one of these things, there is no reason to believe the source. It doesn\u0026rsquo;t matter if something \u0026ldquo;just makes sense\u0026rdquo; or if someone\u0026rsquo;s \u0026ldquo;gut feeling\u0026rdquo; leans a certain way. That\u0026rsquo;s valid for subjective matters, but not for assessing scientific results.\nSo from the get-go, ask yourself: Is there evidence being cited, or does this person have expertise? If the answers are \u0026ldquo;No\u0026rdquo; and \u0026ldquo;No,\u0026rdquo; then you should take anything they are claiming with a very large grain of salt.\nQuestion 2a: Are they providing evidence? There is a saying among statisticians: \u0026ldquo;In God we trust, all others bring data.\u0026rdquo; Probably a bit more common is another saying - called Hitchens\u0026rsquo;s razor: \u0026ldquo;What can be asserted without evidence can also be dismissed without evidence.\u0026rdquo; The point with both of these is that evidence reigns supreme.\nThat being said, simply presenting data does not in and of itself validate a claim. Data needs to be interpreted, and interpreting data is not always often not straightforward. There could be missing context from the subject-matter domain, technical language that could be easily misinterpreted, different \u0026ldquo;levels\u0026rdquo; of data quality, or other issues complicating the understanding of evidence.\nSome might not understand the different levels of evidence. Some more formalized thoughts are available from Burns, Rohrich, and Chung (2011) or Glasofer and Townsend (2019). To summarize a bit from these:\n Tier 1: Systematic review of Tier 2 studies. Tier 2: Carefully designed experiments, including randomization and control. In medical fields, this should include blinding (preferably double-blind), and would often be dscribed as a randomized controlled trial (RCT). Tier 3: Non-experimental studies (including observational studies). These can be subject to various biases. Tier 4: Expert opinion, case studies (i.e., just looking at one or a few individual subjects).  I\u0026rsquo;m sure people could easily expand this list, or make some small adjustments. The point here is not to be comprehensive or \u0026ldquo;final,\u0026rdquo; it\u0026rsquo;s to provide a quick and dirty way to be able to think about evidence being presented. So if you see two seemingly contradictory results, instead of picking the one that you like the best, you might be able to think about the data quality from each study, and hence assess which result is based on the more reliable data.\nThis is one way in which expertise helps: Experts have been trained and have experience interpreting data from their field. They generally have a better understanding of data quality, of what the data might say, and of what data are important to consider. That last bit can be important: A person outside the field might point to some data and say \u0026ldquo;Look at X, this is big!\u0026rdquo; But a trained and experienced professional in the field might say, \u0026ldquo;Well, X isn\u0026rsquo;t really that important, we should be talking about Y instead.\u0026rdquo;\nFor example, suppose we were talking about the dangers of an electrical shock. Someone might say, \u0026ldquo;I sustained a 1,000 volt shock and I was just fine.\u0026rdquo; But someone who knows a bit more could retort \u0026ldquo;Okay, but voltage isn\u0026rsquo;t the important thing, amperage is much more relevant here.\u0026rdquo;\nIn general, I\u0026rsquo;d rank the \u0026ldquo;reliability\u0026rdquo; of claims as:\n Expert who is citing good evidence: Very high. Expert who is citing weak evidence: Good, pending the precise nature of the data. Expert speaking about their field, but not citing evidence: Good, but with reservations. Even experts can be mistaken, or have lost their credibility as did the former scientist featured in \u0026ldquo;Plandemic.\u0026rdquo; Non-expert citing evidence: Good, but with reservations, depending on quality of evidence and appropriateness of interpretation. Non-expert without evidence: Low, no reason to give them the time of day.  Question 2b: What is their expertise? Now, just because someone has expertise doesn\u0026rsquo;t mean they are automatically a reliable source. The field of expertise also matters. I myself am a statistician, I can speak to statistical matters. Statistics is somewhat unique in that it is the language of science, permeating the scientific endeavor: Researchers need to communicate their results, and that communication typically involves statistical experimentation and analysis. As John Tukey put it, statisticians \u0026ldquo;get to play in everyone\u0026rsquo;s backyard\u0026rdquo;, meaning we get exposed to and pick up a bit of various topics. We might not be experts in the field, but we can usually understand and assess the results.\nBut you shouldn\u0026rsquo;t ask me to describe why or how a given protein does what it does. You shouldn\u0026rsquo;t ask me to describe the mechanism by which a medication functions. Or why some molecule will or will not react with another, or how the gravity of the sun and planets interact with each other in the solar system. Similarly, you should not ask me to run electrical wiring for a house or to repair a car engine.\nMy point here is that specialty matters. Nobody is an expert in all things. Some fields are closely related, so an expert in one may be highly competent in another, but many fields are exceedingly diverse. So it\u0026rsquo;s important to keep someone\u0026rsquo;s specialty in mind when listening to what they have to say.\nI\u0026rsquo;ve seen a number of times people quote physicians (MD or DO) when talking about COVID-19. That\u0026rsquo;s perfectly fine, when the topic is a clinical matter. Just as being a statistician doesn\u0026rsquo;t make me an expert in medicine, having a medical degree doesn\u0026rsquo;t make one an expert in statistics or epidemiology. Just because a topic is related in some way to the medical field does not make physicians the foremost experts.\nOne job that I\u0026rsquo;ve held has been as a professor in a biostatistics department. I\u0026rsquo;ve collaborated with MDs and residents, and I\u0026rsquo;ve taught aspiring doctors. Some of the doctors had a solid grasp of statistical methods. Most did not. Fewer still should conduct their own statistical analysis. The students aiming for med school often took one or two courses in statistics, not enough to offer a comprehensive understanding statistics, much less the ability to properly conduct statistical analysis. Some were diligent students that probably retained the ideas of the course past the end of the semester. But most were (due to the competitiveness of med school applications) were hyper-focused on grades rather than understanding, and likely retained little if anything that we talked about after the final exam.\nTo be sure, there are some MDs who are excellent researchers, Dr. Anthony Fauci is a prime example. But when talking about a statistical or epidemiological analysis an MD is not automatically an expert, they may very well be talking outside of their domain of expertise.\nQuestion 4: Are the claims extravagant? As Carl Sagan said, \u0026ldquo;Extraordinary claims require extraordinary evidence.\u0026rdquo; There are sometimes dramatic breakthroughs, but usually results are much more mundane or incremental in nature. If you see a headline or claim that seems too good to be true, it\u0026rsquo;s probably either misleading clickbait, misinformation, or conspiracy.\nForbes did an article about this subject, you can find it here. The Atlantic did a piece on Dr. John Ioannidis which can be read here. In it, Dr. Ioannidis is quoted to say:\n Often the claims made by studies are so extravagant that you can immediately cross them out without needing to know much about the specific problems with the studies\n Even scientists can be guilty of over-hyping their conclusions. Press releases may exaggurate a bit more, media outlets take it up another few notches, and people on social media blow it up to cosmic proportions.\nQuestion 5: Does the source have a known bias? Biased sources are likely to selectively interpret results, and either overlook or deliberately obscure flaws. That\u0026rsquo;s why a lot of research has a statement about funding sources. Remember those doctors from Bakersfield, California? They run a private urgent care facilities. They had an enormous conflict of interest: Personal profit. If they did not have that incredible bias, they may have been more reserved about their comments, and perhaps noticed the dramatic statistical and scientific errors they were making, and for which they were condemned by two major medical associations, the American College of Emergency Physicians and the American Academy of Emergency Medicine.\nA financial bias is not the only bias out there. Various news outlets have a strong bias, and will omit certain facts, or selectively choose stories that spin a particular narrative. If a source is known to have a strong bias (be it right or left), then caution should be exercised in assessing the claims being made. This goes doubly so when the source itself acknowledges that it is biased, which ties back to the first question: Who is the source and why should we believe them? If the source admits they are biased, we should be skeptical of anything they have to say.\nThat\u0026rsquo;s not to say that a source can\u0026rsquo;t be biased without acknowleding it, the bias might just be more difficult to verify. My point is that if the source is already overlty admitting a bias then you should from the get-go not expect a neutral or objective consideration of the topic. And if that\u0026rsquo;s the case, then you should again take any conclusion with a large grain of salt.\n So that\u0026rsquo;s it. The first real post I put on my website, and what I think is a solid set of questions to consider when faced with some scientific claim.\n","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596233884,"objectID":"92f5a8417c8fe0fa6ead5a2fd18b57db","permalink":"/post-other/a-scientific-smell-test/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/post-other/a-scientific-smell-test/","section":"post-other","summary":"The short version When encountering scientific claims - or really any objective claim - it can be difficult to know what to believe, especially when there seems to be conflicting stories. In the world of COVID-19 shutdowns and stay-at-home orders, it can be hard to parse through the deluge of information and figure out what\u0026rsquo;s going on. The following set of questions can serve as a sort of \u0026ldquo;smell test\u0026rdquo; to get an initial sense of the validity of the claims at hand.","tags":["covid","coronavirus","science","evidence"],"title":"A Scientific Smell Test","type":"post-other"},{"authors":[],"categories":["general"],"content":"Welcome! If you\u0026rsquo;ve found your way to this page \u0026hellip; well, I\u0026rsquo;m more than a little surprised. This is my personal website. I\u0026rsquo;m not entirely sure what I\u0026rsquo;ll be doing here. My plan at the moment is to use it mainly for:\n Listing professional things like publications independently of my place of employment. Host the slides for talks or workshops that I give.  I may have some posts (is that \u0026lsquo;blogging\u0026rsquo;? Save me!) about topics that pique my interest. This would likely be my thoughts on some bit of Statistics or science that came across my screen.\nI\u0026rsquo;ve been talking about science and statistics in the context of COVID-19 on social media for a while now. I may transition some of that here, since social media isn\u0026rsquo;t usually conducive to the long-form comments I make often enough.\n","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594839175,"objectID":"3b462785c865e5125b0dd0f932deec72","permalink":"/post-other/welcome-to-my-site/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/post-other/welcome-to-my-site/","section":"post-other","summary":"Welcome! If you\u0026rsquo;ve found your way to this page \u0026hellip; well, I\u0026rsquo;m more than a little surprised. This is my personal website. I\u0026rsquo;m not entirely sure what I\u0026rsquo;ll be doing here. My plan at the moment is to use it mainly for:\n Listing professional things like publications independently of my place of employment. Host the slides for talks or workshops that I give.  I may have some posts (is that \u0026lsquo;blogging\u0026rsquo;?","tags":["greetings","welcome"],"title":"Welcome to my site","type":"post-other"},{"authors":["Casey Jelsema","Rajib Paul","Joseph W McKean"],"categories":["publication","methods"],"content":"","date":1590883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590883200,"objectID":"89aa3b96937486d2635c159123cc612d","permalink":"/publication/2020-robust-v/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/2020-robust-v/","section":"publication","summary":"For large datasets, spatial covariances are often modeled using basis functions and covariance of a reduced dimensional latent spatial process. For skewed data, likelihood based approaches with Gaussian assumption may not lead to faithful inference. Any $L_{2}$ norm based estimation is susceptible to long tails and outliers due to contamination. Our method is based on an empirical binned covariance matrix using the median absolute deviation and minimizes $L_{1}$ norm between empirical covariance and the model covariance. The consistency of the proposed estimate is established theoretically. The improvement is demonstrated using simulated data and cloud data obtained from NASA's Terra satellite.","tags":["spatial"],"title":"Robust estimation of reduced rank models to large spatial datasets","type":"publication"},{"authors":[],"categories":["r package"],"content":"CLME stands for Constrained Linear Mixed Effects. I wrote this R package (CRAN link) during my postdoctoral work at NIEHS.\nThe fundamental idea is similar to the Jonckheere–Terpstra or any other test for ordered alternatives: If the treatment groups are ordinal, then a trend of some sort may be of interest. If a researcher has such a hypothesis, they can not only test for the ordered alternative, but they can constrain the estimation to respect the order from the alternative hypothesis. This results in getting more power than a comparable test that does not impose constraints or test for an order (e.g., ANOVA).\nFeel free to check out the github repository. There are a variety of improvements I\u0026rsquo;d like to make, including cleaning up my code, removing dependancies, and adding some features. Just need time to get to them. If you\u0026rsquo;d like to join, give me a shout!\nDisclaimer: This is really just a post to get something into the \u0026ldquo;Project\u0026rdquo; space of my website. I\u0026rsquo;ll probably modify this post later to add clarity.\n","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565797783,"objectID":"1cf66225e7593c17a980f29de3004b4b","permalink":"/project/clme/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/project/clme/","section":"project","summary":"CLME stands for Constrained Linear Mixed Effects. I wrote this R package (CRAN link) during my postdoctoral work at NIEHS.\nThe fundamental idea is similar to the Jonckheere–Terpstra or any other test for ordered alternatives: If the treatment groups are ordinal, then a trend of some sort may be of interest. If a researcher has such a hypothesis, they can not only test for the ordered alternative, but they can constrain the estimation to respect the order from the alternative hypothesis.","tags":["package","order restricted inference","bootstrap"],"title":"CLME","type":"project"},{"authors":[],"categories":["lecture","talk"],"content":"\rThe slides for my Introduction to Spatial Data (geared for non-Statisticians) can be found here:\nhttps://jelsema.github.io/presentations/2019-intro-spatial/intro_spatial.html#1\n","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565794885,"objectID":"7484acca2de3c888eb0a9ddec9379035","permalink":"/talk/wvu-bios-604-intro-to-spatial/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/talk/wvu-bios-604-intro-to-spatial/","section":"talk","summary":"The slides for my Introduction to Spatial Data (geared for non-Statisticians) can be found here:\nhttps://jelsema.github.io/presentations/2019-intro-spatial/intro_spatial.html#1","tags":["spatial","intro"],"title":"WVU BIOS 604 Intro to Spatial","type":"talk"},{"authors":["Casey Jelsema","Richard Kwok","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"fd1c3abfdd33965446ec4475cad4126e","permalink":"/publication/2019-threshold-knots/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2019-threshold-knots/","section":"publication","summary":"Large spatial datasets are typically modelled through a small set of knot locations; often these locations are specified by the investigator by arbitrary criteria. Existing methods of estimating the locations of knots assume their number is known a priori, or are otherwise computationally intensive. We develop a computationally efficient method of estimating both the location and number of knots for spatial mixed effects models. Our proposed algorithm, Threshold Knot Selection (TKS), estimates knot locations by identifying clusters of large residuals and placing a knot in the centroid of those clusters. We conduct a simulation study showing TKS in relation to several comparable methods of estimating knot locations. Our case study utilizes data of particulate matter concentrations collected during the course of the response and clean-up effort from the 2010 *Deepwater Horizon* oil spill in the Gulf of Mexico.","tags":["spatial"],"title":"Threshold knot selection for large-scale spatial models with applications to the Deepwater Horizon disaster","type":"publication"},{"authors":["Eric Law","Keith Morris","Casey Jelsema"],"categories":["publication","methods"],"content":"","date":1529280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529280000,"objectID":"11de56111c7b98c52114236295bf78e2","permalink":"/publication/2018-test-fire2/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-test-fire2/","section":"publication","summary":"The Association of Firearm and Toolmark Examiners recommends a minimum of two test fires be performed when an unknown firearm is submitted to a laboratory prior to doing a comparison with a cartridge case collected from a crime scene. Limited research has been performed to determine how many test fires are necessary to be representative of the match distribution of a firearm. Various makes and models of firearms comprising five calibers were tested using a hybrid equivalence test to determine how many cartridge cases were required to represent the match distribution of an unknown firearm based on both breech face and firing pin correlation scores from an IBIS® Heritage^(TM) System. The same general trend was observed for each caliber of firearm where the equivalence percentage increased from 10 to 30 cartridge cases. Overall, 15 cartridge cases are sufficient for above an 80% probability of representing the full match distribution for an unknown firearm. To approach full equivalence, 25 cartridge cases are enough because 30 cartridge cases were not found to be significantly higher in equivalence percentage for any caliber of firearm tested.","tags":["monte carlo","equivalence testing","forensics","test fires"],"title":"Determining the number of test fires needed to represent the variability present within firearms of various calibers","type":"publication"},{"authors":["Ori Davidov","Casey Jelsema","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"daa2d98e4f42f496756703976902fb26","permalink":"/publication/2018-osin/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-osin/","section":"publication","summary":"There are many applications in which a statistic follows, at least asymptotically, a normal distribution with a singular or nearly singular variance matrix. A classic example occurs in linear regression models under multicollinearity but there are many more such examples. There is well-developed theory for testing linear equality constraints when the alternative is two-sided and the variance matrix is either singular or nonsingular. In recent years, there is considerable, and growing, interest in developing methods for situations in which the estimated variance matrix is nearly singular. However, there is no corresponding methodology for addressing one-sided, that is, constrained or ordered alternatives. In this article, we develop a unified framework for analyzing such problems. Our approach may be viewed as the trimming or winsorizing of the eigenvalues of the corresponding variance matrix. The proposed methodology is applicable to a wide range of scientific problems and to a variety of statistical models in which inequality constraints arise. We illustrate the methodology using data from a gene expression microarray experiment obtained from the NIEHS’ Fibroid Growth Study. Supplementary materials for this article are available online.","tags":["order restricted inference","bootstrap","singular"],"title":"Testing for Inequality Constraints in Singular Models by Trimming or Winsorizing the Variance Matrix","type":"publication"},{"authors":["Amy Hessl","Kevin Anchukaitis","Casey Jelsema","Benjamin Cook","Oyunsanaa Byambasuren","Caroline Leland","Baatarbileg Nachin","Neil Pederson","Hanqin Tian","Laia Andreu Hayles"],"categories":["publication","collaborative"],"content":"","date":1520985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520985600,"objectID":"ef5f07c837121252d69a8f0686615cb5","permalink":"/publication/2018-mongolia-drought/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-mongolia-drought/","section":"publication","summary":"The severity of recent droughts in semiarid regions is increasingly attributed to anthropogenic climate change, but it is unclear whether these moisture anomalies exceed those of the past and how past variability compares to future projections. On the Mongolian Plateau, a recent decade-long drought that exceeded the variability in the instrumental record was associated with economic, social, and environmental change. We evaluate this drought using an annual reconstruction of the Palmer Drought Severity Index (PDSI) spanning the last 2060 years in concert with simulations of past and future drought through the year 2100 CE. We show that although the most recent drought and pluvial were highly unusual in the last 2000 years, exceeding the 900-year return interval in both cases, these events were not unprecedented in the 2060-year reconstruction, and events of similar duration and severity occur in paleoclimate, historical, and future climate simulations. The Community Earth System Model (CESM) ensemble suggests a drying trend until at least the middle of the 21st century, when this trend reverses as a consequence of elevated precipitation. Although the potential direct effects of elevated CO2 on plant water use efficiency exacerbate uncertainties about future hydroclimate trends, these results suggest that future drought projections for Mongolia are unlikely to exceed those of the last two millennia, despite projected warming.","tags":["copula","geography","drought","mongolia"],"title":"Past and future drought in Mongolia","type":"publication"},{"authors":["Eric Law","Keith Morris","Casey Jelsema"],"categories":["publication","methods"],"content":"","date":1493769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493769600,"objectID":"6160d3ede8397d566b4b0f1ba430cd2f","permalink":"/publication/2017-test-fire1/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2017-test-fire1/","section":"publication","summary":"Many studies have been performed in recent years in the field of firearm examination with the goal of providing an objective method for comparisons of fired cartridge cases. No published research to support the number of test fires needed to represent the variability present within the impressions left on a cartridge case could be found. When a suspect firearm is submitted to a firearm examiner, typically two to four test fires are performed. The recovered cartridge cases are compared to each other to determine which characteristics from the firearm are reproducing, and then compared to any cartridge cases collected at a crime scene. The aim of this research was to determine the number of test fires examiners should perform when a suspect firearm is submitted to the lab to balance cartridge case acquisition time with performance accuracy. Each firearm in the IBIS® database at West Virginia University® is represented by approximately 100 fired cartridge case entries. Random samples of cartridge cases were taken separately from the breech face match score and firing pin match score lists. This subset was compared to the total match distribution of the firearm using a hybrid equivalence test to determine if the subset of similarity scores were statistically equivalent to the larger distribution of scores. For the sampled distribution to remain above 80% equivalent to the match distribution, a minimum of 15 cartridge cases should be used to model the match distribution, based on IBIS® scores. Thirty cartridge cases is a conservative estimate, allowing one to determine that the location and dispersion of the match and sampling distributions are equivalent with nearly 100% probability.","tags":["monte carlo","equivalence testing","forensics","test fires"],"title":"Determining the number of test fires needed to represent the variability present within 9mm Luger firearms","type":"publication"},{"authors":["Casey Jelsema","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1479513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479513600,"objectID":"b39d1577f8cf960898d166be326f8ad4","permalink":"/publication/2016-clme/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2016-clme/","section":"publication","summary":"In many applications researchers are typically interested in testing for inequality constraints in the context of linear fixed effects and mixed effects models. Although there exists a large body of literature for performing statistical inference under inequality constraints, user friendly statistical software implementing such methods is lacking, especially in the context of linear fixed and mixed effects models. In this article we introduce **CLME**, a package in the **R** language that can be used for testing a broad collection of inequality constraints. It uses residual bootstrap based methodology which is reasonably robust to non-normality as well as heteroscedasticity. The package is illustrated using two data sets. The package also contains a graphical user interface built using the shiny package.","tags":["order restricted inference","bootstrap"],"title":"CLME An R package for linear mixed effects models under inequality constraints","type":"publication"},{"authors":["Thomas (Joost) Van't Erve","Fred Lih","Casey Jelsema","Leesa Deterding","Thomas Eling","Ronald Mason","Maria Kadiiska"],"categories":["publication","collaborative"],"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"d9c3553108ffc3099a82e9d384d547fc","permalink":"/publication/2016-joost/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2016-joost/","section":"publication","summary":"Oxidative stress is elevated in numerous environmental exposures and diseases. Millions of dollars have been spent to try to ameliorate this damaging process using anti-oxidant therapies. Currently, the best accepted biomarker of oxidative stress is the lipid oxidation product 8-iso-prostaglandin F2α (8-iso-PGF2α), which has been measured in over a thousand human and animal studies. 8-iso-PGF2α generation has been exclusively attributed to nonenzymatic chemical lipid peroxidation (CLP). However, 8-iso-PGF2α can also be produced enzymatically by prostaglandin-endoperoxide synthases (PGHS) in vivo. When failing to account for PGHS-dependent generation, 8-iso-PGF2α cannot be interpreted as a selective biomarker of oxidative stress. We investigated the formation of 8-iso-PGF2α in rats exposed to carbon tetrachloride (CCl4) or lipopolysaccharide (LPS) using the 8-iso-PGF2α/PGF2α ratio to quantitatively determine the source(s) of 8-iso-PGF2α. Upon exposure to a 120mg/kg dose of CCl4, the contribution of CLP accounted for only 55.6±19.4% of measured 8-iso-PGF2α, whereas in the 1200mg/kg dose, CLP was the predominant source of 8-iso-PGF2α (86.6±8.0% of total). In contrast to CCl4, exposure to 0.5mg/kg LPS was characterized by a significant increase in both the contribution of PGHS (59.5±7.0) and CLP (40.5±14.0%). In conclusion, significant generation of 8-iso-PGF2α occurs through enzymatic as well as chemical lipid peroxidation. The distribution of the contribution is dependent on the exposure agent as well as the dose. The 8-iso-PGF2α/PGF2α ratio accurately determines the source of 8-iso-PGF2α and provides an absolute measure of oxidative stress in vivo.","tags":["order restricted inference","bootstrap"],"title":"Reinterpreting the best biomarker of oxidative stress: The 8-iso-prostaglandin F2α/prostaglandin F2α ratio shows complex origins of lipid peroxidation biomarkers in animal models","type":"publication"},{"authors":["Rajib Paul","Casey Jelsema","Rex Lau"],"categories":["publication","methods"],"content":"","date":1432166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1432166400,"objectID":"ad6c9f50ae37a69c5c44e910231fbc56","permalink":"/publication/2015-frssm/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2015-frssm/","section":"publication","summary":"In environmental studies, the datasets exhibiting non-Gaussian properties, such as heavier or lighter tails and multimodality, are very common. The research on dealing with such datasets in reduced rank perspectives is very limited. In this chapter, a flexible class of Bayesian reduced rank spatial model is developed that can handle non-Gaussian properties adequately. The spatial model provides the flexibility to deal with such properties through scale mixtures of Gaussian distributions and a two-level marginally noninformative inverse-Wishart prior. A general framework for posterior summaries based on Markov Chain Central Limit Theorem (MCCLT) has been developed and conditions of MCCLT on ergodic averages are theoretically verified. The Monte Carlo standard errors based on MCCLT are computed using batch-mean method. The performance of the proposed model and method are assessed using several simulated datasets and a dataset on daily maximum of total column ozone obtained from National Aeronautic and Space Administration Terra satellite.","tags":["spatial"],"title":"A flexible class of reduced rank spatial models for large non-Gaussian datasets","type":"publication"},{"authors":["Casey Jelsema","Rajib Paul"],"categories":["publication","methods"],"content":"","date":1375142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375142400,"objectID":"ed3635726a97802e43fbe1de050476a1","permalink":"/publication/2013-lognormal-coal/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2013-lognormal-coal/","section":"publication","summary":"We analyze data on the geochemical make-up of coal samples throughout the state of Illinois. The goal is to estimate the geochemical properties at unobserved locations over a specified region. Multivariate spatial modeling requires characterization of both spatial and cross-spatial covariances. Reduced rank spatial models are popular in analyzing large spatial datasets. We develop a multivariate spatial mixed effects model for log-normal processes and show how to implement with compositional data to predict on point locations, as well as the average prediction over a finite area. We use log-normal kriging for the components of compositional data, and show how to obtain estimates and measures of precision in isometric log-ratio coordinates.","tags":["spatial"],"title":"Lognormal block kriging with applications to goal geology","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"Experience / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Experience / Contact","type":"widget_page"}]