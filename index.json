[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a Statistician, currently working as part of a statistics group supporting an engineering research lab. My main research interests are spatial statistics and statistical computing, but I also enjoy \u0026ldquo;playing in everyone else\u0026rsquo;s backyard\u0026rdquo; (i.e. collaborations). Some of the backyards I\u0026rsquo;ve played in - from my previous gig as an assistant professor at West Virginia University - include Forensics and medical applications.\n","date":1590883200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1590883200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a Statistician, currently working as part of a statistics group supporting an engineering research lab. My main research interests are spatial statistics and statistical computing, but I also enjoy \u0026ldquo;playing in everyone else\u0026rsquo;s backyard\u0026rdquo; (i.e. collaborations). Some of the backyards I\u0026rsquo;ve played in - from my previous gig as an assistant professor at West Virginia University - include Forensics and medical applications.","tags":null,"title":"Casey Jelsema","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["stat101","methods","lecture"],"content":"\r\r\r\rIntroduction\rIn a previous post I talked about estimating a binomial proportion, including for rare events. The reason I wrote that was for background to this post. Here, we’ll again be looking at estimating proportions - and can include rare events - but with an added wrinkle: A population that is finite.\nIn the Binomial case, every observation is assumed to be independent and have a fixed probability of the outcome of interest (a “success” or a “failure”). But when the population is finite, then the probability of success changes. One of the quintessential examples is dealing cards from a well-shuffled standard deck without replacement (that is: draw the card, and don’t put it back). Say we’re interested in the probability of getting a spade. On the first draw, this is 13/52. But the second card is either 13/51 (if the first card was not a spade) or 12/51 (if the first card was a spade), neither of which are the same as 13/52.\nSampling binary outcomes (e.g., “spade” vs “not a spade”) from a finite population gives rise to the hypergeometric distribution. This is defined as:\n\\[\rP(X = k) = \\dfrac{\\binom{M}{k}\\binom{N-M}{n-k}}{\\binom{N}{n}}\r\\]\nwhere \\(k \\in \\max(0,n+M-N)\\) and\n\r\\(N\\) is the size of the population.\r\\(M\\) is the number of successes (or whatever event of interest) in the population.\r\\(n\\) is the number of samples taken.\r\\(k\\) is the number of successes in the sample.\r\rWhat this is doing is just counting the number of ways, when drawing \\(n\\) samples, to get \\(k\\) successes.\nThe same situation arises in other - very practical - scenarios.\nCars have a specific year and model. Once the next “year-model” starts, no more of the previous year’s version can be manufactured. Hence, there are a finite number of, say, 2011 Chevy Cruzes available. If a problem is detected with that year-model, then the number of 2011 Chevy Cruzes which have this problem will follow a hypergeometric distribution.\nFor example, my own vehicle was subject to the Takata Corp airbag recall. From that wiki page, I found there were recalls of Chevy Cruze models as well.\nGeneralizing this idea, many components are manufactured in “batches” of some sort. That could be particular date(s) of production or batchs of raw materials. Additionally, there could be errors in the manufacturing or assembly which could potentially affect some but not all of the units produced on that day.\n\rEstimation\rWhen the population is finite, there are generally two possible questions of interest. Looking at the parameters of the hypergeometric distribution, this should become relatively clear: Since \\(x\\) and \\(n\\) are values from the sample (which is observed), they are fully known and not subject to uncertainty. Instead, either \\(M\\) or \\(N\\) will be the unknown quantity (hopefully not both!). In this post, I’m going to focus on the former, so the scenario can be phrased as:\n\rFor a population of size \\(N\\), we take a sample of size \\(n\\) and observe \\(x\\) defective units. Our goal is to estimate \\(M\\), the total number of defective units.\n\rWith a finite population, interest in the total number of defectives is directly related to interest in the population proportion of defectives. If we have \\(M\\) defectives in a population of size \\(N\\), then the population proportion of defectives is \\(p = M/N\\). So if we estimate \\(M\\) (perhaps with some interval bounds) we can easily convert to a proportion by dividing by the non-random population size \\(N\\). We could also go the other way, but \\(p\\) is more interpretable than \\(M\\), so it’s more natural to put results in terms of \\(p\\).\nSo how do we estimate the proportion? As with the case of infinite populations I discussed in my previous post, there are several approaches. One of them is the same as the “typical” approach: Divide the observed number of defectives by the sample size, \\(\\hat{p} = k/n\\).\nA wrinkle in this, however, is that when the population is finite, the samples are not independent, there is some covariance. As a result of this, the standard errors from before are no longer correct. One way of addressing this is through what’s called the Finite Population Correction Factor (FPCF).\nFinite population correction factor\rI think the “easy” way to see how the FPCF come about is to look at the mean and variance of the hypergeometric distribution. These are:\n\\[\r\\begin{aligned}\rE[X] = \\mu \u0026amp;= n \\dfrac{M}{N} \\\\\r\u0026amp; \\\\\rV[X] = \\sigma^{2} \u0026amp;= n \\dfrac{M}{N} \\dfrac{N-M}{N} \\dfrac{N-n}{N-1} \\end{aligned}\r\\]\nRemember that the hypergeometric distribution is dealing with the number of successes (in our case, defective units), so we take \\(X/n\\) to deal with the proportion. This factor of \\(1/n\\) gets squared in the variance, so we have:\n\\[\r\\begin{aligned}\rE[X/n] = p \u0026amp;= \\dfrac{M}{N} \\\\\r\u0026amp; \\\\\rV[X/n] = \\sigma^{2}_{p} \u0026amp;= \\dfrac{1}{n} \\dfrac{M}{N} \\dfrac{N-M}{N} \\dfrac{N-n}{N-1} \\end{aligned}\r\\]\nDividing the number of defects by the sample size gets us the proportion, so in the expected value, this becomes the population proportion, \\(p = M/N\\), which makes sense. If we rewrite the variance in terms of \\(p\\), we get:\n\\[\r\\sigma^{2}_{p} = \\dfrac{1}{n} p(1-p) \\dfrac{N-n}{N-1} = \\dfrac{p(1-p)}{n} \\left(\\dfrac{N-n}{N-1}\\right)\r\\]\nThis expression looks very close to the “ordinary” version of the standard error of \\(\\hat{p}\\) when sampling from a Binomial distribution. There’s just an extra bit that the variance is getting multiplied by. This is the FPCP:\n\\[\r\\mbox{FPCP} = \\dfrac{N-n}{N-1}\r\\]\nThere is another derivation of this for a more general case (just assuming a mean and a variance), but it’s fairly long, so I don’t want to get side-tracked with that.\n\rBayesian estimation\rAs before, I’ve been interested in the Bayesian approach to this problem. There’s a paper by Jones \u0026amp; Johnson (2015) that talks about this (as a precursor to a more complex idea). In section 2 of their paper, they have a nice summary of the approach using a concept called superpopulation. This is a way of describing a theoretical population from which our population is drawn.\nClear as mud, right? I think of it this way: We have a population of \\(N\\) units, of which \\(K\\) are defectives (and so there is proportion \\(p\\) defectives). But we can consider a more general process from which our population was drawn. In this more general “superpopulation” there is some underlying proportion of defectives, which we will denote \\(\\pi\\). With this framework, we can consider the population as being a sample of size \\(N\\) from the superpopulation.\nThe next important bit of this is that we separate the population into the observed and unobserved samples. These are effectively two independent samples from a Binomial distribution, of sizes \\(n\\) and \\(N-n\\), respectively. The diagram below is how I picture it.\n\r{\"x\":{\"diagram\":\"digraph {\\n graph [layout = dot, rankdir = LR]\\n\\n node [shape = rectangle, style = filled, fillcolor = lightblue]\\n A [label = \\\"Superpopulation\\n\\npi\\\", shape = circle, width=3, fontsize=28]\\n \\n \\n subgraph cluster_population {\\n style=dotted; label=\\\"Population\\\"; fontsize=24;\\n node [shape = rectangle, style = filled, fillcolor = lightblue, width=2]\\n { \\n B [label = \\\"Observed Sample\\n\\nx, n\\\", shape = circle, fontsize=20]\\n C [label = \\\"Unobserved Sample\\n\\nM-x, N-n\\\", shape = circle, fontsize=20]\\n }\\n };\\n\\n # edge definitions with the node IDs\\n A - B \\n A - C \\n B - A [style=dashed]\\n A - C [style=dashed]\\n\\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\rIn this, a solid arrow denotes the random sampling, and a dashed arrow denotes inference. So from the superpopulation we sample the population, but we conceptualize two independent samples: One which we observe, and the other which we do not. Then the observed sample is used to infer about the superpopulation, and that information about the superpopulation is used to make probabalistic statements about the unobserved sample - that is: The rest of the population.\nJones \u0026amp; Johnson (2015) tell that inference about \\(U = M-x\\) (from which we can derive \\(M\\), and therefore \\(p\\)) will make use of a Beta-binomial distribution, which I’ll denote \\(betabin( N, \\alpha, \\beta)\\). This distribution has the following PDF:\n\\[\rP(U=k) = \\binom{N}{k} \\dfrac{B\\left(k+\\alpha, N-k+\\beta \\right)}{B\\left(\\alpha, \\beta \\right)},\r\\]\nwhere \\(B(\\cdot,\\cdot)\\) is the Beta function (a core feature of the Beta distribution). It will be useful to see the form of this:\n\\[\rB\\left(\\alpha, \\beta \\right) = \\int_{0}^{1} x^{\\alpha-1} (1-x)^{\\beta-1} dx\r\\]\nThe Beta distribution is obtained “simply” by dividing both sides by \\(B\\left(\\alpha, \\beta \\right)\\) so that the integral comes out to \\(1\\).\nSo how does the math work out? We will assuming the following likelihoods and prior:\n\\[\r\\begin{aligned}\rk|\\pi \u0026amp;\\sim \\mbox{Bin}\\left( n, \\pi \\right) \\\\\rU|\\pi \u0026amp;\\sim \\mbox{Bin}\\left( N-n, \\pi \\right) \\\\\r\\pi \u0026amp;\\sim \\mbox{Beta}\\left( \\alpha, \\beta \\right)\r\\end{aligned}\r\\]\nThen to get the distribution of \\(U|k\\), we can say the following:\n\\[\r\\begin{aligned}\rf(U|k) \u0026amp;\\propto f(U|\\pi) f(\\pi | k) \\\\ \u0026amp;= f(U|\\pi) \\times \\left[f(k | \\pi) f(\\pi)\\right]\r\\end{aligned}\r\\]\nThe second term on the second line is mainly for clarity of what we’re doing. We already know the posterior distribution of a proportion when sampling from a Binomial distribution (since we walked through it in the previous post with infinite populations). With the likelihoods and priors denoted above, we can say that \\(\\pi|k \\sim \\mbox{Beta}( k+\\alpha, n-k+\\beta)\\).\nNow, the idea is to get at the distribution of \\(U\\), so conceptually we could think of doing the following (which is why the Beta-binomial can be considered a Binomial distribution for which the probability of success is unknown and randomly drawn from a beta distribution):\n\\[\rf(U) = \\int_{0}^{1} f(U|\\pi) f(\\pi)d\\pi\r\\]\nBut \\(f(\\pi)\\) is selling ourselves short; we have more information than the prior, right? So really what we’re doing is:\n\\[\rf(U|k) = \\int_{0}^{1} f(U|\\pi) f(\\pi|k) d\\pi\r\\]\nThis allows us to take advantage of what \\(k\\) and \\(n\\) told us about \\(\\pi\\). The dependence on \\(k\\) gets “inherited” from \\(f(\\pi|k)\\). The math is below, though for ease of notation, I’m going to write \\(\\alpha^{*}=k+\\alpha\\) and \\(\\beta^{*} = n-k+\\beta\\).\n\\[\r\\begin{aligned}\rf(U|k) \u0026amp;= \\int_{0}^{1} f(U|\\pi) f(\\pi|k)f(\\pi|k) d\\pi \\\\\r\u0026amp; \\\\\r\u0026amp;= \\int_{0}^{1} \\binom{N-n}{u} \\pi^{u}(1-\\pi)^{N-n-u} \\dfrac{1}{B(\\alpha^{*}, \\beta^{*})} \\pi^{\\alpha^{*}-1} (1-\\pi)^{\\beta^{*}-1} d\\pi\r\u0026amp; \\\\\r\u0026amp;\\\\\r\u0026amp;= \\binom{N-n}{u}\\dfrac{1}{B(\\alpha^{*}, \\beta^{*})} \\int_{0}^{1} \\pi^{u+\\alpha^{*}-1}(1-\\pi)^{N-n-u+\\beta^{*}-1} d\\pi\r\\end{aligned}\r\\]\nIf you look back to the definition of the Beta function, you’ll note that the integral here is exactly that, so we can simplify this to:\n\\[\r\\begin{aligned}\rf(U|k) \u0026amp;= \\binom{N-n}{u}\\dfrac{1}{B(\\alpha^{*}, \\beta^{*})} B( u + \\alpha^{*}, N-n-u+\\beta^{*})\r\\end{aligned}\r\\]\nThis, we can note, is precisely the form of the Beta-binomial distribution. So we can say that:\n\\[\rU|k \\sim \\mbox{betabinomial}( N-n, k+\\alpha, n-k+\\beta)\r\\]\nFrom the Beta-binomial distribution we can get values such as the median of \\(U|x\\), or quantiles to form a credible interval. Hence, the Bayesian approach offers an alternative form of point and interval estimation.\n\r\rComparison\rAs before, I will compare the methods computationally, using the coverage probability as the metric of interest. This will need to be a bit more elaborate, since we have added another parameter, \\(N\\), into the mix. As before, I’m calculating exact coverage probabilities, though now the hypergeometric distribution is used to find the probability that the number of defects is within the proper range.\nSince the “standard” method was empirically worse than the others for the infinite population case, I’m no longer considering it, and will only compare the following three approaches:\nWilson interval with FPFC\rAgresti-Coull interval with FPFC\rBayesian interval\r\rAs with the infinite-population case, these three intervals behave fairly similarly, with the Agresti-Coull interval being perhaps over-conservative (wider interval) in some cases. What is interesting to me is that the Bayesian interval dropped in coverage probability when the sampling rate was small (notably, the \\(N=1000, n=10\\) case). This will hopefully be a fairly rare occurrence - and in my practice has been - so I wouldn’t necessarily rule out the Bayesian interval.\n\rSummary\rThis was actually the post that I wanted to write, but I thought the infinite population case needed to be covered first. Maybe that’s just my inner professor coming out and wanting to build concepts from the ground up. Anyway, I hope these posts were interesting, they were for me!\n\r","date":1623283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606491847,"objectID":"e976b9979aacc71d038fd2e3a7e24074","permalink":"/post-stats/estimating-proportions-for-finite-populations/","publishdate":"2021-06-10T00:00:00Z","relpermalink":"/post-stats/estimating-proportions-for-finite-populations/","section":"post-stats","summary":"Introduction\rIn a previous post I talked about estimating a binomial proportion, including for rare events. The reason I wrote that was for background to this post. Here, we’ll again be looking at estimating proportions - and can include rare events - but with an added wrinkle: A population that is finite.\nIn the Binomial case, every observation is assumed to be independent and have a fixed probability of the outcome of interest (a “success” or a “failure”).","tags":["categorical","proportion","hypergeometric","rare events"],"title":"Estimating proportions for finite populations","type":"post-stats"},{"authors":[],"categories":["politics","opinion"],"content":"\rIntroduction\rThis is part 1 of a 3-part bit on voting and elections.\nFirst of all, a disclaimer: I’m not a political scientist, I am not an expert on the constitution or any other relevant field. I’m commenting here just as an individual. These ideas are either my own, or ideas that I’ve come across and like. This is very much an “armchair expert” situation. That being said, I’d like to share my thoughts on the electoral process in the United States, and an idea for alternatives.\nA YouTuber/podcaster who goes by CGP Grey has done a series of five short videos, titled\rPolitics in the Animal Kingdom that I think does a fantastic job explaining methods of voting and some problems that arise with some of them. In this post, I’ll be summarizing some of these voting methods, but I highly recommend watching those videos.\nInitially this may seem odd to some people. Voting is voting, right? You cast your ballot for who you support, and then whoever gets the most votes wins. While that is one way of voting, it is not the only way (and actually has some significant drawbacks).\n\rPlurality voting\rThe current approach used in most races in the United States uses a plurality system\r(sometimes called a first-past-the-post (FPTP) system) of voting.\rEveryone votes for a single candidate, and to determine the winner we just look at who got the most votes. They win even if they didn’t get a majority. In case that seems odd, it’s because a majority would be over 50% of votes, while a plurality just means a larger percentage than any other individual candidate.\nFor example, in a race with 3 candidate (Calvin, Hobbes, and Suzie), suppose Calvin gets 40% of the vote, Hobbes gets 35%, and Suzie gets 25%. Calvin would win by plurality, even though he did not get a majority. Even if all of Suzie’s voters preferred Hobbes to Calvin (so that, if Suzie was not in the race, Hobbes would get 60% - a majority), Calvin still wins.\nThis is how virtually every senator and representative in the country is selected (Maine was the first to use a different method in the 2018 election, and Alaska approved a different method in the 2020 election). There are several problems with plurality voting.\n\rSome Problems with plurality\rNo majority\rFirst, as demonstrated in the little example above, a plurality system does not guarantee that the winner gets a majority of votes, or is the most broadly preferred candidate (which casts a shadow on the concept of “majority rule”).\n\rDuopoly\rSecond, a plurality system tends to lead to a 2-party system, which we’ve seen for a long time now. If you don’t like the 2-party system, then you should hate a FPTP system of voting.\nThe example hinted at this: If all of Suzie’s supporters would have preferred Hobbes to Calvin, then eventually they will grow tired of losing elections to their least-favorite candidate. Instead of voting for their favorite, they’ll vote a candidate that is still acceptable to them, but more likely to win. This is known as Duverger’s law, and it is how we wind up with two “big tent” parties, and why third parties are all but invisible: Since they are not likely to win, people who agree with the third party will instead vote for one of the two major-party candidates, making it even less likely for the third party to win, which leads even more supporters to vote for the main parties.\nTo be clear: I do not subscribe to the notion that a third party vote is a “wasted” vote. However, I think that it needs to be understood as a protest vote - an indication that there are engaged voters who are unsatisfied with both main parties - rather than a real attempt to elect a candidate into office.\n\rGerrymandering\rA third problem with plurality voting is the prospect of gerrymandering. I don’t think that gerrymandering is technically limited to plurality voting, but plurality voting in single-member districts (FPTP) is particularly susceptible to it.\nTo back up slightly: While it may seem like there is “an election,” in reality it’s more like 534 separate elections: 1 president, 100 senators, and 435 house representatives. Well, sort of. Only 1/3 of the senate is up for election at any given election, but that doesn’t really matter for this discussion.\nSenators are elected from a state-wide vote, but house representatives are elected from a district within a state. These districts are not fixed things. Within a state they are supposed to be approximately equal population, and so as the population changes, the districts may change. Gerrymandering, also called “cracking and packing”, is the tactic by which these districts are drawn to favor one part or the other. The basic idea is that you “crack” the opposing party’s support across many districts, to dilute its effect. If it’s impossible to fully achieve that, you “pack” as many of the opposing party into a single district as possible, so that they are guaranteed to win that district, but with many of their votes concentrated there, they are weaker in multiple other districts.\nConsider a square state with 81 voters who need to be divvied up into 9 districts, each with 9 voters. These voters vote for one of two parties, Blue or Gold. In the figure below, I’ve randomly distributed the votes around the state. The vote tallies are as evenly divided as possible: 40 for Blue, and 41 for Gold.\nNow, we need to draw districts. With no knowledge of this fictional state, we might just draw 9 boxes, no? That would be one approach. It turns out that doing so, the parties each win roughly half of the districts, which seems more or less fair (and, since I distributed the votes randomly, should be expected).\nHowever, say the Blue party gets to draw the districts, and they decide to gerrymander to give themselves an advantage. Their target should be to create as many districts with 5 Blue votes as possible.\nHow might they do that? Well, I just came up with some gerrymandered lines myself. I noticed a block of Gold votes that was all connected, so I put them ALL into a single district (“packing”), giving Gold a guarantee to win that district. However, that siphons off a lot of the Gold votes, allowing me to spread the rest of them out so that Blue gets all of the remaining districts.\nSo even though they only received roughly (and actually just under) 50% of the vote, Blue was able to claim nearly all but one district, for roughly 90% of the representation. That seems wildly unfair. And I’m not even an expert at this! I just looked at this map for a little bit and drew some lines.\n\r\rSummary\rSo, voting systems that can be gerrymandered have this big flaw. We’d prefer a system that is immune to gerrymandering. You can continue reading my take on some alternative methods in Voting Methods Part 2.\n\r","date":1605916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606003898,"objectID":"c94fae3f44f79c42549459a3cffbfd11","permalink":"/post-other/voting-methods-part-1-fptp/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/post-other/voting-methods-part-1-fptp/","section":"post-other","summary":"Introduction\rThis is part 1 of a 3-part bit on voting and elections.\nFirst of all, a disclaimer: I’m not a political scientist, I am not an expert on the constitution or any other relevant field. I’m commenting here just as an individual. These ideas are either my own, or ideas that I’ve come across and like. This is very much an “armchair expert” situation. That being said, I’d like to share my thoughts on the electoral process in the United States, and an idea for alternatives.","tags":["voting","elections"],"title":"Voting methods part 1: FPTP","type":"post-other"},{"authors":[],"categories":["politics","opinion"],"content":"\r\rIntroduction\rThis is part 2 a 3-part bit on voting and elections.\nAs before, the disclaimer that I’m not a political scientist, I am not an expert on the constitution or any other relevant field. I’m commenting here just as an individual. Also, a reminder that CGP Grey’s videos Politics in the Animal Kingdom are fantastic, and a much better explanation that I give here.\n\rAlternative voting systems\rIn Voting Methods Part 1 I talked about FPTP / plurality and some problems such as gerrymandering that can arise. So if we don’t want a system like that, what are some alternative methods that might address these problems?\nRanked choice voting\rCurrently, most Americans vote by selecting one candidate from a set as “The person they vote for.” But that’s not the only way to go about it. Think back to the example with Calvin, Hobbes, and Suzie - every Suzie voter preferred Hobbes over Calvin. We could then say that their preference order was: Suzie -\u0026gt; Hobbes -\u0026gt; Calvin. So they could fill out their ballot by ranking these candidates, e.g.\nRank\nCandidate\n3\nCalvin\n2\nHobbes\n1\nSuzie\n\rThis way, when counting ballots, if the voter’s preferred candidate (in this case, Suzie) does not win, their vote goes to the second most preferred candidate, in this case Hobbes. In this way, it guards against the “wasted vote” because if you vote for someone who has a low chance of winning (like a third party), your vote will still contribute to who ultimately wins the election.\nThere are multiple ways to tally the votes. One way is known as the Borda count (though there are actual multiple ways to implement a Borda count). In the Borda count, each position is assigned some number of points. For a 3-person race, we might assign 3 points for a 1st-place rank, 2 points for a 2nd-place rank, and 1 point for a 3rd-place rank.\nBorda count: Example 1\rGoing back to the example, we’d have a situation that looks like this:\nVotes\nCHS\nHSC\nSHC\nProportion\n40%\n35%\n25%\nRank 1\nCalvin\nHobbes\nSuzie\nRank 2\nHobbes\nSuzie\nHobbes\nRank 3\nSuzie\nCalvin\nCalvin\n\rThis may seem a bit odd. The three columns at the right denote the three unique rankings of candidates that were cast. There are three more possible orders of the preference ranking, which I’ll consider in a second example. The numbers at the top represent the proportion of voters who cast a ballot with that order.\nFor now, I’ve just extended the votes so that:\n\rAll of Calvin’s voters prefer Hobbes to Suzie\rAll of Hobbes’ voters prefer Suzie to Calvin.\rAll of Suzie’ voters prefer Hobbes to Calvin.\r\rWe can see this as a split vote on one side, creating a “spoiler effect.”\nJust by inspection, we can kind of see that Hobbes is a more broadly popular candidate, right? He’s ranked 1st or 2nd, while Calvin has only 1st and 3rd place votes. Applying the Borda count would be a weighted average of the points (1st = 3 points, 2nd = 2 points, 3rd = 1 point), using the proportion of votes at each rank as the weight. For example, to get Suzie’s borda count, we took a weighted average of the ranks:\n\\[\rBC_{S} = (0.40 \\times 1) + (0.35 \\times 2) + (0.25 \\times 3)\r\\]\rApplying the same to all of the candidates, we would get the following results:\nCandidate\nCount\nCalvin\n1.80\nHobbes\n2.35\nSuzie\n1.85\n\rAnd so Hobbes would be declared the winner of the election.\n\rBorda count: Example 2\rGoing to a slightly more complex ballot, let’s say that Calvin and Suzie are the “extreme” candidates, while Hobbes is a more centrist candidate. So Hobbes’ voters second choice are a bit more split, but Suzie’s and Calvin’s would primarily go to Hobbes.\nVotes\nCHS\nCSH\nHCS\nHSC\nSHC\nSCH\nProportion\n35%\n5%\n20%\n15%\n20%\n5%\nRank 1\nCalvin\nCalvin\nHobbes\nHobbes\nSuzie\nSuzie\nRank 2\nHobbes\nSuzie\nCalvin\nSuzie\nHobbes\nCalvin\nRank 3\nSuzie\nHobbes\nSuzie\nCalvin\nCalvin\nHobbes\n\rThen applying the Borda count, we would get:\nCandidate\nCount\nCalvin\n2.05\nHobbes\n2.25\nSuzie\n1.70\n\rSo while Calvin is now a bit more popular than Suzie, Hobbes still wins the election.\n\rBorda count: Example 3\rOne more example, just because I wrote a function to turn these ballots into a borda count, and I want to get my use out of it, let’s say that Hobbes is actually in last place when considering strictly first-place votes. To put that in real terms: People are voting for the “extreme” candidates of their party (Calvin with his G.R.O.S.S. club, Suzie with her tea parties), but have a more moderate/centrist as a backup choice (Hobbes just wants a tuna sandwich for everyone).\nVotes\nCHS\nHCS\nHSC\nSHC\nProportion\n45%\n5%\n5%\n45%\nRank 1\nCalvin\nHobbes\nHobbes\nSuzie\nRank 2\nHobbes\nCalvin\nSuzie\nHobbes\nRank 3\nSuzie\nSuzie\nCalvin\nCalvin\n\rSo here, only 10% of voters put Hobbes first, while 90% voted for their preferred “extreme” candidate. But when we tally up the Borda count, we get:\nCandidate\nCount\nCalvin\n1.95\nHobbes\n2.10\nSuzie\n1.95\n\rSo Hobbes still wins the day, because enough votes from either side thought he was an acceptable backup.\nUsing a Borda count isn’t the only way to tally ranked ballots, but it’s an example of how to better represent the desires of the voters compared to plurality.\n\r\rProportional representation\rAnother method that I, at least currently, am interested in is proportional representation. There are multiple ways to implement this, such as the Single Transferable Vote or\rMixed-member proportional representation, and some of them can get a little complicated, but the basic idea is that the single-member districts gets pooled together, and the representatives are allocated to (approximately) align with the result of the vote.\nOne simple (possibly naive) way of implementing is to note how much percent of the population each representative is supposed to represent. If there are 10 representatives to elect, then to be elected, a candidate should need 10% of the vote. If no candidates get to the 10% threshold, then the least-popular candidate is removed and (using ranked voting) their votes would be transferred to their next ranked candidate. The CGP Grey video on Single Transferable Vote has a nice demonstration of this.\nOne very attractive feature of this this method is that it renders gerrymandering difficult or impossible, because there are no longer districts to be redrawn: All representatives are elected from a single “district”, the state (though I suppose it’s possible to have a couple multi-member districts for different regions of larger states like New York, California, Texas, etc). It also enables some more diversity, possibly allowing a third party to get a representative or two. For example, in California with it’s 53 house representatives, each represents approximately 1.9% of voters, so depending on the implementation, a party might earn a representative by getting a (very realistic) 2% of the vote (and if this system was implemented, the share of third-party votes could feasible increase).\nSTV: Example\rLet’s say that a district needs 5 representatives - so each represents 20% of the vote - and the vote breakdown is shown below.\nClearly Blue and Gold get a representative, right? But then how are the other three representatives allocated? Nobody got 40%, so there’s not really an argument that Blue or Gold should get two representatives (yet). One way of going forward is to eliminate take the party that got the least votes and allocate them to their second choice. In this case, let’s say that Light Blue voters would have otherwise voted for Blue. So the new picture looks like:\nNow with 40% of the vote, it seems reasonable to give Blue two of the five representatives. But we still have two more, with Green and Purple being tied for last place. If they both would have opted for Gold otherwise, then we would add them to Gold’s tally, as shown below.\nNow it seems rational to give Blue two representatives, and give Gold three representatives, thus proportionally allocating the representatives according to the actual vote of the people.\nNow, you might say that if Green and Purple are “close” to each other, that they might instead group together and get a representative that’s more aligned with them rather than someone from Gold. That’s true. What I showed here is just one possible way to go about allocating representatives, and it’s knowingly a bit simplistic just to illustrate the point. There are more sophisticated methods for proportional allocation and STV which can help with this.\nIt’s also rather unlikely that vote tallies will wind up being so nicely aligned with whatever threshold is needed to claim one of the representatives, so there will be some degree of mismatch between the vote and the allocation of representatives. Generally speaking, the more representatives in a region (e.g., State), the more able the system is to reflect the makeup of the people.\n\r\r\rSummary\rIn Voting Methods Part 1 I described plurality voting and some problems with it. Here in Part 2 I’ve talked a bit about some alternatives. These were both “background” for what I actually wanted to talk about in Voting Methods Part 3, some changes to the electoral process in the United States that I’d like to see.\n\r","date":1605916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606003898,"objectID":"44c82e9332a7997525004cd6f8d61af3","permalink":"/post-other/voting-methods-part-2-rcv/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/post-other/voting-methods-part-2-rcv/","section":"post-other","summary":"Introduction\rThis is part 2 a 3-part bit on voting and elections.\nAs before, the disclaimer that I’m not a political scientist, I am not an expert on the constitution or any other relevant field. I’m commenting here just as an individual. Also, a reminder that CGP Grey’s videos Politics in the Animal Kingdom are fantastic, and a much better explanation that I give here.\n\rAlternative voting systems\rIn Voting Methods Part 1 I talked about FPTP / plurality and some problems such as gerrymandering that can arise.","tags":["voting","elections"],"title":"Voting methods part 2: RCV","type":"post-other"},{"authors":[],"categories":["politics","opinion"],"content":"\r\rIntroduction\rThis is part 3 of what a 3-part bit on voting and elections.\nIn Voting Methods Part 1 I described the plurality voting system that’s used in the United States, and some problems with it. In Voting Methods Part 2 I talked a bit about some alternatives, including ranked voting and proportional representation. These were both “background” for what I actually wanted to talk about now: Changes that I’d like to see in the electoral process in the United States.\nWhile it may seem like there is “an election” in the United States, in reality it’s really more like 534 separate elections: 1 president, 100 senators, and 435 house representatives. Well, sort of, only 1/3 of the senate is up for election at any given election, but that doesn’t really matter for this discussion.\nThis post will be structured as follows:\n\rDescribe the current structure of representatives and how they are elected.\rDescribe a bit of the history of why things are the way they are.\rTalk about some methods of voting, namely to highlight the drawbacks of the existing system.\rDescribe my proposed alternative method of conducting elections.\r\r\rCurrent Structure\rSenate\rEvery state gets 2 senators, regardless of size (by which we mean population size). The senators are elected by a state-wide vote. Whoever wins a plurality gets elected.\n\rHouse of Representatives\rEvery state is apportioned some number of representatives based on population, though it’s not perfect.\rOriginally, each state received a representative for every 30,000 people, so the house grew with the population. The Reapportionment Act of 1929 put a cap on the house of 435 members. Furthermore, the Uniform Congressional District Act of 1967 required that house representatives be elected from single-member districts. Apparently Hawaii and New Mexico are exempt, though still use single-member districts.\nWhat this means is that each state gets divided into some number of districts, which should be approximately equal in population, and there is a race within that district to determine its representative. These races are, like the senate, decided by plurality.\n\rPresident\rFor electing the president, we use the electoral college (EC). In this, each state gets a number of electoral votes (EVs): One for each senator, and one for each house representative. Washington DC also gets 3. The states decide how to divvy these EVs between the candidates. Most states use a winner-take-all approach, where the (again, plurality) winner of the popular vote within the state gets all of the electoral votes for that state. So winning 0% and winning 49% of a state is equivalent: You get zero EV from that state. This leads to some interesting if rather unlikely examples of how someone could become president with just 27% of the vote.\nIn a slight deviation from the trend, the electors then meet to vote for president and vice president, but a candidate needs to get a majority (and not simply a plurality) in order to be elected. This has the possibility to trigger some less well known mechanisms in the electoral process, but is a bit beside the point.\n\r\rA Brief History\rThis system was set up by the original 13 states as a compromise. Back then, we had the Articles of Confederation, which were being reformed. The large states liked the Virginia Plan, which allocated representation on the basis of population: Since they had more people, they’d contribute more tax, and should get more say in the government, or so their thinking went. The small states thought this gave too much power to the large states, and instead favored the New Jersey Plan, which gave every state equal representation. Ultimately the Connecticut Compromise came about, which is why we have 2 senators for each state regardless of size, but the house of representatives is (somewhat) proportional to population. Then, to elect the president, they came up with the electoral college. With this method, the election of the president is a balance of equal representation for all states vs the population.\nThen the Reapportionment Act of 1929 put a cap on the number of house representatives (435), but also ensured all states receive at least one house representative. Originally the senate was supposed to equally weight states, and the house was supposed to be population-based, but with this act the number of representatives was also slightly biased slightly in favor of the small states. Not entirely, but skewed from it’s original intent. Consequently, the electoral college also received this skew. Additionally, the Uniform Congressional District Act of 1967 put some further restrictions on the system.\n\rPersonal pipe dreams\rSo how do I think voting should take place? I’ll cover the senate and house first, since those are easier.\nCongress\rHouse of Representatives\rFor the house, I’d like to see proportional representation, probably by a Single Transferable Vote approach, but really any proportional allocation would be preferable to me.\nI’ll spare the details here, since I talked about them in Part 2, but something like this could actually give third parties a voice in the government, and serve to reduce the political polarization we see. Polarization wouldn’t be completely “solved”, since we’d probably wind up with more, smaller, groups that were more homogeneous, but if they wanted any say in running things, they would have to compromise.\nHowever I would like to see the house returned to a better proportional representation, whether it’s by a fixed ratio of population, or by something more like the Wyoming rule where the lowest-population state gets 1 representative, and all other states are allocated representatives in a proportional manner based on relative population. Though instead of the lowest population state getting just one representative, I’d like to see them get 2 or 3, because increasing the number of representatives allows a proportional representation system to better represent the vote of the people.\nNote that from what I have seen, the Reapportionment Act of 1929 didn’t have a dramatic effect on presidential outcomes, but I find it conceptually preferable to having both chambers somewhat skewed towards low-population states.\n\rSenate\rFor the senate, I’d like to see ranked choice voting. These are, functionally, single-member races, so proportional representation doesn’t make sense. This way, if the “mainstream” party put forth someone really unpopular, voters who ordinarily supported that party could vote for someone else that they find less objectionable. Then as a second choice, they could select the unpopular person, or even the other mainstream party candidate.\n\rOther Thoughts\rActually, I’m not sure that I’m entirely opposed to an even larger shakeup of the current system. I’ve seen people complain about the Senate and want to get rid of it altogether. I’m not sure I’d go that far, but I’d like to see the power of the senate reduced somewhat. Currently the Senators vs Population graph looks like this.\nSo half of the senate represents less than 15% of the population of the United States, while over 50% of the population is represented by only 20 senators. Proponents of the equal representation of the senate will say this is a feature, not a bug, because the United States is the union of independent states. I tend to think that the civil war put the final nail in the coffin of that idea. I think having a second chamber of congress to be able to temper legislation and policy is a good idea, but I’m not convinced that the senate in its current implementation and degree of power is the right form of that chamber.\n\r\rPresident\rGo with popular vote\rFor the election of the president, many people are in favor of a popular vote. That’s one approach I’m fine with. A common complaint about this is that just a couple of states (or even a couple of cities) will then determine the election and rule everyone else, so the lower population states won’t get any representation. After all, doesn’t the figure I just made show that? A mere 10 states have 50% of the population.\nWell, not really. There are at least three major points that this complaint overlooks.\nFirst, it kind of ignores an important fact: Sure, just 10 states represent 50% of the population. But that’s 50% of the population. What is a country if not the people who make it up? And, going back to the Calvin and Hobbes example, if half or more of the nation votes for Hobbes, why should should Calvin become president?\nSecond, the president is a single person, just one part of the government. The lower-population states still get represented in the form of the house of representatives and the senate. If the small states think that they are not represented by the president because the large states determined him/her, couldn’t the large states say the same if the president gets determined by the small states? And if it comes to that, shouldn’t a single office holder be more representative of the larger portion of the population?\nThird, an most importantly I think, the complaint overlooks the fact that those 10 states represent quite a lot of diversity.\nLooking at these states (colored by the winner of the 2020 election) we see a mix of traditionally Democratic or Republican leaning states. There is some Pacific coast, some Atlantic coast, some Midwest/Great Lakes, and Texas. It’s far from a homogeneous block. Looking at the presidential vote results over several elections, this becomes even more clear.\nState\nPop\n2012\n\n2016\n\n2020\nDem\nRep\nDem\nRep\nDem\nRep\nCalifornia\n39,538,223\n60.24%\n37.12%\n\n61.73%\n31.62%\n\n63.48\n34.32\nTexas\n29,145,505\n41.38%\n57.17%\n\n43.24%\n52.23%\n\n46.48\n52.06\nFlorida\n21,538,187\n50.01%\n49.13%\n\n47.82%\n49.02%\n\n47.86\n51.22\nNew York\n20,201,249\n63.35%\n35.17%\n\n59.01%\n36.52%\n\n60.86\n37.75\nPennsylvania\n13,011,844\n51.97%\n46.59%\n\n47.46%\n48.18%\n\n50.01\n48.84\nIllinois\n12,812,508\n57.6%\n40.73%\n\n55.83%\n38.76%\n\n57.54\n40.55\nOhio\n11,799,448\n50.67%\n47.69%\n\n43.56%\n51.69%\n\n45.24\n53.27\nGeorgia\n10,711,908\n45.48%\n53.3%\n\n45.64%\n50.77%\n\n49.47\n49.24\nNorth Carolina\n10,439,388\n48.35%\n50.39%\n\n46.17%\n49.83%\n\n48.59\n49.93\nMichigan\n10,077,331\n54.21%\n44.71%\n\n47.27%\n47.5%\n\n50.62\n47.84\n\rOf the 10 states, 6 voted for the same party in 2012 and 2016, while 4 switched. Then from 2016 to 2020, 3 switched. Only 5 of the states had no switches, 2 Republican, 2 Democrat. Additionally, the margins of many of them are quite small, indicating that within these states, there is a lot of diversity. Hence, these states are not a monolithic voting bloc either individually or as a set.\nBecause of all this, I just don’t see the complaint of a couple high-population Democratic states dominating the results as holding water.\n\rModify the electoral college\rOkay, let’s say we think about it, and we still don’t like the popular vote for president (even switching to ranked choice voting), and for whatever reason are sticking to the electoral college. Could we do better within that system? I think so.\nCurrently, all but two states allocate all of their electoral votes to the state’s popular vote winner. But this is not required. As one example, there are two states (Maine and Nebraska) which allocate electoral votes on a per-district basis, with only the two extra votes from the senate seats being given to the popular vote winner. When I first heard of this, I thought it was a great idea: By divvying up the EVs more locally, individual districts would be more competitive, and more people would be engaged in the process. However, I’ve since realized there is a dramatic flaw with this approach. Remember how congressional districts can be gerrymandered? By allocating EVs on a per-district basis, this causes the presidential election to also be subject to gerrymandering. I think this is very, very bad, and so I no longer like the idea of district-based electoral votes.\nInstead, similar to the change I’d like to see with the house, I’m interested in states proportionally allocating their electoral votes to reflect the popular vote within that state. This means that Republicans in “blue states” and Democrats in “red states” don’t have their voice drowned out. There are some drawbacks to this idea, however.\nFirst is that all states would need to simultaneously enact this (or it would need to be done at the federal level). Since this method would only help the person who lost the popular vote in a state, which would tend to be the person who lost the popular vote overall. If only one or two large states did this, then the person who lost the popular vote would be more likely to win the election.\nSecond, it opens up the possibility for some EVs to go to third party candidates. While ordinarily I’d be in favor of getting more diverse representation, the current mechanics of the EC means that if there is no majority, then the election gets sent to congress. If we’re to keep the EC, I’d rather see a bit more effort at that level rather than having the house of representatives decide the president. This would require further changes to the mechanics of elections, perhaps to allow coalitions or negotiations for sharing of power. From my understanding, this is how parliamentary systems work: If no party gets a majority, they need to form a coalition government, so both of those parties get some say in government.\rI’m not sure how that would work in our current system, but I think it’s an idea worth exploring.\nThe advantage of this approach is that it retains the balance of population-weighted vs equal representation that the current electoral college system employs, and which many people do like. It just changes the allocation of the votes and - if the house is reformed as well - moves the dial a bit on the precise balance of size vs equal.\n\r\r\rSummary\rSo, that’s my idea. For senators, a ranked choice ballot should be used. For the house, proportional representation, and for the president either a popular (ranked choice) vote, or failing that a proportional allocation of electoral votes. With these tweaks, I think there could be a substantial reduction in political polarization, as well as a better representation of the will of the people.\nAlso, as a bit of a fun thing, the website 270towin has a tool where you can implement some different methods of allocating electoral votes. I think it’s kind of fun to explore.\n\r","date":1605916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606003898,"objectID":"13819c221a9f71d7a1e55a93545cf42a","permalink":"/post-other/voting-methods-part-3-changes/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/post-other/voting-methods-part-3-changes/","section":"post-other","summary":"Introduction\rThis is part 3 of what a 3-part bit on voting and elections.\nIn Voting Methods Part 1 I described the plurality voting system that’s used in the United States, and some problems with it. In Voting Methods Part 2 I talked a bit about some alternatives, including ranked voting and proportional representation. These were both “background” for what I actually wanted to talk about now: Changes that I’d like to see in the electoral process in the United States.","tags":["voting","elections"],"title":"Voting methods part 3: Tweaking the System","type":"post-other"},{"authors":[],"categories":["stat101","methods","lecture"],"content":"\r\rIntroduction\rThe common approach\rCommon Approach: The math\rWhy the common approach is bad\r\rAlternatives\rAlternative 1: Wilson and Agresi-Coull intervals\rWilson interval: The math\rAlternative 2: Bayesian method\r\rComparisons\rSummary\r\r\rIntroduction\rEstimating a proportion gets covered in virtually every introductory statistics course, so why would I be writing a post about it? There are three reasons:\nOne of my goals with these posts is to explain some basic statistical concepts.\rThe “standard” approach from many - possibly most - introductory books is bad.\rEspecially when it is a “rare event” of interest, the standard method breaks down.\rTo motivate myself to do some of the math and write a simulation comparing the alternatives so that I better understand them.\r\rFor items 2 and 3, there are some modifications and alternatives which are not really that difficult or complicated, so I think they’re suitable for a general audience (though I can see why they’re less talked about in intro stats).\nTo introduce some notation, let’s consider a sample of size \\(n\\) from an infinite population which has a proportion \\(p\\) of the outcome of interest, and in this sample we count \\(x\\) occurrences of the event of interest. For a couple of examples:\n\rThere is a stable manufacturing process creating some component, but a certain proportion of these components will exhibit a fatal flaw. We randomly sample 100 units and count the number with the flaw. This could be either presence/absence of some defect, or pass/fail inspection on a numeric measurement.\rThe CDC randomly selects a sample of 1000 individuals and counts how many of them test positive for a particular illness (influenza, COVID-19, etc).\r\rThis post will be organized as:\n\rTalk about the common approach usually taught in intro stats.\rExplain why that approach is not so great, for two reasons.\rIntroduce two alternative approaches and explain how they arise.\rCompare the performance of the intervals.\r\r\rThe common approach\rWhen an introductory statistics textbook talks about estimating a binomial proportion, they will typically say that we estimate \\(p\\) using \\(\\hat{p} = x / n\\). This estimate has a sample standard error \\(se(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}\\). From this, we can construct a Wald statistic, which is asymptotically Normal, then we can use the asymptotic normality of Wald statistics to say that:\n\\[\r\\dfrac{\\hat{p}-p}{\\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}} \\sim N( 0, 1)\r\\]\nIf we use this to set up a 2-tailed test, we can then unpack it (or “invert” the test) to obtain a confidence interval:\n\\[\r\\hat{p} \\pm Z_{\\alpha/2} \\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}\r\\]\nThis is the confidence interval that is often (though not universally) taught in introductory statistics as the “default” or “standard.”\nAs a side-note, it may be easier for beginners to “see” the asymptotic normality arising as a consequence of the normal approximation to the binomial distribution. If \\(X \\sim \\mbox{Bin}(n,p)\\), then \\(X \\stackrel{\\text{aprx}}{\\sim} N( np, np(1-p) )\\) under certain conditions that are not consistent from source to source. There are different ways to go about establishing this, one of which is via the Central Limit Theorem.\nCommon Approach: The math\rIf you haven’t seen the math of deriving \\(\\hat{p} = x / n\\) and \\(se(\\hat{p}) = \\sqrt{\\hat{p}(1-\\hat{p})/n}\\) and would like to see, read on. If you know it already, or don’t have much calculus or probability theory background (maybe a calculus-based introductory statistics course), you may want to skip this subsection.\nFirst we’ll derive \\(\\hat{p}\\) using maximum likelihood estimation (MLE, which is usually pronounced M-L-E, but one prof of mine pronounced “me-lee” which was always weird to me). To do this, we take the likelihood of the data (in this case, a Binomial distribution), and find the value of \\(p\\) which maximizes it. Mathematically, this is just a calculus problem, and for the Binomial distribution is fairly straightforward.\nThe likelihood is the joint probability, but since we observe the data we think of the data as constant, and treat it as a function of the parameters of interest. In this case, the individual observations are Bernoulli (success/failure), and the joint probability is a Binomial distribution. Hence the likelihood is: \\(\\mathcal{L}(p|x) = \\binom{n}{x}p^{x}(1-p)^{n-x}\\). The typical MLE approach is to take the derivative of the log-likelihood, since that won’t move the maximum, and generally makes solving easier.\n\\[\r\\begin{aligned}\rL(p|X) \u0026amp;= \\ln\\mathcal{L}(p|X) = \\ln \\binom{n}{X} + X\\ln(p) + (n-X)\\ln(1-p) \\\\\r\\dfrac{\\partial}{\\partial p}L(p|X) \u0026amp;= \\dfrac{X}{p} - \\dfrac{n-X}{1-p}\r\\end{aligned}\r\\]\nFrom there, we set the derivative to zero and solve for the parameter.\n\\[\r\\begin{aligned}\r0 \u0026amp;= \\dfrac{x}{\\hat{p}} - \\dfrac{n-x}{1-\\hat{p}} \\\\\r\\dfrac{n-x}{1-\\hat{p}} \u0026amp;= \\dfrac{n}{\\hat{p}} \\\\\rn\\hat{p} - x\\hat{p} \u0026amp;= x - x\\hat{p} \\\\\rn\\hat{p} \u0026amp;= x \\\\\r\\hat{p} \u0026amp;= \\dfrac{x}{n}\r\\end{aligned}\r\\]\nWe also want the standard error of our MLE, which is just the square root of the variance.\n\\[\r\\begin{aligned}\rV\\left[ \\hat{p} \\right] \u0026amp;= V\\left[ \\dfrac{X}{n} \\right] \\\\\r\u0026amp;= \\dfrac{1}{n^2} V\\left[ X \\right]\r\\end{aligned}\r\\]\nThen since \\(x\\) comes from a Binomial distribution, we know that \\(V\\left[ X \\right] = np(1-p)\\), substituting that in results in:\n\\[\r\\begin{aligned}\rV\\left[ \\hat{p} \\right] \u0026amp;= \\dfrac{np(1-p)}{n^2} \\\\\r\u0026amp;= \\dfrac{p(1-p)}{n}\r\\end{aligned}\r\\]\nFinally, a Wald statistic is a statistic of the form:\n\\[\r\\begin{aligned}\rz = \\dfrac{\\hat{\\theta} - \\theta}{se(\\hat{\\theta})}\r\\end{aligned}\r\\]\nwhere \\(\\hat{\\theta}\\) is the MLE. Since we have the MLE and its standard error, the Wald statistic is:\n\\[\r\\begin{aligned}\rz = \\dfrac{\\hat{p} - p}{ \\sqrt{\\dfrac{p(1-p)}{n}} }\r\\end{aligned}\r\\]\nThen, because Wald statistics are asymptotically Normal, we use this statistic to obtain the confidence interval we saw before.\n\rWhy the common approach is bad\rSo why should we be looking beyond the basic approach? Because it’s bad. One way to understand that it’s bad is to look at the coverage probability of the confidence interval. If we consider the coverage probability across a range of \\(p\\) values, we’ll see that it often drops below the nominal value, especially so at the tails, where \\(p\\) is close to \\(0\\) or \\(1\\). The figure below shows the coverage probability for each \\(p\\) (in the interval \\([0.02,0.98]\\)) at a selection of sample sizes.\nThis is a similar pattern as shown in several figures from Brown, Cai, \u0026amp; DasGupta (2001) (which I’ll abbreviate BCD), for example, the bottom-left panel here is the same as their Figure 3, just at a different resolution. We can alternatively switch, to look at a sequence of \\(n\\) for a selection of \\(p\\).\nHere, the bottom-right figure is the same as their Figure 1. In both figures we see a very disturbing pattern: The coverage probability is usually below the nominal value, sometimes substantially so. Even at \\(n=100\\) or \\(n=200\\), the coverage probability rarely gets up to the nominal value for any \\(p\\). This is discussed in more detail in Brown, Cai, \u0026amp; DasGupta (2001), as well as several responses to their paper (which are included in the publication I linked above).\nAn in addition to that, they are very jittery. Ideally we’d like to think that the coverage probability is a smooth function, but that’s not the case here. It’s a result of the underlying discreteness of of \\(X\\), and there are “lucky” and “unlucky” combinations of \\(n\\) and \\(p\\).\nAside: These figures were not generated by simulation, they are exact coverage probabilities. It’s fairly straightforward to come up with the formula. For illustration, I’ll just pick a certain value of \\(p\\), say \\(p=0.20\\).\rWe need to know the probability that, when \\(p=0.20\\), the confidence interval we compute will actually contain the value \\(0.20\\). But \\(p\\) isn’t what the confidence interval formula uses, right? Even if we know that \\(p=0.20\\), there are many possible values for \\(\\hat{p}\\), which mean there are many possible values of the endpoints for the confidence interval. So to calculate the coverage probability, what we do (or what I did), was:\nSet up a vector \\(X\\) which contains the values \\(0, 1, ..., n\\). This is the sequence of “successes”.\rFor each, compute \\(\\hat{p} = X/n\\).\rFrom \\(\\hat{p}\\), compute the endpoints of the confidence interval.\rFor each confidence interval, determine whether the true value of \\(p\\) has been captured.\rIdentify the bounds of \\(X\\) which result in “successful” confidence intervals. That is, determine \\(X_{max}\\) and \\(X_{min}\\), the largest and smallest values of \\(X\\) which produce a confidence interval which captures \\(p\\). For example, when \\(p=0.20\\), if \\(X=60\\) and \\(n=100\\), then the CI is \\((0.504, 0.696)\\), which does not capture \\(p\\).\rCompute the probability that \\(X\\) is contained in the interval \\([X_{min}, X_{max}]\\). Since we know \\(p\\), this is just a binomial probability: \\(P( X \\leq X_{max} ) - P( X \\leq X_{min}-1 )\\). Note we subtract 1, because \\(X_{min}\\) is a valid value, so \\(X\\) needs to be strictly below \\(X_{min}\\) for the interval to fail.\rRepeat steps 1-6 for a sequence of \\(p\\) from some “small” value to some “large” value (as indicated, I chose \\(0.02\\) to \\(0.98\\)).\r\rDeveloping this idea and creating the code to produce a plot like the above would probably be a good exercise for a low-level statistics course that includes a probability component.\n\r\rAlternatives\rNow that we’re all convinced that the standard approach has some deficiencies, what are some alternatives? I’m going to talk about three, though two are very similar. For each I’ll try to provide a more intuitive overview, and then as before, dig into a bit of the math.\nAlternative 1: Wilson and Agresi-Coull intervals\rThere are two (very similar) intervals I’ll mention. BCD call them the Wilson interval, and the Agresti-Coull interval. The idea is to use a slightly modified estimator in place of \\(\\hat{p}\\):\n\\[\r\\tilde{p} = \\dfrac{X + \\dfrac{\\kappa^{2}}{2}}{n + \\kappa^{2} }\r\\]\nIn this, in part to reduce notation, I’ve used \\(\\kappa = Z_{\\alpha/2}\\), which is what BCD did.\nThis may look weird, but it’s actually not that strange. Notice that the general form \\(X/n\\) is still present, but there’s a little bit added to each. And notably, it’s adding precisely half as much to the numerator as to the denominator. The effect this has is like adding a few extra samples, of which half are successes and half are failures. For example, when we select 95% confidence, then \\(\\kappa = 1.96\\) and \\(\\kappa^{2} = 3.84\\), so we’re adding approximately 2 to the numerator, and approximately 4 to the denominator (which was the approach suggested by\rAgresti \u0026amp; Coull (1998),\ralternative link on JSTOR). This has an effect of pulling the estimate slightly closer to 0.5, and also prevents it from being exactly zero, which uases problems with the standard error.\nBoth the Wilson and Agresi-Coull approach use this as the point estimate, but the standard errors they use are slightly different.\nThe Wilson confidence interval uses:\n\\[\rse(\\tilde{p}) = \\left(\\dfrac{\\sqrt{n}}{n + \\kappa^{2}}\\right)\\sqrt{\\hat{p}(1-\\hat{p}) + \\dfrac{\\kappa^{2}}{4n}}\r\\]\nThe Agresi-Coull confidence interval uses:\n\\[\rse(\\tilde{p}) = \\sqrt{\\dfrac{\\tilde{p}(1-\\tilde{p})}{\\tilde{n}}}\r\\]\nwhere \\(\\tilde{n} = n+\\kappa^{2}\\). With \\(\\tilde{n}\\), we could also define \\(\\tilde{X}=X+\\kappa\\) and write the point estimate as \\(\\tilde{p} = \\tilde{X}/\\tilde{n}\\). So the Agresi-Coull approach is really just the standard methods after having applied this adjustment of adding an equal number of successes and failures.\n\rWilson interval: The math\rAlright, so how do we get these estimators and standard errors? BCD aren’t really focused on the derivation, so we need to go back to the source:\rWilson (1927)\r(alternative: JSTOR link).\nReading that, Wilson does something interesting. First, he considers the sampling distribution of the sample proportion.\nIn this, \\(\\sigma = \\sqrt{p(1-p)/n}\\) and \\(\\kappa\\) is some constant. He then points out that the probability of some value of \\(p\\) falling outside of the interval is \\(p \\pm \\kappa\\sigma\\) is \\(P( Z \\geq \\kappa)\\). The question is then how to extract \\(p\\) from this. What Wilson does is square the distance between the true value \\(p\\) and some sample proportion \\(\\hat{p}\\). And since we know that this distance is (with a probability determined by \\(\\kappa\\)), we can equate the two.\n\\[\r\\left(\\hat{p} - p\\right)^{2} = \\left(\\kappa\\sigma\\right)^2 = \\dfrac{\\kappa^{2}p(1-p)}{n}\r\\]\nSmall note: Wilson used \\(\\lambda\\), I’m using \\(\\kappa\\) because that’s what BCD used. Having set this up Wilson (probably to save notation) sets \\(t = \\kappa^{2}/n\\), and points out that this is a quadratic expression in \\(p\\), so we can use the quadratic formula to solve it. We first need to expand the square, get everything on one side, and collect like terms.\n\\[\r\\begin{aligned}\rp^{2} - 2p\\hat{p} + \\hat{p}^{2} \u0026amp;= tp - tp^{2} \\\\ 0 \u0026amp;= p^{2}\\left(1+t\\right) - p\\left(2\\hat{p}+t\\right) + \\hat{p}^{2}\r\\end{aligned}\r\\]\nApplying the quadratic formula with\r\\(a=(1+t)\\), \\(b=-(2\\hat{p}+t)\\), and \\(c=\\hat{p}^{2}\\)\rwe get the solution to be:\n\\[\r\\begin{align}\rp \u0026amp;= \\dfrac{2\\hat{p} + t \\pm \\sqrt{ (2\\hat{p}+t)^{2} - 4(1+t)\\hat{p}^{2} }}{2(1+t)} \\\\\r\u0026amp; \\\\\r\u0026amp; \\mbox{(some algebra)} \\\\\r\u0026amp; \\\\\r\u0026amp;= \\dfrac{\\hat{p} + \\dfrac{t}{2} \\pm \\sqrt{ t\\hat{p}(1-\\hat{p}) + \\dfrac{t^{2}}{4} }}{1+t}\r\\end{align}\r\\]\nI’ve skipped a little algebra, but it’s only a couple lines at most. Once we’re here, we need to recall that \\(t = \\kappa^{2}/n\\), and substitute that back in. I’m going to separate the fraction at the \\(\\pm\\), and pull a \\(1/n\\) out from the denominator - thus multiplying the numerator by \\(n\\) - to cancel some of these hideous fractions-within-fractions.\n\\[\r\\begin{align}\rp \u0026amp;= \\dfrac{\\hat{p} + \\dfrac{\\kappa^{2}}{2n} \\pm \\sqrt{ \\dfrac{\\kappa^{2}}{n}\\hat{p}(1-\\hat{p}) + \\kappa^{2}\\dfrac{\\kappa^{2}}{4n^{2}} }}{1+\\kappa^{2}/n} \\\\\r\u0026amp; \\\\\r\u0026amp;= \\dfrac{x + \\dfrac{\\kappa^{2}}{2}}{n+\\kappa^{2}} \\pm \\dfrac{\\sqrt{ \\kappa^{2}n \\hat{p}(1-\\hat{p}) + \\kappa^{2}n\\dfrac{\\kappa^{2}}{4n} }}{n+\\kappa^{2}} \\\\\r\u0026amp; \\\\\r\u0026amp;= \\dfrac{x + \\dfrac{\\kappa^{2}}{2}}{n+\\kappa^{2}} \\pm \\dfrac{ \\kappa\\sqrt{n} \\sqrt{ \\hat{p}(1-\\hat{p}) + \\dfrac{\\kappa^{2}}{4n} }}{n+\\kappa^{2}} \\\\\r\u0026amp; \\\\\r\u0026amp;= \\dfrac{x + \\dfrac{\\kappa^{2}}{2}}{n+\\kappa^{2}} \\pm \\kappa \\dfrac{ \\sqrt{n}}{n+\\kappa^{2}} \\sqrt{ \\hat{p}(1-\\hat{p}) + \\dfrac{\\kappa^{2}}{4n} } \\end{align}\r\\]\nNote that on line 2, I also multiplied the last term under the radical by \\(n/n = 1\\). This was to make it “match” the first term in having \\(\\kappa^{2}n\\) that could be extracted from the radical. with this, we’ve arrived at the form of the interval I presented originally.\nRemember that \\(\\kappa\\) was defined as a quantile from the normal distribution. By expressing the solution as I have, this results has the form of a traditional normal-based confidence interval:\n\\[\r\\mbox{Estimate} \\pm \\mbox{Critical Value}\\times\\mbox{Std Error}\r\\]\nAgresi and Coull wanted to simplify this a bit, so they used the same estimator, defining:\n\\[\r\\begin{align}\r\\tilde{x} \u0026amp;= x + \\kappa^{2}/2 \\\\\r\\tilde{n} \u0026amp;= n + \\kappa^{2} \\\\\r\\tilde{p} \u0026amp;= \\tilde{x} / \\tilde{n}\r\\end{align}\r\\]\nwhich is adding some number of trials, evenly split between successes and failures. They then use this \\(\\tilde{p}\\) in the “standard” form of the confidence interval. In this way they create a much better-behaving confidence interval, but which is a bit more straightforward and easier to remember than the Wilson interval.\n\rAlternative 2: Bayesian method\rLately I’ve been going to the Bayesian approach to this problem. This might get slightly more technical if you’re at a lower mathematical level. For Bayesian analysis, we define a likelihood for the data, and a prior for the parameters. In this case, the data are Binomial, and the only parameter is the proportion, for which a Beta distribution works well. So we will assume:\n\\[\r\\begin{aligned}\rX | p \u0026amp;\\sim \\mbox{Binomial(n,p)} \\\\\rp \u0026amp;\\sim \\mbox{Beta}(\\alpha, \\beta)\r\\end{aligned}\r\\]\nSetting \\(\\alpha=\\beta=1\\) results in a uniform or “flat” prior, meaning we don’t have any initial judgment on whether \\(p\\) is likely to be large or small, all possible values of \\(p\\) are equally likely. Then, if we call the likelihood \\(L(x|p)\\) and the prior \\(\\pi(p)\\), the posterior for \\(p\\) is found by:\n\\[\r\\pi(p|x) \\propto L(x|p)\\pi(p)\r\\]\nSo, inserting the Binomial PMF for \\(L(x|p)\\) and the Beta PDF for \\(\\pi(p)\\), we get:\n\\[\r\\begin{aligned}\r\\pi(p|x) \u0026amp;\\propto \\binom{n}{x}p^{x}(1-p)^{n-x} \\times \\dfrac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha,\\beta)} \\\\\r\u0026amp;\\propto p^{x - \\alpha-1}(1-p)^{n - x + \\beta-1}\r\\end{aligned}\r\\]\nFormally, we should be performing some integration, but since we’re really interested in \\(p\\) here, I just want to see what the form of the resulting posterior will look like, and in this case it’s another Beta distribution, specifically, a \\(\\mbox{Beta}(x+\\alpha, n-x+\\beta)\\) distribution.\nWith this posterior distribution, we can estimate \\(p\\) in several ways. We can take the mean or median as a point estimate, and we can obtain a credible interval (the Bayesian answer to the confidence interval). For example, say we had a sample of size \\(n=50\\), and observed \\(x=1\\) occurrences of the event of interest. Then taking \\(\\alpha = \\beta = 1\\) (which is a uniform prior on \\(p\\)), our posterior would look like below. Note that the x-axis is fairly truncated!\nThe estimate using the Wilson approach with 95% confidence, for reference, would be:\n\\[\r\\tilde{p} = \\dfrac{1 + 1.96^2/2}{50 + 1.96^2} = \\dfrac{2.921}{53.84} = 0.0542\r\\]\n\r\rComparisons\rSo that’s an overview of three methods for estimating \\(p\\) (both a point estimate and a confidence interval), but how to they compare to each other? We computed the coverage probability before, so let’s follow the same framework and compute the coverage probability for all four intervals. As before, this won’t be a simulation, but exact coverage probabilities.\nWe see here again that the standard interval performs poorly, but now we can see how the alternatives stack up against it. The Agresti-Coull interval tends to have the highest coverage probability, while the Wilson and Bayes intervals are very close (being on top of each other much of the time!).\nIn another way of thinking about the intervals, we can consider the expected interval length. Recall when I computed the interval, I’d take a given value of \\(p\\) and \\(n\\), and compute the interval for all possible values of \\(x\\). So when I calculated the interval, I also obtained the length of the interval and computed a weighted average (with weight \\(P(X=x)\\)) of the lengths. We see that result below.\nCertainly for smaller sample sizes, the Wilson and Bayes intervals produce shorter intervals (that is: more precision on \\(p\\)). One may be tempted to think that for small \\(n\\) and small \\(p\\) the standard interval is good here, but don’t forget the coverage probability: It drops precipitously at that point! Once we get much beyond \\(n=100\\) there isn’t much difference between the Wilson, Agresti-Coull, and Bayes intervals.\n\rSummary\rIn this post we explored estimating a Binomial proportion, seeing how the standard method is derived, and why it’s bad, and explored some alternatives (including their derivation). Based on the coverage probabilities and interval lengths, my suggestion would be to use either the Wilson or Bayes intervals - they both have good coverage, and tend to be shorter than the Agresti-Coull interval. The Agresti-Coull has slightly higher coverage probability, but that comes at the expense of a longer interval. That being said, it depends on the behavior you want to see. The Wilson and Bayes intervals seem to have 95% coverage probability on average, while the Agresti-Coull interval seems to maintain at least 95% coverage (or very close to it) throughout.\n\r","date":1602460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602521484,"objectID":"bd402fe2864830ce7cb78f3e44c756eb","permalink":"/post-stats/estimating-binomial-proportions/","publishdate":"2020-10-12T00:00:00Z","relpermalink":"/post-stats/estimating-binomial-proportions/","section":"post-stats","summary":"Introduction\rThe common approach\rCommon Approach: The math\rWhy the common approach is bad\r\rAlternatives\rAlternative 1: Wilson and Agresi-Coull intervals\rWilson interval: The math\rAlternative 2: Bayesian method\r\rComparisons\rSummary\r\r\rIntroduction\rEstimating a proportion gets covered in virtually every introductory statistics course, so why would I be writing a post about it? There are three reasons:\nOne of my goals with these posts is to explain some basic statistical concepts.","tags":["categorical","proportion","binomial","rare events"],"title":"Estimating binomial proportions for rare events","type":"post-stats"},{"authors":[],"categories":["stat101","methods","lecture"],"content":"\rIntroduction\rShould you use a two-tailed test or a one-tailed test (or similarly, a confidence interval or 1-sided confidence bound)? For those just learning statistics, or who have had only a little training in the subject, this question comes up fairly often. And there is some conflicting information and advice out there. Most often I’ve seen comments critical of one-sided methods, such as:\n\rThe short answer is: Never use one tailed tests.\r- The Analysis Factor\n\ror\n\rTwo-Tailed Tests are the Default Choice\r- Statistics by Jim\n\rThough I did come across at least one person I’ve seen suggests the opposite!\n\rI actually intend to go all the way and argue that barring some very narrow use-cases, one should never use a two-tailed statistical test.\r- Analytics Toolkit blog\n\rI disagree with all of these statements to varying degrees, though mostly the first and the third. In this post, I’ll try to clarify how to think about this so that you can know whether to use a 1-sided or 2-sided test (spoiler: It has nothing little to do with Statistics). As an aside, “1-tailed” and “2-sided” are interchangable terms, and for the 1-tailed methods, there can be upper-tail or lower-tail.\n\rCrash course on hypothesis testing\rUnderstanding 1- or 2-tailed methods is probably easiest to describe in terms of hypothesis testing, but the same concepts apply to interval estimation (instead of, e.g., a confidence interval, you can construct a 1-sided confidence bound). So to start off, I’ll give a very short summary of basic hypothesis testing. I’m going to focus on a 1-sample t-test, but the same ideas will apply to any situation where you can have two tails. If you already understand t-tests, you probably want to skip to the next section.\nStep 1: Define your null and alternative hypotheses. Here are two such sets, for an upper-tail and two-tailed test, respectively, on the mean.\n\rUpper tailed: \\(H_{o}: \\mu \\leq \\mu_{0}\\) vs \\(H_{a}: \\mu \u0026gt; \\mu_{0}\\)\rTwo tailed: \\(H_{o}: \\mu = \\mu_{0}\\) vs \\(H_{a}: \\mu \\ne \\mu_{0}\\)\r\rIn these, \\(\\mu_{0}\\) is some hypothesized value, some value we are interested in comparing against.\nStep 2: Select a significance level, denoted by \\(\\alpha\\). This is the probability of committing a Type I error. This is the event in which we reject the null hypothesis when we should not have, because the null hypothesis is actually true. Any probability can be selected here, some common values are 0.1, 0.05, and 0.01. In practice we will not know whether or not a commit a Type I error.\nStep 3: From the data, compute the test statistic which (under certain assumptions) will follow a particular sampling distribution. One of the most basic examples is the t-test, where the test statistic is \\(t = (\\bar{x} - \\mu_{0}) / (s / \\sqrt{n} )\\) will follow a t-distribution (assuming the conditions are satisfied).\nStep 4: We then put this test statistic on the t-distribution and see if it falls into the rejection region (RR). This RR is determined from the significance level, depending on how the alternative hypothesis is specified. For an upper-tail test, all of the \\(\\alpha\\) probability is piled into the upper tail. For a two-tailed test, the α probability is split between the two tails. I made a graphic to illustrate this.\nFor each figure, the shaded region represents 5% of the total area. But we have arranged that 5% differently, because the tests are answering different questions.\nStep 5: Reject or fail to reject the null hypothesis. From that decision, you’d then take appropriate action - or not. I’ll provide a couple examples further on.\nIf you’re more familiar with p-values or critical values, do not fear. There’s a 1-1 correspondence between these. The RR is bounded by the critical value, which is the value at which the tail probability is equal to \\(\\alpha\\). Hence:\n\rIf the test statistic is in the rejection region (\\(t \\in RR\\)), then \\(p \\leq \\alpha\\)\n\r\rHow to choose 1 or 2 tails\rOkay, so that was a very quick overview of hypothesis testing. Then we come to the question of whether we should use a 1-tailed or 2-tailed method. To understand this, we need to understand what we’re actually doing when testing a hypothesis. Usually, we arrange the hypotheses so that the alternative hypothesis represents an actionable event, and the null hypothesis is the “status quo.” This means that when we reject the null hypothesis, we’re going to do something. I read and comment on the subreddits r/statistics and r/AskStatistics (among others). There, I think a user with the handle The_Sodomister phrased it well: Rejecting the null hypothesis means we “take action.” On the other hand, failing to reject the null hypothesis means we do not take action.\nSo we need to consider the context of the data, and whether we care about the direction. Let’s consider a couple of examples.\n\rAirlines regularly overbook flights (sell more tickets than they have seats). But for various reasons, not everyone shows up for their flight, so it often balances out. What they really care about is how many people show up, since too many people showing up means they have to pay out to reimburse anyone who can’t board that flight. So if they find that too many people will show up, they’ll take action by overbooking to a smaller degree. However, if too few people show up, well, those people paid for their ticket anyway, so the airline isn’t really losing out (that being said, they could be interested in the lower bound, so that they can overbook even further!).\n\rA pharmaceutical company is developing a new drug as an alternative to an existing drug. They’re doing the science to try to improve upon the existing drug, so they’d like to see improved outcomes. But it is extremely important to know if the new treatment actually leads to worse outcomes. So even though the company wants to show an effect in a particular direction, they also need to be able to detect if the effect is in the opposite direction. So the company will take action in either case (push the drug to market, or end R\u0026amp;D on the drug). So a two-tailed test would be of interest.\n\rAn engineer is designing a critical component for a rocket to send astronauts into space. That component has some failure rate, and a failure would mean the loss of millions or billions of dollars, and possibly lead to the death of the astronauts.\rSince this component was carefully designed and manufactured, the plan is to use it unless the error rate is too high. So the engineer is really only interested in the upper tail, how larger might the failure rate be, rather than how low it might be; they essentially treat the upper bound as “It’s possible for the error rate to be as large as X%, so we’re going to assume it is X%.” The question then fundamentally becomes “Is X% too high?”\n\rAn individual is considering whether to buy more of a particular stock, so they look at the average daily returns. They are interested if the daily return is positive, since they would then like to buy more. However, if the daily return is negative, that probably would lead to an action as well (say, selling that stock). Hence, detecting an effect in both directions is important.\n\r\rTo conduct a 1-tailed test, the opposite tail needs to be of utterly no concern. In the stock example, we would have to not care at all if the stock’s price was negative. There’s a good chance that’s not the case. That said, I’m no economist. Maybe for a prospective investor who doesn’t own any stock yet, their default position would be “Don’t invest,” so they really don’t care if the returns are negative, they’re only going to take action if the returns are positive.\nWhat’s important is specifying up-front what question we are interested in answering, and what results would cause us to take action. These should be clarified before data is collected. The statistical hypotheses and direction of the method are consequences of the answers. So as I hinted at before: This isn’t really really part of the statistical method, it’s part of the research hypothesis. This means it’s a subject-matter issue more than it is a statistical issue.\nIf we only take action for a particular direction, then we should only be testing in that direction. If we would take action for both directions, then we should test in both directions. The statistical part of this is, in my opinion, simply translating the question of interest from English (or whatever your language of choice might be) into statistical terms, such as be proposing a statistical model and rephrasing the question in terms of statistical parameters.\nSo I guess my take-away or tl;dr for this would be:\n\rIf you only care about (or would only take action) for results in a particular direction, then only test in that direction.\n\r\rTrailing thoughts\rThere are a few comments I want to make that I didn’t find a good place to work into the above discussion.\n\rOne of the quotes that I started off with said that two-tailed tests should be the default. The reason I disagree with this is that I don’t think there should be a “default” and an “exception.” I think we should carefully assess the context, but not be predisposed to one or the other.\n\rThis whole discussion only applies to a subset of statistical methods. Some methods, such as Analysis of Variance (ANOVA) are inherently 2-tailed. However, there are some directional variants, broadly described as tests for ordered alternatives, which put a series of inequalities (\\(\\leq\\) or \\(\\geq\\)) into the alternative hypothesis.\n\rAn old and reasonably well-known one is the Jonckheere–Terpstra test, which modifies a nonparametric ANOVA for the case of ordered alternatives.\rFor some shameless self-promotion, some of my research has been on the subject of ordered alternatives. See for example the papers on which I am co- or lead-author,\rDavidov, Jelsema, \u0026amp; Peddada (2018) and\rJelsema \u0026amp; Peddada (2016). In addition, a doctoral student I worked with at WVU used these methods in two applied papers,\rLaw, Morris, \u0026amp; Jelsema (2017) and\rLaw, Morris, \u0026amp; Jelsema (2018)\r\r\r\r","date":1602288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602351732,"objectID":"884c6fe4861a1ba01cf9274513de261b","permalink":"/post-stats/one-or-two-tailed-tests/","publishdate":"2020-10-10T00:00:00Z","relpermalink":"/post-stats/one-or-two-tailed-tests/","section":"post-stats","summary":"Introduction\rShould you use a two-tailed test or a one-tailed test (or similarly, a confidence interval or 1-sided confidence bound)? For those just learning statistics, or who have had only a little training in the subject, this question comes up fairly often. And there is some conflicting information and advice out there. Most often I’ve seen comments critical of one-sided methods, such as:\n\rThe short answer is: Never use one tailed tests.","tags":["hypothesis testing","tails"],"title":"One or two tails?","type":"post-stats"},{"authors":[],"categories":["rmarkdown"],"content":"Preface For this post, I\u0026rsquo;m going to assume you are fairly familiar with RMarkdown. If not, you may want to start with RMarkdown: The Definitive Guide, or R for Data Science. The short version is that RMarkdown is a flavor of the markup language Markdown, which uses plain-text formatting and can be rendered into a variety of output types, including PDF, MS Word, HTML, and others. With RMarkdown, you can incorporate R \u0026ldquo;code chunks\u0026rdquo; to create a fully reproducible document.\nWith RMarkdown, you can also create a PowerPoint presentation, meaning that you can create a reproducible slide deck which includes code and results. You might ask why use PowerPoint when there are a variety of presentation formats that RMarkdown supports. I personally like the xaringan  format. However, sometimes your hands are tied.\n Perhaps you\u0026rsquo;re doing an analysis for someone who insists on PP (maybe they\u0026rsquo;ll just take a couple of your slides to incorporate into a larger presentation). Maybe you have a corporate PP template that you have to use. Maybe the technology that the presentation will be projected on doesn\u0026rsquo;t support HTML slides. Maybe it\u0026rsquo;s something else.  So for whatever the reason, you need to make do with PowerPoint.\nAnother complication: Suppose you need to do the same basic analysis for a number of variables. For example, say you need to generate a set of boxplots, and a table of summary statistics for each variable in Fisher\u0026rsquo;s Iris data, comparing across the three Species. You\u0026rsquo;re already using RMarkdown, because you don\u0026rsquo;t want to be running results in R and then pasting figures and tables over to the slide deck manually. But you also don\u0026rsquo;t really want to be copy-pasting the same code all over, right? (Hint: No, you don\u0026rsquo;t). So we\u0026rsquo;d like to automate this process. You should be familiar with writing loops in R, and with that and a child document we can do exactly this.\nWe\u0026rsquo;ll need two files for this task, I\u0026rsquo;m going to name them main_doc.Rmd and child_doc.Rmd. The next sections will walk through the code that will go into each.\nMain Document Only the main document will need a YAML header, child documents can do without (come to think of it, I\u0026rsquo;m not sure if they are allowed to have YAML headers). A basic one might look like this:\n---\rtitle: \u0026quot;Looping for PP Slides\u0026quot;\rsubtitle: \u0026quot;Presentation subtitle\u0026quot;\rauthor: \u0026quot;Casey Jelsema\u0026quot;\rdate: \u0026quot;Generated | `r format(Sys.Date(), '%B %d, %Y')`\u0026quot;\routput:\rpowerpoint_presentation: default\r---\r Though note that I included one minorly fancy thing here, by setting the date to be a bit of R code so that it automatically updates when the file is knit. You may or may not want that. You can also put it elsewhere, for example in document type reports, I often set the subtitle to be the date.\nFor PP presentations, the section header (#) is what tells the document to start a new slide. You can change this with a setting in the YAML header. For instance, maybe you will have several sections, so you want two pound signs to denote the start of a new slide, you\u0026rsquo;d add slide_level: 2 to the output section, nested underneath powerpoint_presentation, see slide level). There are also a variety of other options available, including setting a reference document for a custom template, or having a two-column slide. Both of those are described here.\nThe rest of the main document is below. I\u0026rsquo;ll just past it all and then talk about it.\n```{r, setup, echo = FALSE, message=FALSE, include=FALSE }\rknitr::opts_chunk$set( echo=FALSE, message=FALSE)\rlibrary(\u0026quot;tidyverse\u0026quot;)\rlibrary(\u0026quot;flextable\u0026quot;)\r```\r# Introduction\rIn this presentation we use Fisher's iris data as an example.\r```{r, load-data, results=\u0026quot;hide\u0026quot; }\rdata(iris)\riris \u0026lt;- iris %\u0026gt;% mutate( Species = str_to_sentence(Species) )\r```\r- There is a code chunk here where I'm loading the data and doing some formatting to clean it up.\r- For the demonstration I'm just going to loop through to make some box plots and a table for each of the variables.\r```{r, loop-over-params, results=\u0026quot;hide\u0026quot; }\rparam_vec \u0026lt;- colnames(iris)[1:4]\rnParam \u0026lt;- length(param_vec)\rout \u0026lt;- rep(NA,nParam)\rfor( ii in 1:nParam ){\rparam_ii \u0026lt;- param_vec[ii]\rparam_ii_nice \u0026lt;- str_replace( param_vec[ii], \u0026quot;\\\\.\u0026quot;, \u0026quot; \u0026quot;)\rout[ii] \u0026lt;- knitr::knit_child(\u0026quot;child_doc.Rmd\u0026quot;)\r}\r```\r```{r, print-slides, results=\u0026quot;asis\u0026quot;}\rcat( paste(out, collapse=\u0026quot;\\n\u0026quot;) )\r```\r The setup chunk, is, well, your setup chunk. I usually use that to load which packages I\u0026rsquo;ll be using, setting options, sometimes specifying little helper functions I use, setting up a theme for ggplots or creating linetypes for flextable. The setup chunk here is a pretty basic one.\nThen the section header # Introduction gives us a first slide (well, beyond the title slide) with some comments. Note that the code chunk if there, between the first line and the two bullet points, but we don\u0026rsquo;t see it. If you\u0026rsquo;re familiar with RMarkdown, this shouldn\u0026rsquo;t be surprising.\nThe next chunk, loop-over-params is where the action is. So what am I doing?\n Get the variables to loop over, and get the number of them. Here I was easily able to extract them from the data. You might need to do a bit more work for that. The out object is setting up a container for me to put the slides for each variable, so that I\u0026rsquo;m not \u0026ldquo;growing\u0026rdquo; the object throughout my loop. Then in the for-loop, I\u0026rsquo;m grabbing the variable name, creating a nicer version of it for printing. Finally, I run the child document with the command knitr::knit_child(\u0026quot;child_doc.Rmd\u0026quot;).  This works because I wrote the child document with a generic param_ii, so as that object gets modified, the child document uses different variables. The results get put into the iith space of the out container.\nFinally, the last chunk here will print the results of the looping so that they are incorporated into the document. You need to use the results=\u0026quot;asis\u0026quot; option, and collapse all of the results together with a newline.\nChild Document A basic version of the child document is here. One thing I want to draw your attention to is that none of these chunks are labeled. That\u0026rsquo;s because if they get run multiple times, RMarkdown will (appropriately!) complain that there are chunks with the same name. I haven\u0026rsquo;t tried to dynamically name the chunks yet. An quick search showed that I\u0026rsquo;m not alone in this, and there were some solutions proposed. I haven\u0026rsquo;t tried them, and have not had a need to really try.\n```{r, results=\u0026quot;asis\u0026quot;}\r# figure-slide-title\rcat(\u0026quot;# Response: \u0026quot;, param_ii_nice, \u0026quot; | Box plots\u0026quot; )\r```\r```{r, fig.height=3.5 }\r# figure-slide\rggplot( iris , aes_string(x=\u0026quot;Species\u0026quot;, y=param_ii) ) +\rgeom_boxplot( aes(fill=Species) ) +\rlabs( y=param_ii_nice )\r```\r The first chunk is setting the title of the slide, using the \u0026ldquo;nice\u0026rdquo; version of the parameter name. Again, I\u0026rsquo;m printing with the chunk option results=\u0026quot;asis\u0026quot;, because we want to paste exactly what we write into the Rmd. After that, I\u0026rsquo;m just making the figure. One bit you may not be familiar with is the aes_string command in ggplot. Note that typically variable names are unquoted in ggplot. If you have a quoted string, then aes_string will help you out here.\nThen on to the table.\n```{r, results=\u0026quot;asis\u0026quot;}\r# table-slide-title\rcat(\u0026quot;# Response: \u0026quot;, param_ii_nice, \u0026quot; | Statistics\u0026quot; )\r```\r- Comments you want to make in the child slide need to use some conditional logic.\r- Otherwise the same comments go on every child slide.\r- That means they should probably be rather basic, if this is supposed to be automated.\r```{r, ft.left=1, ft.top=6 }\r# table-slide\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit()\r```\r Again, I\u0026rsquo;m first defining the slide title with a results=\u0026quot;asis\u0026quot; chunk option, then making the content of the slide. I tend to use the flextable package for tables, though some parts of officer help to do some fine-tuning. There are other table-making packages, but these have been more than enough for my needs.\nI don\u0026rsquo;t remember exactly why I switched from using the kable function to flextable, I think that I was having a difficult time with MS Word tables. Maybe I just didn\u0026rsquo;t put in enough effort. Regardless, flextable has been easy for me to make tables in the formats I\u0026rsquo;ve needed, and the author is actively, improving it, so I haven\u0026rsquo;t had a reason to look elsewhere.\nIn the table-slide chunk, note that there are chunk options ft.left=1, ft.top=4. These define where the top-left of the table will be placed. It helps to arrange the content and the output on a slide. I\u0026rsquo;m not sure if they work with tables generated from other packages.\nSo, if you\u0026rsquo;ve copied these code chunks into main_doc.Rmd and child_doc.Rmd (well, you can call the first one whatever you like, but the code references child_doc.Rmd), then you should be able to generate a slide deck that has two slides for each variable, along with a first introductory slide.\nYou should be able to take it from there. Add more slides at the beginning as necessary, add more slides after looping over the variables, add more slides into the child document, make a fancier loop, or anything. Once you have the building blocks, it\u0026rsquo;s just a matter of arranging them appropriately. My hope is that this little demo gave you another building block.\nTwo other aspects I\u0026rsquo;d like to demonstrate are: Two-column slides; and conditional logic for dynamically generating the output comments.\nTwo-column slides This one is pretty easy. As I noted above, the RMarkdown book has examples. The basic idea is that you have a set of colons (:) to denote the start and end of column sets, and a (smaller) set of colons to denote the start and end of columns, like so:\n:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\rContent of the left column.\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\rContent of the right column.\r:::\r::::::\r So, for instance, we might replace the table-slide with the following, we get a two-column slide. Note that the column notation goes outside the chunk, or put another way, the code chunk goes inside a particular column.\n```{r, results=\u0026quot;asis\u0026quot;}\rcat(\u0026quot;# Two-column Slide: \u0026quot;, param_ii_nice, \u0026quot; | Statistics\u0026quot; )\r```\r:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\r```{r, ft.left=1, ft.top=2 }\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit()\r```\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\r- Comments you want to make in the child slide need to use some conditional logic.\r- Otherwise the same comments go on every child slide.\r- That means they should probably be rather basic, if this is supposed to be automated.\r:::\r::::::\r You can put several tables in a single column, using multiple code chunks, so that you can set ft.left and ft.top for each table. I\u0026rsquo;ll demonstrate that in the last section.\nConditional logic for results In the placeholder text for the table slide, I mentioned that you\u0026rsquo;ll probably want to use conditional logic for the results, otherwise every iteration of the child slide will have exactly the same text. Let\u0026rsquo;s say we need to run an ANOVA on each outcome. Yes, there are issues with this, one of which is that you\u0026rsquo;d need to be paying attention to how many comparisons you\u0026rsquo;re making, and adjust p-values appropriately. I\u0026rsquo;m not going to worry about that for the moment.\nIn this part, I\u0026rsquo;ve added a bit of formatting to the tables. One thing I did was define a border. To reduce typing, I add the command h1 \u0026lt;- officer::fp_border( width=0.75 ) to the setup chunk of my document. Often I define several lines of varying width and style (e.g., a dashed line). This way I can use them in any table.\n```{r, results=\u0026quot;asis\u0026quot;}\rcat(\u0026quot;# ANOVA: \u0026quot;, param_ii_nice, \u0026quot;\u0026quot; )\r```\r:::::: {.columns}\r::: {.column width=\u0026quot;40%\u0026quot;}\r```{r, ft.left=1, ft.top=2 }\riris %\u0026gt;%\rgroup_by( Species ) %\u0026gt;%\rsummarize(\rMean = round( mean( get(param_ii) ), 2 ),\rSD = round( sd( get(param_ii) ), 3 ),\rN = n()\r) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit() %\u0026gt;%\rborder( part=\u0026quot;header\u0026quot;, i=1, border.top=h1, border.bottom=h1 ) %\u0026gt;%\rborder( part=\u0026quot;body\u0026quot;, i=3, border.bottom=h1 ) %\u0026gt;%\ralign( part=\u0026quot;all\u0026quot;, j=2:4, align=\u0026quot;center\u0026quot; )\r```\r```{r, ft.left=1, ft.top=4 }\rfrml \u0026lt;- as.formula( str_c( param_ii , \u0026quot; ~ Species\u0026quot; ) )\rlm_out \u0026lt;- lm( frml, data=iris )\rpost_hoc \u0026lt;- TukeyHSD( aov(lm_out) )$Species %\u0026gt;%\ras.data.frame() %\u0026gt;%\rrownames_to_column() %\u0026gt;%\rrename( \u0026quot;Contrast\u0026quot; = \u0026quot;rowname\u0026quot;, \u0026quot;Diff\u0026quot;=\u0026quot;diff\u0026quot;, \u0026quot;pvalue\u0026quot;=\u0026quot;p adj\u0026quot;) %\u0026gt;%\rdplyr::select( Contrast, Diff, pvalue ) %\u0026gt;%\rmutate( tpval = ifelse( pvalue \u0026lt; 0.0001, \u0026quot;\u0026lt; 0.0001\u0026quot;, sprintf(\u0026quot;%0.4f\u0026quot;, pvalue) ) )\rpost_hoc %\u0026gt;% dplyr::select( Contrast, Diff, tpval ) %\u0026gt;%\rflextable() %\u0026gt;% theme_zebra() %\u0026gt;% autofit() %\u0026gt;%\rset_header_labels( \u0026quot;tpval\u0026quot;=\u0026quot;p-value\u0026quot; ) %\u0026gt;%\rborder( part=\u0026quot;header\u0026quot;, i=1, border.top=h1, border.bottom=h1 ) %\u0026gt;%\rborder( part=\u0026quot;body\u0026quot;, i=3, border.bottom=h1 ) %\u0026gt;%\ralign( part=\u0026quot;all\u0026quot;, j=2:3, align=\u0026quot;center\u0026quot; )\r```\r:::\r::: {.column width=\u0026quot;60%\u0026quot;}\r```{r, results=\u0026quot;asis\u0026quot;}\r# Conditional logic to build simple sentences for results\rany_diff \u0026lt;- any( post_hoc[[\u0026quot;pvalue\u0026quot;]] \u0026lt;= 0.05 )\rwhich_diff \u0026lt;- which( post_hoc[[\u0026quot;pvalue\u0026quot;]] \u0026lt;= 0.05 )\rif( any_diff ){\rwhich_contrasts \u0026lt;- str_replace( post_hoc[[\u0026quot;Contrast\u0026quot;]][which_diff], \u0026quot;-\u0026quot;, \u0026quot; and \u0026quot;)\rndiff \u0026lt;- length(which_contrasts)\rbullets \u0026lt;- str_c( \u0026quot;- Pairwise comparisons conducted using Tukey's method.\\n\u0026quot;,\r\u0026quot;- Significant differences in mean \u0026quot;, param_ii_nice, \u0026quot; were found between: \u0026quot; )\rif( ndiff==1 ){\rbullets \u0026lt;- str_c( bullets, which_contrasts )\r} else if( ndiff==2 ){\rbullets \u0026lt;- str_c( bullets, str_c( which_contrasts, collapse=\u0026quot; as well as \u0026quot; ), \u0026quot;.\u0026quot; )\r} else{\rbullets \u0026lt;- str_c( bullets, str_c( which_contrasts[-ndiff], collapse=\u0026quot;, \u0026quot; ), \u0026quot;, and between \u0026quot;, which_contrasts[ndiff], \u0026quot;.\u0026quot; )\r}\r} else{\rbullets \u0026lt;- str_c( \u0026quot;- Pairwise comparisons conducted using Tukey's method.\\n\u0026quot;,\r\u0026quot;- There were no significant differences in mean \u0026quot;, param_ii_nice, \u0026quot; detected.\u0026quot; )\r}\rcat( bullets )\r```\r:::\r::::::\r I\u0026rsquo;ll leave you to inspect the logic creating the sentences. The basic idea is to create a string which has Markdown syntax and then print it using the chunk option results=\u0026quot;asis\u0026quot;. You can get fancier, like incorporating the p-values or other results, and so on. But again, my point here is to provide a building block so that you can use it to do things that I haven\u0026rsquo;t even thought about.\nOne thing about this is that the iris dataset happens to show a difference in mean between all species and for all variables. If you want to see the logic changing the slides, either use a different dataset (you\u0026rsquo;ll naturally need to change the analysis codes!), or when you load the data, make some modification so that the means won\u0026rsquo;t differ. For example, you can permute one variable with the line: iris[[\u0026quot;Sepal.Length\u0026quot;]] \u0026lt;- sample( iris[[\u0026quot;Sepal.Length\u0026quot;]] )\nSo that\u0026rsquo;s it for this post. I hope it will give you a nice little trick to put in your back pocket for use in some project in the future.\n","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596307830,"objectID":"d851dc4bd1d986028f4e029f7d7d5dab","permalink":"/post-stats/looping-powerpoint-slides-in-rmarkdown/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/post-stats/looping-powerpoint-slides-in-rmarkdown/","section":"post-stats","summary":"Preface For this post, I\u0026rsquo;m going to assume you are fairly familiar with RMarkdown. If not, you may want to start with RMarkdown: The Definitive Guide, or R for Data Science. The short version is that RMarkdown is a flavor of the markup language Markdown, which uses plain-text formatting and can be rendered into a variety of output types, including PDF, MS Word, HTML, and others. With RMarkdown, you can incorporate R \u0026ldquo;code chunks\u0026rdquo; to create a fully reproducible document.","tags":["powerpoint","loop","slides","child document"],"title":"Looping PowerPoint Slides in RMarkdown ","type":"post-stats"},{"authors":[],"categories":["science"],"content":"The short version When encountering scientific claims - or really any objective claim - it can be difficult to know what to believe, especially when there seems to be conflicting stories. In the world of COVID-19 shutdowns and stay-at-home orders, it can be hard to parse through the deluge of information and figure out what\u0026rsquo;s going on. The following set of questions can serve as a sort of \u0026ldquo;smell test\u0026rdquo; to get an initial sense of the validity of the claims at hand.\n Who or what is the source? Why should we listen to them? Are they citing sources or presenting data? What is their expertise? Are the claims extravagant? Does the source have a known bias?  The long version The COVID-19 pandemic rather abruptly thrust science into peoples\u0026rsquo; faces. In a very short timeframe we went from ordinary news, to \u0026ldquo;There\u0026rsquo;s an outbreak in China,\u0026rdquo; to \u0026ldquo;It\u0026rsquo;s spreading worldwide,\u0026rdquo; to \u0026ldquo;It\u0026rsquo;s here and governors are issuing expansive shutdowns and stay-at-home orders.\u0026rdquo; People were suddenly inundated statistical claims and scientific results (as of July 20, 2020, Nature reported there to be 65,470 scientific papers on COVID-19).\nThe problem is that most people are not statisticians or epidemiologists. Most people are not microbiologists, immunologists, or researchers in a slew of other biomedical fields. Most people are not highly trained in research methods and reading the results.\nA constantly updating deluge of technical information that most people have a difficult time understanding creates a prime situation for misinformation, confirmation bias, and conspiracy theories to propagate. We have seen much of this in the course of the COVID-19 pandemic.\nMy intent in this post is to give a few questions you can ask yourself to assess new information - whether it\u0026rsquo;s a scientific article, a news report, or a post on social media. My set of questions is not exhaustive, but represents a fairly good first pass to sort out what is reliable from what is not. Many of the claims that I\u0026rsquo;ve seen posted across social media fail at least one, if not several or all, of the questions on my list.\n Question 1: Who or what is the source, and why should we listen to them? When it comes to objective and scientific claims, I think there are two main things to pay attention to: Evidence and expertise. If you can\u0026rsquo;t identify at least one of these things, there is no reason to believe the source. It doesn\u0026rsquo;t matter if something \u0026ldquo;just makes sense\u0026rdquo; or if someone\u0026rsquo;s \u0026ldquo;gut feeling\u0026rdquo; leans a certain way. That\u0026rsquo;s valid for subjective matters, but not for assessing scientific results.\nSo from the get-go, ask yourself: Is there evidence being cited, or does this person have expertise? If the answers are \u0026ldquo;No\u0026rdquo; and \u0026ldquo;No,\u0026rdquo; then you should take anything they are claiming with a very large grain of salt.\nQuestion 2a: Are they providing evidence? There is a saying among statisticians: \u0026ldquo;In God we trust, all others bring data.\u0026rdquo; Probably a bit more common is another saying - called Hitchens\u0026rsquo;s razor: \u0026ldquo;What can be asserted without evidence can also be dismissed without evidence.\u0026rdquo; The point with both of these is that evidence reigns supreme.\nThat being said, simply presenting data does not in and of itself validate a claim. Data needs to be interpreted, and interpreting data is not always often not straightforward. There could be missing context from the subject-matter domain, technical language that could be easily misinterpreted, different \u0026ldquo;levels\u0026rdquo; of data quality, or other issues complicating the understanding of evidence.\nSome might not understand the different levels of evidence. Some more formalized thoughts are available from Burns, Rohrich, and Chung (2011) or Glasofer and Townsend (2019). To summarize a bit from these:\n Tier 1: Systematic review of Tier 2 studies. Tier 2: Carefully designed experiments, including randomization and control. In medical fields, this should include blinding (preferably double-blind), and would often be dscribed as a randomized controlled trial (RCT). Tier 3: Non-experimental studies (including observational studies). These can be subject to various biases. Tier 4: Expert opinion, case studies (i.e., just looking at one or a few individual subjects).  I\u0026rsquo;m sure people could easily expand this list, or make some small adjustments. The point here is not to be comprehensive or \u0026ldquo;final,\u0026rdquo; it\u0026rsquo;s to provide a quick and dirty way to be able to think about evidence being presented. So if you see two seemingly contradictory results, instead of picking the one that you like the best, you might be able to think about the data quality from each study, and hence assess which result is based on the more reliable data.\nThis is one way in which expertise helps: Experts have been trained and have experience interpreting data from their field. They generally have a better understanding of data quality, of what the data might say, and of what data are important to consider. That last bit can be important: A person outside the field might point to some data and say \u0026ldquo;Look at X, this is big!\u0026rdquo; But a trained and experienced professional in the field might say, \u0026ldquo;Well, X isn\u0026rsquo;t really that important, we should be talking about Y instead.\u0026rdquo;\nFor example, suppose we were talking about the dangers of an electrical shock. Someone might say, \u0026ldquo;I sustained a 1,000 volt shock and I was just fine.\u0026rdquo; But someone who knows a bit more could retort \u0026ldquo;Okay, but voltage isn\u0026rsquo;t the important thing, amperage is much more relevant here.\u0026rdquo;\nIn general, I\u0026rsquo;d rank the \u0026ldquo;reliability\u0026rdquo; of claims as:\n Expert who is citing good evidence: Very high. Expert who is citing weak evidence: Good, pending the precise nature of the data. Expert speaking about their field, but not citing evidence: Good, but with reservations. Even experts can be mistaken, or have lost their credibility as did the former scientist featured in \u0026ldquo;Plandemic.\u0026rdquo; Non-expert citing evidence: Good, but with reservations, depending on quality of evidence and appropriateness of interpretation. Non-expert without evidence: Low, no reason to give them the time of day.  Question 2b: What is their expertise? Now, just because someone has expertise doesn\u0026rsquo;t mean they are automatically a reliable source. The field of expertise also matters. I myself am a statistician, I can speak to statistical matters. Statistics is somewhat unique in that it is the language of science, permeating the scientific endeavor: Researchers need to communicate their results, and that communication typically involves statistical experimentation and analysis. As John Tukey put it, statisticians \u0026ldquo;get to play in everyone\u0026rsquo;s backyard\u0026rdquo;, meaning we get exposed to and pick up a bit of various topics. We might not be experts in the field, but we can usually understand and assess the results.\nBut you shouldn\u0026rsquo;t ask me to describe why or how a given protein does what it does. You shouldn\u0026rsquo;t ask me to describe the mechanism by which a medication functions. Or why some molecule will or will not react with another, or how the gravity of the sun and planets interact with each other in the solar system. Similarly, you should not ask me to run electrical wiring for a house or to repair a car engine.\nMy point here is that specialty matters. Nobody is an expert in all things. Some fields are closely related, so an expert in one may be highly competent in another, but many fields are exceedingly diverse. So it\u0026rsquo;s important to keep someone\u0026rsquo;s specialty in mind when listening to what they have to say.\nI\u0026rsquo;ve seen a number of times people quote physicians (MD or DO) when talking about COVID-19. That\u0026rsquo;s perfectly fine, when the topic is a clinical matter. Just as being a statistician doesn\u0026rsquo;t make me an expert in medicine, having a medical degree doesn\u0026rsquo;t make one an expert in statistics or epidemiology. Just because a topic is related in some way to the medical field does not make physicians the foremost experts.\nOne job that I\u0026rsquo;ve held has been as a professor in a biostatistics department. I\u0026rsquo;ve collaborated with MDs and residents, and I\u0026rsquo;ve taught aspiring doctors. Some of the doctors had a solid grasp of statistical methods. Most did not. Fewer still should conduct their own statistical analysis. The students aiming for med school often took one or two courses in statistics, not enough to offer a comprehensive understanding statistics, much less the ability to properly conduct statistical analysis. Some were diligent students that probably retained the ideas of the course past the end of the semester. But most were (due to the competitiveness of med school applications) were hyper-focused on grades rather than understanding, and likely retained little if anything that we talked about after the final exam.\nTo be sure, there are some MDs who are excellent researchers, Dr. Anthony Fauci is a prime example. But when talking about a statistical or epidemiological analysis an MD is not automatically an expert, they may very well be talking outside of their domain of expertise.\nQuestion 4: Are the claims extravagant? As Carl Sagan said, \u0026ldquo;Extraordinary claims require extraordinary evidence.\u0026rdquo; There are sometimes dramatic breakthroughs, but usually results are much more mundane or incremental in nature. If you see a headline or claim that seems too good to be true, it\u0026rsquo;s probably either misleading clickbait, misinformation, or conspiracy.\nForbes did an article about this subject, you can find it here. The Atlantic did a piece on Dr. John Ioannidis which can be read here. In it, Dr. Ioannidis is quoted to say:\n Often the claims made by studies are so extravagant that you can immediately cross them out without needing to know much about the specific problems with the studies\n Even scientists can be guilty of over-hyping their conclusions. Press releases may exaggurate a bit more, media outlets take it up another few notches, and people on social media blow it up to cosmic proportions.\nQuestion 5: Does the source have a known bias? Biased sources are likely to selectively interpret results, and either overlook or deliberately obscure flaws. That\u0026rsquo;s why a lot of research has a statement about funding sources. Remember those doctors from Bakersfield, California? They run a private urgent care facilities. They had an enormous conflict of interest: Personal profit. If they did not have that incredible bias, they may have been more reserved about their comments, and perhaps noticed the dramatic statistical and scientific errors they were making, and for which they were condemned by two major medical associations, the American College of Emergency Physicians and the American Academy of Emergency Medicine.\nA financial bias is not the only bias out there. Various news outlets have a strong bias, and will omit certain facts, or selectively choose stories that spin a particular narrative. If a source is known to have a strong bias (be it right or left), then caution should be exercised in assessing the claims being made. This goes doubly so when the source itself acknowledges that it is biased, which ties back to the first question: Who is the source and why should we believe them? If the source admits they are biased, we should be skeptical of anything they have to say.\nThat\u0026rsquo;s not to say that a source can\u0026rsquo;t be biased without acknowleding it, the bias might just be more difficult to verify. My point is that if the source is already overlty admitting a bias then you should from the get-go not expect a neutral or objective consideration of the topic. And if that\u0026rsquo;s the case, then you should again take any conclusion with a large grain of salt.\n So that\u0026rsquo;s it. The first real post I put on my website, and what I think is a solid set of questions to consider when faced with some scientific claim.\n","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596233884,"objectID":"92f5a8417c8fe0fa6ead5a2fd18b57db","permalink":"/post-other/a-scientific-smell-test/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/post-other/a-scientific-smell-test/","section":"post-other","summary":"The short version When encountering scientific claims - or really any objective claim - it can be difficult to know what to believe, especially when there seems to be conflicting stories. In the world of COVID-19 shutdowns and stay-at-home orders, it can be hard to parse through the deluge of information and figure out what\u0026rsquo;s going on. The following set of questions can serve as a sort of \u0026ldquo;smell test\u0026rdquo; to get an initial sense of the validity of the claims at hand.","tags":["covid","coronavirus","evidence"],"title":"A Scientific Smell Test","type":"post-other"},{"authors":[],"categories":["general"],"content":"Welcome! If you\u0026rsquo;ve found your way to this page \u0026hellip; well, I\u0026rsquo;m more than a little surprised. This is my personal website. I\u0026rsquo;m not entirely sure what I\u0026rsquo;ll be doing here. My plan at the moment is to use it mainly for:\n Listing professional things like publications independently of my place of employment. Host the slides for talks or workshops that I give.  I may have some posts (is that \u0026lsquo;blogging\u0026rsquo;? Save me!) about topics that pique my interest. This would likely be my thoughts on some bit of Statistics or science that came across my screen.\nI\u0026rsquo;ve been talking about science and statistics in the context of COVID-19 on social media for a while now. I may transition some of that here, since social media isn\u0026rsquo;t usually conducive to the long-form comments I make often enough.\n","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594839175,"objectID":"3b462785c865e5125b0dd0f932deec72","permalink":"/post-other/welcome-to-my-site/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/post-other/welcome-to-my-site/","section":"post-other","summary":"Welcome! If you\u0026rsquo;ve found your way to this page \u0026hellip; well, I\u0026rsquo;m more than a little surprised. This is my personal website. I\u0026rsquo;m not entirely sure what I\u0026rsquo;ll be doing here. My plan at the moment is to use it mainly for:\n Listing professional things like publications independently of my place of employment. Host the slides for talks or workshops that I give.  I may have some posts (is that \u0026lsquo;blogging\u0026rsquo;?","tags":["greetings","welcome"],"title":"Welcome to my site","type":"post-other"},{"authors":["Casey Jelsema","Rajib Paul","Joseph W McKean"],"categories":["publication","methods"],"content":"","date":1590883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590883200,"objectID":"89aa3b96937486d2635c159123cc612d","permalink":"/publication/2020-robust-v/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/publication/2020-robust-v/","section":"publication","summary":"For large datasets, spatial covariances are often modeled using basis functions and covariance of a reduced dimensional latent spatial process. For skewed data, likelihood based approaches with Gaussian assumption may not lead to faithful inference. Any $L_{2}$ norm based estimation is susceptible to long tails and outliers due to contamination. Our method is based on an empirical binned covariance matrix using the median absolute deviation and minimizes $L_{1}$ norm between empirical covariance and the model covariance. The consistency of the proposed estimate is established theoretically. The improvement is demonstrated using simulated data and cloud data obtained from NASA's Terra satellite.","tags":["spatial"],"title":"Robust estimation of reduced rank models to large spatial datasets","type":"publication"},{"authors":[],"categories":["r package"],"content":"CLME stands for Constrained Linear Mixed Effects. I wrote this R package (CRAN link) during my postdoctoral work at NIEHS.\nThe fundamental idea is similar to the Jonckheere–Terpstra or any other test for ordered alternatives: If the treatment groups are ordinal, then a trend of some sort may be of interest. If a researcher has such a hypothesis, they can not only test for the ordered alternative, but they can constrain the estimation to respect the order from the alternative hypothesis. This results in getting more power than a comparable test that does not impose constraints or test for an order (e.g., ANOVA).\nFeel free to check out the github repository. There are a variety of improvements I\u0026rsquo;d like to make, including cleaning up my code, removing dependancies, and adding some features. Just need time to get to them. If you\u0026rsquo;d like to join, give me a shout!\nDisclaimer: This is really just a post to get something into the \u0026ldquo;Project\u0026rdquo; space of my website. I\u0026rsquo;ll probably modify this post later to add clarity.\n","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565797783,"objectID":"1cf66225e7593c17a980f29de3004b4b","permalink":"/project/clme/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/project/clme/","section":"project","summary":"CLME stands for Constrained Linear Mixed Effects. I wrote this R package (CRAN link) during my postdoctoral work at NIEHS.\nThe fundamental idea is similar to the Jonckheere–Terpstra or any other test for ordered alternatives: If the treatment groups are ordinal, then a trend of some sort may be of interest. If a researcher has such a hypothesis, they can not only test for the ordered alternative, but they can constrain the estimation to respect the order from the alternative hypothesis.","tags":["package","order restricted inference","bootstrap"],"title":"CLME","type":"project"},{"authors":[],"categories":["lecture","talk"],"content":"\rThe slides for my Introduction to Spatial Data (geared for non-Statisticians) can be found here:\nhttps://jelsema.github.io/presentations/2019-intro-spatial/intro_spatial.html#1\n","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565794885,"objectID":"7484acca2de3c888eb0a9ddec9379035","permalink":"/talk/wvu-bios-604-intro-to-spatial/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/talk/wvu-bios-604-intro-to-spatial/","section":"talk","summary":"The slides for my Introduction to Spatial Data (geared for non-Statisticians) can be found here:\nhttps://jelsema.github.io/presentations/2019-intro-spatial/intro_spatial.html#1","tags":["spatial","intro"],"title":"WVU BIOS 604 Intro to Spatial","type":"talk"},{"authors":["Casey Jelsema","Richard Kwok","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"fd1c3abfdd33965446ec4475cad4126e","permalink":"/publication/2019-threshold-knots/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2019-threshold-knots/","section":"publication","summary":"Large spatial datasets are typically modelled through a small set of knot locations; often these locations are specified by the investigator by arbitrary criteria. Existing methods of estimating the locations of knots assume their number is known a priori, or are otherwise computationally intensive. We develop a computationally efficient method of estimating both the location and number of knots for spatial mixed effects models. Our proposed algorithm, Threshold Knot Selection (TKS), estimates knot locations by identifying clusters of large residuals and placing a knot in the centroid of those clusters. We conduct a simulation study showing TKS in relation to several comparable methods of estimating knot locations. Our case study utilizes data of particulate matter concentrations collected during the course of the response and clean-up effort from the 2010 *Deepwater Horizon* oil spill in the Gulf of Mexico.","tags":["spatial"],"title":"Threshold knot selection for large-scale spatial models with applications to the Deepwater Horizon disaster","type":"publication"},{"authors":["Eric Law","Keith Morris","Casey Jelsema"],"categories":["publication","methods"],"content":"","date":1529280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529280000,"objectID":"11de56111c7b98c52114236295bf78e2","permalink":"/publication/2018-test-fire2/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-test-fire2/","section":"publication","summary":"The Association of Firearm and Toolmark Examiners recommends a minimum of two test fires be performed when an unknown firearm is submitted to a laboratory prior to doing a comparison with a cartridge case collected from a crime scene. Limited research has been performed to determine how many test fires are necessary to be representative of the match distribution of a firearm. Various makes and models of firearms comprising five calibers were tested using a hybrid equivalence test to determine how many cartridge cases were required to represent the match distribution of an unknown firearm based on both breech face and firing pin correlation scores from an IBIS® Heritage^(TM) System. The same general trend was observed for each caliber of firearm where the equivalence percentage increased from 10 to 30 cartridge cases. Overall, 15 cartridge cases are sufficient for above an 80% probability of representing the full match distribution for an unknown firearm. To approach full equivalence, 25 cartridge cases are enough because 30 cartridge cases were not found to be significantly higher in equivalence percentage for any caliber of firearm tested.","tags":["monte carlo","equivalence testing","forensics","test fires"],"title":"Determining the number of test fires needed to represent the variability present within firearms of various calibers","type":"publication"},{"authors":["Ori Davidov","Casey Jelsema","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"daa2d98e4f42f496756703976902fb26","permalink":"/publication/2018-osin/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-osin/","section":"publication","summary":"There are many applications in which a statistic follows, at least asymptotically, a normal distribution with a singular or nearly singular variance matrix. A classic example occurs in linear regression models under multicollinearity but there are many more such examples. There is well-developed theory for testing linear equality constraints when the alternative is two-sided and the variance matrix is either singular or nonsingular. In recent years, there is considerable, and growing, interest in developing methods for situations in which the estimated variance matrix is nearly singular. However, there is no corresponding methodology for addressing one-sided, that is, constrained or ordered alternatives. In this article, we develop a unified framework for analyzing such problems. Our approach may be viewed as the trimming or winsorizing of the eigenvalues of the corresponding variance matrix. The proposed methodology is applicable to a wide range of scientific problems and to a variety of statistical models in which inequality constraints arise. We illustrate the methodology using data from a gene expression microarray experiment obtained from the NIEHS’ Fibroid Growth Study. Supplementary materials for this article are available online.","tags":["order restricted inference","bootstrap","singular"],"title":"Testing for Inequality Constraints in Singular Models by Trimming or Winsorizing the Variance Matrix","type":"publication"},{"authors":["Amy Hessl","Kevin Anchukaitis","Casey Jelsema","Benjamin Cook","Oyunsanaa Byambasuren","Caroline Leland","Baatarbileg Nachin","Neil Pederson","Hanqin Tian","Laia Andreu Hayles"],"categories":["publication","collaborative"],"content":"","date":1520985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520985600,"objectID":"ef5f07c837121252d69a8f0686615cb5","permalink":"/publication/2018-mongolia-drought/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2018-mongolia-drought/","section":"publication","summary":"The severity of recent droughts in semiarid regions is increasingly attributed to anthropogenic climate change, but it is unclear whether these moisture anomalies exceed those of the past and how past variability compares to future projections. On the Mongolian Plateau, a recent decade-long drought that exceeded the variability in the instrumental record was associated with economic, social, and environmental change. We evaluate this drought using an annual reconstruction of the Palmer Drought Severity Index (PDSI) spanning the last 2060 years in concert with simulations of past and future drought through the year 2100 CE. We show that although the most recent drought and pluvial were highly unusual in the last 2000 years, exceeding the 900-year return interval in both cases, these events were not unprecedented in the 2060-year reconstruction, and events of similar duration and severity occur in paleoclimate, historical, and future climate simulations. The Community Earth System Model (CESM) ensemble suggests a drying trend until at least the middle of the 21st century, when this trend reverses as a consequence of elevated precipitation. Although the potential direct effects of elevated CO2 on plant water use efficiency exacerbate uncertainties about future hydroclimate trends, these results suggest that future drought projections for Mongolia are unlikely to exceed those of the last two millennia, despite projected warming.","tags":["copula","geography","drought","mongolia"],"title":"Past and future drought in Mongolia","type":"publication"},{"authors":["Eric Law","Keith Morris","Casey Jelsema"],"categories":["publication","methods"],"content":"","date":1493769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493769600,"objectID":"6160d3ede8397d566b4b0f1ba430cd2f","permalink":"/publication/2017-test-fire1/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2017-test-fire1/","section":"publication","summary":"Many studies have been performed in recent years in the field of firearm examination with the goal of providing an objective method for comparisons of fired cartridge cases. No published research to support the number of test fires needed to represent the variability present within the impressions left on a cartridge case could be found. When a suspect firearm is submitted to a firearm examiner, typically two to four test fires are performed. The recovered cartridge cases are compared to each other to determine which characteristics from the firearm are reproducing, and then compared to any cartridge cases collected at a crime scene. The aim of this research was to determine the number of test fires examiners should perform when a suspect firearm is submitted to the lab to balance cartridge case acquisition time with performance accuracy. Each firearm in the IBIS® database at West Virginia University® is represented by approximately 100 fired cartridge case entries. Random samples of cartridge cases were taken separately from the breech face match score and firing pin match score lists. This subset was compared to the total match distribution of the firearm using a hybrid equivalence test to determine if the subset of similarity scores were statistically equivalent to the larger distribution of scores. For the sampled distribution to remain above 80% equivalent to the match distribution, a minimum of 15 cartridge cases should be used to model the match distribution, based on IBIS® scores. Thirty cartridge cases is a conservative estimate, allowing one to determine that the location and dispersion of the match and sampling distributions are equivalent with nearly 100% probability.","tags":["monte carlo","equivalence testing","forensics","test fires"],"title":"Determining the number of test fires needed to represent the variability present within 9mm Luger firearms","type":"publication"},{"authors":["Casey Jelsema","Shyamal Peddada"],"categories":["publication","methods"],"content":"","date":1479513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479513600,"objectID":"b39d1577f8cf960898d166be326f8ad4","permalink":"/publication/2016-clme/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2016-clme/","section":"publication","summary":"In many applications researchers are typically interested in testing for inequality constraints in the context of linear fixed effects and mixed effects models. Although there exists a large body of literature for performing statistical inference under inequality constraints, user friendly statistical software implementing such methods is lacking, especially in the context of linear fixed and mixed effects models. In this article we introduce **CLME**, a package in the **R** language that can be used for testing a broad collection of inequality constraints. It uses residual bootstrap based methodology which is reasonably robust to non-normality as well as heteroscedasticity. The package is illustrated using two data sets. The package also contains a graphical user interface built using the shiny package.","tags":["order restricted inference","bootstrap"],"title":"CLME An R package for linear mixed effects models under inequality constraints","type":"publication"},{"authors":["Thomas (Joost) Van't Erve","Fred Lih","Casey Jelsema","Leesa Deterding","Thomas Eling","Ronald Mason","Maria Kadiiska"],"categories":["publication","collaborative"],"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"d9c3553108ffc3099a82e9d384d547fc","permalink":"/publication/2016-joost/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2016-joost/","section":"publication","summary":"Oxidative stress is elevated in numerous environmental exposures and diseases. Millions of dollars have been spent to try to ameliorate this damaging process using anti-oxidant therapies. Currently, the best accepted biomarker of oxidative stress is the lipid oxidation product 8-iso-prostaglandin F2α (8-iso-PGF2α), which has been measured in over a thousand human and animal studies. 8-iso-PGF2α generation has been exclusively attributed to nonenzymatic chemical lipid peroxidation (CLP). However, 8-iso-PGF2α can also be produced enzymatically by prostaglandin-endoperoxide synthases (PGHS) in vivo. When failing to account for PGHS-dependent generation, 8-iso-PGF2α cannot be interpreted as a selective biomarker of oxidative stress. We investigated the formation of 8-iso-PGF2α in rats exposed to carbon tetrachloride (CCl4) or lipopolysaccharide (LPS) using the 8-iso-PGF2α/PGF2α ratio to quantitatively determine the source(s) of 8-iso-PGF2α. Upon exposure to a 120mg/kg dose of CCl4, the contribution of CLP accounted for only 55.6±19.4% of measured 8-iso-PGF2α, whereas in the 1200mg/kg dose, CLP was the predominant source of 8-iso-PGF2α (86.6±8.0% of total). In contrast to CCl4, exposure to 0.5mg/kg LPS was characterized by a significant increase in both the contribution of PGHS (59.5±7.0) and CLP (40.5±14.0%). In conclusion, significant generation of 8-iso-PGF2α occurs through enzymatic as well as chemical lipid peroxidation. The distribution of the contribution is dependent on the exposure agent as well as the dose. The 8-iso-PGF2α/PGF2α ratio accurately determines the source of 8-iso-PGF2α and provides an absolute measure of oxidative stress in vivo.","tags":["order restricted inference","bootstrap"],"title":"Reinterpreting the best biomarker of oxidative stress: The 8-iso-prostaglandin F2α/prostaglandin F2α ratio shows complex origins of lipid peroxidation biomarkers in animal models","type":"publication"},{"authors":["Rajib Paul","Casey Jelsema","Rex Lau"],"categories":["publication","methods"],"content":"","date":1432166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1432166400,"objectID":"ad6c9f50ae37a69c5c44e910231fbc56","permalink":"/publication/2015-frssm/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2015-frssm/","section":"publication","summary":"In environmental studies, the datasets exhibiting non-Gaussian properties, such as heavier or lighter tails and multimodality, are very common. The research on dealing with such datasets in reduced rank perspectives is very limited. In this chapter, a flexible class of Bayesian reduced rank spatial model is developed that can handle non-Gaussian properties adequately. The spatial model provides the flexibility to deal with such properties through scale mixtures of Gaussian distributions and a two-level marginally noninformative inverse-Wishart prior. A general framework for posterior summaries based on Markov Chain Central Limit Theorem (MCCLT) has been developed and conditions of MCCLT on ergodic averages are theoretically verified. The Monte Carlo standard errors based on MCCLT are computed using batch-mean method. The performance of the proposed model and method are assessed using several simulated datasets and a dataset on daily maximum of total column ozone obtained from National Aeronautic and Space Administration Terra satellite.","tags":["spatial"],"title":"A flexible class of reduced rank spatial models for large non-Gaussian datasets","type":"publication"},{"authors":["Casey Jelsema","Rajib Paul"],"categories":["publication","methods"],"content":"","date":1375142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1375142400,"objectID":"ed3635726a97802e43fbe1de050476a1","permalink":"/publication/2013-lognormal-coal/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/publication/2013-lognormal-coal/","section":"publication","summary":"We analyze data on the geochemical make-up of coal samples throughout the state of Illinois. The goal is to estimate the geochemical properties at unobserved locations over a specified region. Multivariate spatial modeling requires characterization of both spatial and cross-spatial covariances. Reduced rank spatial models are popular in analyzing large spatial datasets. We develop a multivariate spatial mixed effects model for log-normal processes and show how to implement with compositional data to predict on point locations, as well as the average prediction over a finite area. We use log-normal kriging for the components of compositional data, and show how to obtain estimates and measures of precision in isometric log-ratio coordinates.","tags":["spatial"],"title":"Lognormal block kriging with applications to goal geology","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"Experience / Contact","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Experience / Contact","type":"widget_page"}]