<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hypergeometric | Organized Chaos</title>
    <link>/tags/hypergeometric/</link>
      <atom:link href="/tags/hypergeometric/index.xml" rel="self" type="application/rss+xml" />
    <description>hypergeometric</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 10 Jun 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>hypergeometric</title>
      <link>/tags/hypergeometric/</link>
    </image>
    
    <item>
      <title>Estimating proportions for finite populations</title>
      <link>/post-stats/estimating-proportions-for-finite-populations/</link>
      <pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post-stats/estimating-proportions-for-finite-populations/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In a &lt;a href=&#34;https://jelsema.github.io/post-stats/estimating-binomial-proportions/&#34;&gt;previous post&lt;/a&gt; I talked about estimating a binomial proportion, including for rare events. The reason I wrote that was for background to this post. Here, we’ll again be looking at estimating proportions - and can include rare events - but with an added wrinkle: A population that is finite.&lt;/p&gt;
&lt;p&gt;In the Binomial case, every observation is assumed to be independent and have a fixed probability of the outcome of interest (a “success” or a “failure”). But when the population is finite, then the probability of success changes. One of the quintessential examples is dealing cards from a well-shuffled standard deck without replacement (that is: draw the card, and don’t put it back). Say we’re interested in the probability of getting a spade. On the first draw, this is 13/52. But the second card is either 13/51 (if the first card was not a spade) or 12/51 (if the first card was a spade), neither of which are the same as 13/52.&lt;/p&gt;
&lt;p&gt;Sampling binary outcomes (e.g., “spade” vs “not a spade”) from a finite population gives rise to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hypergeometric_distribution&#34;&gt;hypergeometric distribution&lt;/a&gt;. This is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(X = k) = \dfrac{\binom{M}{k}\binom{N-M}{n-k}}{\binom{N}{n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(k \in \max(0,n+M-N)\)&lt;/span&gt; and&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the size of the population.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; is the number of successes (or whatever event of interest) in the population.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of samples taken.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of successes in the sample.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What this is doing is just counting the number of ways, when drawing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; samples, to get &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; successes.&lt;/p&gt;
&lt;p&gt;The same situation arises in other - very practical - scenarios.&lt;/p&gt;
&lt;p&gt;Cars have a specific year and model. Once the next “year-model” starts, no more of the previous year’s version can be manufactured. Hence, there are a finite number of, say, 2011 Chevy Cruzes available. If a problem is detected with that year-model, then the number of 2011 Chevy Cruzes which have this problem will follow a hypergeometric distribution.&lt;/p&gt;
&lt;p&gt;For example, my own vehicle was subject to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Takata_Corporation#Recalls&#34;&gt;Takata Corp&lt;/a&gt; airbag recall. From that wiki page, I found there were recalls of &lt;a href=&#34;https://en.wikipedia.org/wiki/Chevrolet_Cruze#Reliability_and_recalls&#34;&gt;Chevy Cruze&lt;/a&gt; models as well.&lt;/p&gt;
&lt;p&gt;Generalizing this idea, many components are manufactured in “batches” of some sort. That could be particular &lt;a href=&#34;see%20General%20Mills,%202019&#34;&gt;date(s) of production&lt;/a&gt; or &lt;a href=&#34;https://www.npr.org/2020/11/06/929078678/cdc-report-officials-knew-coronavirus-test-was-flawed-but-released-it-anyway&#34;&gt;batchs&lt;/a&gt; of raw materials. Additionally, there could be errors in the manufacturing or assembly which could potentially affect some but not all of the units produced on that day.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimation&lt;/h2&gt;
&lt;p&gt;When the population is finite, there are generally two possible questions of interest. Looking at the parameters of the hypergeometric distribution, this should become relatively clear: Since &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; are values from the &lt;em&gt;sample&lt;/em&gt; (which is &lt;em&gt;observed&lt;/em&gt;), they are fully known and not subject to uncertainty. Instead, either &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; will be the unknown quantity (hopefully not both!). In this post, I’m going to focus on the former, so the scenario can be phrased as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For a population of size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, we take a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and observe &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; defective units. Our goal is to estimate &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, the &lt;em&gt;total&lt;/em&gt; number of defective units.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With a finite population, interest in the total number of defectives is directly related to interest in the population proportion of defectives. If we have &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; defectives in a population of size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, then the population proportion of defectives is &lt;span class=&#34;math inline&#34;&gt;\(p = M/N\)&lt;/span&gt;. So if we estimate &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; (perhaps with some interval bounds) we can easily convert to a proportion by dividing by the non-random population size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. We could also go the other way, but &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is more interpretable than &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, so it’s more natural to put results in terms of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So how do we estimate the proportion? As with the case of infinite populations I discussed in my &lt;a href=&#34;https://jelsema.github.io/post-stats/estimating-binomial-proportions/&#34;&gt;previous post&lt;/a&gt;, there are several approaches. One of them is the same as the “typical” approach: Divide the observed number of defectives by the sample size, &lt;span class=&#34;math inline&#34;&gt;\(\hat{p} = k/n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A wrinkle in this, however, is that when the population is finite, the samples are not independent, there is some covariance. As a result of this, the standard errors from before are no longer correct. One way of addressing this is through what’s called the Finite Population Correction Factor (FPCF).&lt;/p&gt;
&lt;div id=&#34;finite-population-correction-factor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Finite population correction factor&lt;/h3&gt;
&lt;p&gt;I think the “easy” way to see how the FPCF come about is to look at the mean and variance of the hypergeometric distribution. These are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E[X] = \mu &amp;amp;= n \dfrac{M}{N} \\
 &amp;amp; \\
V[X] = \sigma^{2} &amp;amp;= n \dfrac{M}{N} \dfrac{N-M}{N} \dfrac{N-n}{N-1}  
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remember that the hypergeometric distribution is dealing with the &lt;em&gt;number&lt;/em&gt; of successes (in our case, defective units), so we take &lt;span class=&#34;math inline&#34;&gt;\(X/n\)&lt;/span&gt; to deal with the proportion. This factor of &lt;span class=&#34;math inline&#34;&gt;\(1/n\)&lt;/span&gt; gets squared in the variance, so we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
E[X/n] = p &amp;amp;= \dfrac{M}{N} \\
 &amp;amp; \\
V[X/n] = \sigma^{2}_{p} &amp;amp;= \dfrac{1}{n} \dfrac{M}{N} \dfrac{N-M}{N} \dfrac{N-n}{N-1}  
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Dividing the number of defects by the sample size gets us the proportion, so in the expected value, this becomes the population proportion, &lt;span class=&#34;math inline&#34;&gt;\(p = M/N\)&lt;/span&gt;, which makes sense. If we rewrite the variance in terms of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma^{2}_{p} = \dfrac{1}{n} p(1-p) \dfrac{N-n}{N-1} = \dfrac{p(1-p)}{n} \left(\dfrac{N-n}{N-1}\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This expression looks very close to the “ordinary” version of the standard error of &lt;span class=&#34;math inline&#34;&gt;\(\hat{p}\)&lt;/span&gt; when sampling from a Binomial distribution. There’s just an extra bit that the variance is getting multiplied by. This is the FPCP:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{FPCP} = \dfrac{N-n}{N-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There is another derivation of this for a more general case (just assuming a mean and a variance), but it’s fairly long, so I don’t want to get side-tracked with that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-estimation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayesian estimation&lt;/h3&gt;
&lt;p&gt;As before, I’ve been interested in the Bayesian approach to this problem. There’s a paper by &lt;a href=&#34;https://link.springer.com/article/10.1007/s13253-015-0239-9&#34;&gt;Jones &amp;amp; Johnson (2015)&lt;/a&gt; that talks about this (as a precursor to a more complex idea). In section 2 of their paper, they have a nice summary of the approach using a concept called &lt;em&gt;superpopulation&lt;/em&gt;. This is a way of describing a theoretical population from which our population is drawn.&lt;/p&gt;
&lt;p&gt;Clear as mud, right? I think of it this way: We have a population of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; units, of which &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; are defectives (and so there is proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; defectives). But we can consider a more general process from which our population was drawn. In this more general “superpopulation” there is some underlying proportion of defectives, which we will denote &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;. With this framework, we can consider the population as being a sample of size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; from the superpopulation.&lt;/p&gt;
&lt;p&gt;The next important bit of this is that we separate the population into the &lt;em&gt;observed&lt;/em&gt; and &lt;em&gt;unobserved&lt;/em&gt; samples. These are effectively two independent samples from a Binomial distribution, of sizes &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N-n\)&lt;/span&gt;, respectively. The diagram below is how I picture it.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;digraph {\n  graph [layout = dot, rankdir = LR]\n\n  node [shape = rectangle, style = filled, fillcolor = lightblue]\n  A [label = \&#34;Superpopulation\n\npi\&#34;, shape = circle, width=3, fontsize=28]\n  \n  \n  subgraph cluster_population {\n    style=dotted; label=\&#34;Population\&#34;; fontsize=24;\n    node [shape = rectangle, style = filled, fillcolor = lightblue, width=2]\n  { \n    B [label = \&#34;Observed Sample\n\nx, n\&#34;, shape = circle, fontsize=20]\n    C [label = \&#34;Unobserved Sample\n\nM-x, N-n\&#34;, shape = circle, fontsize=20]\n  }\n  };\n\n  # edge definitions with the node IDs\n  A -&gt; B \n  A -&gt; C \n  B -&gt; A [style=dashed]\n  A -&gt; C [style=dashed]\n\n}&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;In this, a solid arrow denotes the random sampling, and a dashed arrow denotes inference. So from the superpopulation we sample the population, but we conceptualize two independent samples: One which we observe, and the other which we do not. Then the &lt;em&gt;observed&lt;/em&gt; sample is used to infer about the superpopulation, and that information about the superpopulation is used to make probabalistic statements about the unobserved sample - that is: The rest of the population.&lt;/p&gt;
&lt;p&gt;Jones &amp;amp; Johnson (2015) tell that inference about &lt;span class=&#34;math inline&#34;&gt;\(U = M-x\)&lt;/span&gt; (from which we can derive &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, and therefore &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;) will make use of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta-binomial_distribution&#34;&gt;Beta-binomial distribution&lt;/a&gt;, which I’ll denote &lt;span class=&#34;math inline&#34;&gt;\(betabin( N, \alpha, \beta)\)&lt;/span&gt;. This distribution has the following PDF:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(U=k) = \binom{N}{k} \dfrac{B\left(k+\alpha, N-k+\beta \right)}{B\left(\alpha, \beta \right)},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(B(\cdot,\cdot)\)&lt;/span&gt; is the Beta function (a core feature of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;Beta distribution&lt;/a&gt;). It will be useful to see the form of this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
B\left(\alpha, \beta \right) = \int_{0}^{1} x^{\alpha-1} (1-x)^{\beta-1} dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Beta distribution is obtained “simply” by dividing both sides by &lt;span class=&#34;math inline&#34;&gt;\(B\left(\alpha, \beta \right)\)&lt;/span&gt; so that the integral comes out to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So how does the math work out? We will assuming the following likelihoods and prior:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
k|\pi &amp;amp;\sim \mbox{Bin}\left( n, \pi \right) \\
U|\pi &amp;amp;\sim \mbox{Bin}\left( N-n, \pi \right) \\
\pi &amp;amp;\sim \mbox{Beta}\left( \alpha, \beta \right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then to get the distribution of &lt;span class=&#34;math inline&#34;&gt;\(U|k\)&lt;/span&gt;, we can say the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(U|k) &amp;amp;\propto f(U|\pi) f(\pi | k) \\ 
      &amp;amp;=       f(U|\pi) \times \left[f(k | \pi) f(\pi)\right]
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The second term on the second line is mainly for clarity of what we’re doing. We already know the posterior distribution of a proportion when sampling from a Binomial distribution (since we walked through it in the previous post with infinite populations). With the likelihoods and priors denoted above, we can say that &lt;span class=&#34;math inline&#34;&gt;\(\pi|k \sim \mbox{Beta}( k+\alpha, n-k+\beta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, the idea is to get at the distribution of &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;, so conceptually we could think of doing the following (which is why the Beta-binomial can be considered a Binomial distribution for which the probability of success is unknown and randomly drawn from a beta distribution):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(U) = \int_{0}^{1} f(U|\pi) f(\pi)d\pi
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But &lt;span class=&#34;math inline&#34;&gt;\(f(\pi)\)&lt;/span&gt; is selling ourselves short; we have more information than the prior, right? So really what we’re doing is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(U|k) = \int_{0}^{1} f(U|\pi) f(\pi|k) d\pi
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This allows us to take advantage of what &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; told us about &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;. The dependence on &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; gets “inherited” from &lt;span class=&#34;math inline&#34;&gt;\(f(\pi|k)\)&lt;/span&gt;. The math is below, though for ease of notation, I’m going to write &lt;span class=&#34;math inline&#34;&gt;\(\alpha^{*}=k+\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta^{*} = n-k+\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(U|k) &amp;amp;= \int_{0}^{1} f(U|\pi) f(\pi|k)f(\pi|k) d\pi \\
&amp;amp; \\
&amp;amp;= \int_{0}^{1} \binom{N-n}{u} \pi^{u}(1-\pi)^{N-n-u} \dfrac{1}{B(\alpha^{*}, \beta^{*})} 
   \pi^{\alpha^{*}-1} (1-\pi)^{\beta^{*}-1} d\pi
&amp;amp; \\
&amp;amp;\\
&amp;amp;= \binom{N-n}{u}\dfrac{1}{B(\alpha^{*}, \beta^{*})}  
   \int_{0}^{1} \pi^{u+\alpha^{*}-1}(1-\pi)^{N-n-u+\beta^{*}-1}   d\pi
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you look back to the definition of the Beta function, you’ll note that the integral here is exactly that, so we can simplify this to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(U|k) &amp;amp;= \binom{N-n}{u}\dfrac{1}{B(\alpha^{*}, \beta^{*})}  
          B( u + \alpha^{*}, N-n-u+\beta^{*})
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This, we can note, is precisely the form of the Beta-binomial distribution. So we can say that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
U|k \sim \mbox{betabinomial}( N-n, k+\alpha, n-k+\beta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the Beta-binomial distribution we can get values such as the median of &lt;span class=&#34;math inline&#34;&gt;\(U|x\)&lt;/span&gt;, or quantiles to form a credible interval. Hence, the Bayesian approach offers an alternative form of point and interval estimation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;p&gt;As before, I will compare the methods computationally, using the coverage probability as the metric of interest. This will need to be a bit more elaborate, since we have added another parameter, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, into the mix. As before, I’m calculating &lt;em&gt;exact&lt;/em&gt; coverage probabilities, though now the hypergeometric distribution is used to find the probability that the number of defects is within the proper range.&lt;/p&gt;
&lt;p&gt;Since the “standard” method was empirically worse than the others for the infinite population case, I’m no longer considering it, and will only compare the following three approaches:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Wilson interval with FPFC&lt;/li&gt;
&lt;li&gt;Agresti-Coull interval with FPFC&lt;/li&gt;
&lt;li&gt;Bayesian interval&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/post-stats/2020-11-27-estimating-proportions-for-finite-populations.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As with the infinite-population case, these three intervals behave fairly similarly, with the Agresti-Coull interval being perhaps over-conservative (wider interval) in some cases. What is interesting to me is that the Bayesian interval dropped in coverage probability when the sampling rate was small (notably, the &lt;span class=&#34;math inline&#34;&gt;\(N=1000, n=10\)&lt;/span&gt; case). This will hopefully be a fairly rare occurrence - and in my practice has been - so I wouldn’t necessarily rule out the Bayesian interval.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;This was actually the post that I wanted to write, but I thought the &lt;a href=&#34;https://jelsema.github.io/post-stats/estimating-binomial-proportions/&#34;&gt;infinite population&lt;/a&gt; case needed to be covered first. Maybe that’s just my inner professor coming out and wanting to build concepts from the ground up. Anyway, I hope these posts were interesting, they were for me!&lt;/p&gt;
&lt;!--- 

## Sources that I looked at

https://en.wikipedia.org/wiki/Beta-binomial_distribution

https://en.wikipedia.org/wiki/Standard_error#Correction_for_finite_population

Derivation?


https://stats.stackexchange.com/questions/5158/explanation-of-finite-correction-factor

https://stats.stackexchange.com/questions/299086/question-on-covariance-for-sampling-without-replacement


https://math.stackexchange.com/questions/926478/how-does-accuracy-of-a-survey-depend-on-sample-size-and-population-size/1357604#1357604

https://online.stat.psu.edu/stat415/lesson/6/6.3
---&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
